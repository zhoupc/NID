{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NID.datasets import Pinky40\n",
    "from NID.models import DnCNN\n",
    "from NID.utils import weights_init_kaiming, batch_PSNR\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import cv2\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter \n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "@dataclass\n",
    "class training_configurations: \n",
    "    '''class for storing options of the netowrk training'''\n",
    "    preprocess: bool = False  \n",
    "    batchSize: int = 256    # training batch size \n",
    "    num_of_layers: int = 10 # number of total layers \n",
    "    epochs: int = 3   #number of training epochs \n",
    "    milestone: int = 1 # when to decay learning rate \n",
    "    lr: float = 1e-3  # initial learning rate \n",
    "    outf: str = 'logs' #path of log files \n",
    "    mode: str = 'M' #with known noise level (S) or blind training (B)\n",
    "    noiseL: float = 0.1 # noise level\n",
    "    val_noiseL: float = 0.1 # nosie level used on validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NID.datasets.Pinky40'>\n",
      "# of training samples: 40\n",
      "\n",
      "# of testing samples: 311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_train = Pinky40(train=True)\n",
    "print(type(dataset_train))\n",
    "dataset_val = Pinky40(train=False)\n",
    "loader_train = DataLoader(dataset=dataset_train, num_workers=4, \n",
    "                         batch_size=256, shuffle=True)\n",
    "print(\"# of training samples: %d\\n\" % len(dataset_train))\n",
    "print(\"# of testing samples: %d\\n\" % len(dataset_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.training_configurations'>\n",
      "logs\n"
     ]
    }
   ],
   "source": [
    "opt = training_configurations()\n",
    "print(type(opt))\n",
    "testOpt = opt.outf\n",
    "print(testOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we entered the else\n",
      "we are now doing model = nn.DataParallel...\n",
      "we exit the else\n"
     ]
    }
   ],
   "source": [
    "net = DnCNN(channels=1, num_of_layers=opt.num_of_layers)\n",
    "# Move to GPU\n",
    "device_ids = [0]\n",
    "#initialize weights\n",
    "file_results = os.path.join(opt.outf, 'net_{}_0point1NewDataS.pth'.format(opt.num_of_layers))\n",
    "if os.path.exists(file_results):\n",
    "    print(\"we entered the if\")\n",
    "    print('use the trained model as the initialization')\n",
    "    model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "    model.load_state_dict(torch.load(file_results))\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"we entered the else\")\n",
    "    net.apply(weights_init_kaiming)\n",
    "    print(\"we are now doing model = nn.DataParallel...\")\n",
    "    model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "    print(\"we exit the else\")\n",
    "    \n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "criterion.cuda()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
    "# training\n",
    "writer = SummaryWriter(opt.outf)\n",
    "step = 0\n",
    "opt.lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training - GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.000100\n",
      "[epoch 1][1/834] loss: 32.3205 PSNR_train: 16.8717\n",
      "[epoch 1][2/834] loss: 31.8563 PSNR_train: 16.8466\n",
      "[epoch 1][3/834] loss: 31.5392 PSNR_train: 16.8560\n",
      "[epoch 1][4/834] loss: 31.0924 PSNR_train: 16.8602\n",
      "[epoch 1][5/834] loss: 30.7566 PSNR_train: 16.8688\n",
      "[epoch 1][6/834] loss: 30.4676 PSNR_train: 16.8353\n",
      "[epoch 1][7/834] loss: 30.0385 PSNR_train: 16.8430\n",
      "[epoch 1][8/834] loss: 29.7016 PSNR_train: 16.8309\n",
      "[epoch 1][9/834] loss: 29.3083 PSNR_train: 16.8469\n",
      "[epoch 1][10/834] loss: 28.8119 PSNR_train: 16.8552\n",
      "[epoch 1][11/834] loss: 28.4581 PSNR_train: 16.8576\n",
      "[epoch 1][12/834] loss: 28.0682 PSNR_train: 16.8399\n",
      "[epoch 1][13/834] loss: 27.5922 PSNR_train: 16.8511\n",
      "[epoch 1][14/834] loss: 27.2345 PSNR_train: 16.8366\n",
      "[epoch 1][15/834] loss: 26.7839 PSNR_train: 16.8336\n",
      "[epoch 1][16/834] loss: 26.3449 PSNR_train: 16.8388\n",
      "[epoch 1][17/834] loss: 25.8914 PSNR_train: 16.8185\n",
      "[epoch 1][18/834] loss: 25.3613 PSNR_train: 16.8288\n",
      "[epoch 1][19/834] loss: 24.9583 PSNR_train: 16.8142\n",
      "[epoch 1][20/834] loss: 24.4668 PSNR_train: 16.8288\n",
      "[epoch 1][21/834] loss: 24.0502 PSNR_train: 16.8079\n",
      "[epoch 1][22/834] loss: 23.5698 PSNR_train: 16.8400\n",
      "[epoch 1][23/834] loss: 23.1531 PSNR_train: 16.8515\n",
      "[epoch 1][24/834] loss: 22.7317 PSNR_train: 16.8173\n",
      "[epoch 1][25/834] loss: 22.1513 PSNR_train: 16.8357\n",
      "[epoch 1][26/834] loss: 21.6610 PSNR_train: 16.8539\n",
      "[epoch 1][27/834] loss: 21.2723 PSNR_train: 16.8433\n",
      "[epoch 1][28/834] loss: 20.8908 PSNR_train: 16.8259\n",
      "[epoch 1][29/834] loss: 20.3414 PSNR_train: 16.8339\n",
      "[epoch 1][30/834] loss: 19.9967 PSNR_train: 16.8437\n",
      "[epoch 1][31/834] loss: 19.5031 PSNR_train: 16.8514\n",
      "[epoch 1][32/834] loss: 19.1095 PSNR_train: 16.8229\n",
      "[epoch 1][33/834] loss: 18.6461 PSNR_train: 16.8719\n",
      "[epoch 1][34/834] loss: 18.2374 PSNR_train: 16.8679\n",
      "[epoch 1][35/834] loss: 17.8068 PSNR_train: 16.8582\n",
      "[epoch 1][36/834] loss: 17.3052 PSNR_train: 16.8710\n",
      "[epoch 1][37/834] loss: 16.9849 PSNR_train: 16.8570\n",
      "[epoch 1][38/834] loss: 16.6033 PSNR_train: 16.8350\n",
      "[epoch 1][39/834] loss: 16.1386 PSNR_train: 16.8762\n",
      "[epoch 1][40/834] loss: 15.7958 PSNR_train: 16.8609\n",
      "[epoch 1][41/834] loss: 15.4198 PSNR_train: 16.8812\n",
      "[epoch 1][42/834] loss: 15.0139 PSNR_train: 16.8685\n",
      "[epoch 1][43/834] loss: 14.6840 PSNR_train: 16.8519\n",
      "[epoch 1][44/834] loss: 14.2807 PSNR_train: 16.8761\n",
      "[epoch 1][45/834] loss: 13.8920 PSNR_train: 16.9121\n",
      "[epoch 1][46/834] loss: 13.5589 PSNR_train: 16.8804\n",
      "[epoch 1][47/834] loss: 13.1092 PSNR_train: 16.9191\n",
      "[epoch 1][48/834] loss: 12.8937 PSNR_train: 16.9093\n",
      "[epoch 1][49/834] loss: 12.4071 PSNR_train: 16.9324\n",
      "[epoch 1][50/834] loss: 12.1007 PSNR_train: 16.9452\n",
      "[epoch 1][51/834] loss: 11.8494 PSNR_train: 16.9415\n",
      "[epoch 1][52/834] loss: 11.4350 PSNR_train: 16.9451\n",
      "[epoch 1][53/834] loss: 11.1767 PSNR_train: 16.9403\n",
      "[epoch 1][54/834] loss: 10.7895 PSNR_train: 17.0033\n",
      "[epoch 1][55/834] loss: 10.4864 PSNR_train: 17.0310\n",
      "[epoch 1][56/834] loss: 10.1949 PSNR_train: 17.0454\n",
      "[epoch 1][57/834] loss: 9.8764 PSNR_train: 17.0977\n",
      "[epoch 1][58/834] loss: 9.5970 PSNR_train: 17.1298\n",
      "[epoch 1][59/834] loss: 9.3459 PSNR_train: 17.1857\n",
      "[epoch 1][60/834] loss: 9.1137 PSNR_train: 17.2456\n",
      "[epoch 1][61/834] loss: 8.8121 PSNR_train: 17.3367\n",
      "[epoch 1][62/834] loss: 8.5498 PSNR_train: 17.4398\n",
      "[epoch 1][63/834] loss: 8.2519 PSNR_train: 17.5775\n",
      "[epoch 1][64/834] loss: 8.0300 PSNR_train: 17.7069\n",
      "[epoch 1][65/834] loss: 7.7643 PSNR_train: 17.8535\n",
      "[epoch 1][66/834] loss: 7.5505 PSNR_train: 18.0340\n",
      "[epoch 1][67/834] loss: 7.3405 PSNR_train: 18.2028\n",
      "[epoch 1][68/834] loss: 7.1516 PSNR_train: 18.3867\n",
      "[epoch 1][69/834] loss: 6.9442 PSNR_train: 18.6195\n",
      "[epoch 1][70/834] loss: 6.7199 PSNR_train: 18.8730\n",
      "[epoch 1][71/834] loss: 6.5366 PSNR_train: 19.1049\n",
      "[epoch 1][72/834] loss: 6.3412 PSNR_train: 19.3782\n",
      "[epoch 1][73/834] loss: 6.2254 PSNR_train: 19.6191\n",
      "[epoch 1][74/834] loss: 6.0071 PSNR_train: 19.9187\n",
      "[epoch 1][75/834] loss: 5.8012 PSNR_train: 20.2586\n",
      "[epoch 1][76/834] loss: 5.6484 PSNR_train: 20.5480\n",
      "[epoch 1][77/834] loss: 5.5132 PSNR_train: 20.8899\n",
      "[epoch 1][78/834] loss: 5.3441 PSNR_train: 21.2431\n",
      "[epoch 1][79/834] loss: 5.2456 PSNR_train: 21.5298\n",
      "[epoch 1][80/834] loss: 5.1134 PSNR_train: 21.8416\n",
      "[epoch 1][81/834] loss: 4.9902 PSNR_train: 22.1879\n",
      "[epoch 1][82/834] loss: 4.8519 PSNR_train: 22.5422\n",
      "[epoch 1][83/834] loss: 4.7389 PSNR_train: 22.8623\n",
      "[epoch 1][84/834] loss: 4.6073 PSNR_train: 23.2028\n",
      "[epoch 1][85/834] loss: 4.4999 PSNR_train: 23.5493\n",
      "[epoch 1][86/834] loss: 4.3411 PSNR_train: 23.9331\n",
      "[epoch 1][87/834] loss: 4.2863 PSNR_train: 24.1484\n",
      "[epoch 1][88/834] loss: 4.2287 PSNR_train: 24.4006\n",
      "[epoch 1][89/834] loss: 4.0948 PSNR_train: 24.6904\n",
      "[epoch 1][90/834] loss: 4.0701 PSNR_train: 24.8772\n",
      "[epoch 1][91/834] loss: 3.9812 PSNR_train: 25.1589\n",
      "[epoch 1][92/834] loss: 3.8451 PSNR_train: 25.4469\n",
      "[epoch 1][93/834] loss: 3.8750 PSNR_train: 25.4638\n",
      "[epoch 1][94/834] loss: 3.7050 PSNR_train: 25.8499\n",
      "[epoch 1][95/834] loss: 3.7042 PSNR_train: 25.8499\n",
      "[epoch 1][96/834] loss: 3.6773 PSNR_train: 25.9045\n",
      "[epoch 1][97/834] loss: 3.5732 PSNR_train: 26.1559\n",
      "[epoch 1][98/834] loss: 3.5439 PSNR_train: 26.1790\n",
      "[epoch 1][99/834] loss: 3.4086 PSNR_train: 26.4554\n",
      "[epoch 1][100/834] loss: 3.4811 PSNR_train: 26.2812\n",
      "[epoch 1][101/834] loss: 3.3863 PSNR_train: 26.4396\n",
      "[epoch 1][102/834] loss: 3.3108 PSNR_train: 26.5302\n",
      "[epoch 1][103/834] loss: 3.3206 PSNR_train: 26.4701\n",
      "[epoch 1][104/834] loss: 3.2335 PSNR_train: 26.6272\n",
      "[epoch 1][105/834] loss: 3.1711 PSNR_train: 26.6404\n",
      "[epoch 1][106/834] loss: 3.1410 PSNR_train: 26.6945\n",
      "[epoch 1][107/834] loss: 3.1659 PSNR_train: 26.5764\n",
      "[epoch 1][108/834] loss: 3.1454 PSNR_train: 26.5548\n",
      "[epoch 1][109/834] loss: 3.0735 PSNR_train: 26.6573\n",
      "[epoch 1][110/834] loss: 3.0670 PSNR_train: 26.6360\n",
      "[epoch 1][111/834] loss: 3.0223 PSNR_train: 26.6588\n",
      "[epoch 1][112/834] loss: 3.0086 PSNR_train: 26.6495\n",
      "[epoch 1][113/834] loss: 2.9593 PSNR_train: 26.7026\n",
      "[epoch 1][114/834] loss: 2.9559 PSNR_train: 26.6425\n",
      "[epoch 1][115/834] loss: 2.9140 PSNR_train: 26.6577\n",
      "[epoch 1][116/834] loss: 2.8715 PSNR_train: 26.7340\n",
      "[epoch 1][117/834] loss: 2.9010 PSNR_train: 26.6248\n",
      "[epoch 1][118/834] loss: 2.8611 PSNR_train: 26.6251\n",
      "[epoch 1][119/834] loss: 2.8237 PSNR_train: 26.7113\n",
      "[epoch 1][120/834] loss: 2.7891 PSNR_train: 26.7279\n",
      "[epoch 1][121/834] loss: 2.7966 PSNR_train: 26.6553\n",
      "[epoch 1][122/834] loss: 2.7251 PSNR_train: 26.8110\n",
      "[epoch 1][123/834] loss: 2.7693 PSNR_train: 26.7025\n",
      "[epoch 1][124/834] loss: 2.7092 PSNR_train: 26.7457\n",
      "[epoch 1][125/834] loss: 2.7731 PSNR_train: 26.6345\n",
      "[epoch 1][126/834] loss: 2.6144 PSNR_train: 26.9554\n",
      "[epoch 1][127/834] loss: 2.6586 PSNR_train: 26.8617\n",
      "[epoch 1][128/834] loss: 2.6387 PSNR_train: 26.8460\n",
      "[epoch 1][129/834] loss: 2.6294 PSNR_train: 26.8726\n",
      "[epoch 1][130/834] loss: 2.6119 PSNR_train: 26.8179\n",
      "[epoch 1][131/834] loss: 2.5969 PSNR_train: 26.8697\n",
      "[epoch 1][132/834] loss: 2.5924 PSNR_train: 26.8963\n",
      "[epoch 1][133/834] loss: 2.5444 PSNR_train: 26.9512\n",
      "[epoch 1][134/834] loss: 2.5079 PSNR_train: 26.9898\n",
      "[epoch 1][135/834] loss: 2.5027 PSNR_train: 27.0078\n",
      "[epoch 1][136/834] loss: 2.4457 PSNR_train: 27.0635\n",
      "[epoch 1][137/834] loss: 2.4403 PSNR_train: 27.0496\n",
      "[epoch 1][138/834] loss: 2.4800 PSNR_train: 26.9172\n",
      "[epoch 1][139/834] loss: 2.4415 PSNR_train: 27.0308\n",
      "[epoch 1][140/834] loss: 2.3645 PSNR_train: 27.0925\n",
      "[epoch 1][141/834] loss: 2.3598 PSNR_train: 27.1594\n",
      "[epoch 1][142/834] loss: 2.3987 PSNR_train: 27.0074\n",
      "[epoch 1][143/834] loss: 2.3220 PSNR_train: 27.1487\n",
      "[epoch 1][144/834] loss: 2.3667 PSNR_train: 26.9776\n",
      "[epoch 1][145/834] loss: 2.2744 PSNR_train: 27.1676\n",
      "[epoch 1][146/834] loss: 2.3095 PSNR_train: 27.0501\n",
      "[epoch 1][147/834] loss: 2.2829 PSNR_train: 27.0922\n",
      "[epoch 1][148/834] loss: 2.2468 PSNR_train: 27.0680\n",
      "[epoch 1][149/834] loss: 2.2487 PSNR_train: 27.0287\n",
      "[epoch 1][150/834] loss: 2.2536 PSNR_train: 27.0113\n",
      "[epoch 1][151/834] loss: 2.2007 PSNR_train: 27.0443\n",
      "[epoch 1][152/834] loss: 2.1739 PSNR_train: 27.0951\n",
      "[epoch 1][153/834] loss: 2.1859 PSNR_train: 26.9960\n",
      "[epoch 1][154/834] loss: 2.1501 PSNR_train: 27.0385\n",
      "[epoch 1][155/834] loss: 2.1113 PSNR_train: 27.1245\n",
      "[epoch 1][156/834] loss: 2.1750 PSNR_train: 26.9577\n",
      "[epoch 1][157/834] loss: 2.1199 PSNR_train: 26.9925\n",
      "[epoch 1][158/834] loss: 2.0673 PSNR_train: 27.1012\n",
      "[epoch 1][159/834] loss: 2.0620 PSNR_train: 27.0915\n",
      "[epoch 1][160/834] loss: 2.0715 PSNR_train: 27.0449\n",
      "[epoch 1][161/834] loss: 2.0261 PSNR_train: 27.1402\n",
      "[epoch 1][162/834] loss: 2.0186 PSNR_train: 27.0687\n",
      "[epoch 1][163/834] loss: 1.9808 PSNR_train: 27.1231\n",
      "[epoch 1][164/834] loss: 1.9841 PSNR_train: 27.1223\n",
      "[epoch 1][165/834] loss: 1.9649 PSNR_train: 27.1450\n",
      "[epoch 1][166/834] loss: 1.9809 PSNR_train: 27.0739\n",
      "[epoch 1][167/834] loss: 1.8776 PSNR_train: 27.2690\n",
      "[epoch 1][168/834] loss: 1.9349 PSNR_train: 27.1027\n",
      "[epoch 1][169/834] loss: 1.8820 PSNR_train: 27.2375\n",
      "[epoch 1][170/834] loss: 1.9045 PSNR_train: 27.1575\n",
      "[epoch 1][171/834] loss: 1.8801 PSNR_train: 27.1729\n",
      "[epoch 1][172/834] loss: 1.8162 PSNR_train: 27.3163\n",
      "[epoch 1][173/834] loss: 1.8311 PSNR_train: 27.2671\n",
      "[epoch 1][174/834] loss: 1.8644 PSNR_train: 27.1787\n",
      "[epoch 1][175/834] loss: 1.8112 PSNR_train: 27.3014\n",
      "[epoch 1][176/834] loss: 1.8355 PSNR_train: 27.2922\n",
      "[epoch 1][177/834] loss: 1.7551 PSNR_train: 27.4257\n",
      "[epoch 1][178/834] loss: 1.8065 PSNR_train: 27.3308\n",
      "[epoch 1][179/834] loss: 1.7664 PSNR_train: 27.4382\n",
      "[epoch 1][180/834] loss: 1.7518 PSNR_train: 27.4434\n",
      "[epoch 1][181/834] loss: 1.7704 PSNR_train: 27.3986\n",
      "[epoch 1][182/834] loss: 1.7311 PSNR_train: 27.4522\n",
      "[epoch 1][183/834] loss: 1.7672 PSNR_train: 27.3829\n",
      "[epoch 1][184/834] loss: 1.7631 PSNR_train: 27.3925\n",
      "[epoch 1][185/834] loss: 1.7710 PSNR_train: 27.4031\n",
      "[epoch 1][186/834] loss: 1.6767 PSNR_train: 27.5665\n",
      "[epoch 1][187/834] loss: 1.7075 PSNR_train: 27.4838\n",
      "[epoch 1][188/834] loss: 1.6415 PSNR_train: 27.7072\n",
      "[epoch 1][189/834] loss: 1.6535 PSNR_train: 27.6731\n",
      "[epoch 1][190/834] loss: 1.6667 PSNR_train: 27.6277\n",
      "[epoch 1][191/834] loss: 1.6168 PSNR_train: 27.7821\n",
      "[epoch 1][192/834] loss: 1.5801 PSNR_train: 27.9127\n",
      "[epoch 1][193/834] loss: 1.5892 PSNR_train: 27.9665\n",
      "[epoch 1][194/834] loss: 1.5768 PSNR_train: 28.0343\n",
      "[epoch 1][195/834] loss: 1.6255 PSNR_train: 27.9058\n",
      "[epoch 1][196/834] loss: 1.5941 PSNR_train: 28.0320\n",
      "[epoch 1][197/834] loss: 1.5693 PSNR_train: 28.1011\n",
      "[epoch 1][198/834] loss: 1.5439 PSNR_train: 28.1884\n",
      "[epoch 1][199/834] loss: 1.5061 PSNR_train: 28.3320\n",
      "[epoch 1][200/834] loss: 1.4816 PSNR_train: 28.4066\n",
      "[epoch 1][201/834] loss: 1.4890 PSNR_train: 28.4147\n",
      "[epoch 1][202/834] loss: 1.5124 PSNR_train: 28.3832\n",
      "[epoch 1][203/834] loss: 1.4819 PSNR_train: 28.4396\n",
      "[epoch 1][204/834] loss: 1.4510 PSNR_train: 28.5773\n",
      "[epoch 1][205/834] loss: 1.4603 PSNR_train: 28.5119\n",
      "[epoch 1][206/834] loss: 1.4651 PSNR_train: 28.5427\n",
      "[epoch 1][207/834] loss: 1.4222 PSNR_train: 28.6797\n",
      "[epoch 1][208/834] loss: 1.3810 PSNR_train: 28.7731\n",
      "[epoch 1][209/834] loss: 1.4294 PSNR_train: 28.6164\n",
      "[epoch 1][210/834] loss: 1.4337 PSNR_train: 28.6255\n",
      "[epoch 1][211/834] loss: 1.4150 PSNR_train: 28.6560\n",
      "[epoch 1][212/834] loss: 1.3970 PSNR_train: 28.7229\n",
      "[epoch 1][213/834] loss: 1.3966 PSNR_train: 28.7164\n",
      "[epoch 1][214/834] loss: 1.4699 PSNR_train: 28.5734\n",
      "[epoch 1][215/834] loss: 1.3597 PSNR_train: 28.8815\n",
      "[epoch 1][216/834] loss: 1.3681 PSNR_train: 28.8634\n",
      "[epoch 1][217/834] loss: 1.4047 PSNR_train: 28.7621\n",
      "[epoch 1][218/834] loss: 1.3729 PSNR_train: 28.8540\n",
      "[epoch 1][219/834] loss: 1.3286 PSNR_train: 29.0792\n",
      "[epoch 1][220/834] loss: 1.3243 PSNR_train: 29.0757\n",
      "[epoch 1][221/834] loss: 1.2937 PSNR_train: 29.2263\n",
      "[epoch 1][222/834] loss: 1.3060 PSNR_train: 29.1934\n",
      "[epoch 1][223/834] loss: 1.3127 PSNR_train: 29.1708\n",
      "[epoch 1][224/834] loss: 1.3241 PSNR_train: 29.1614\n",
      "[epoch 1][225/834] loss: 1.2891 PSNR_train: 29.2831\n",
      "[epoch 1][226/834] loss: 1.2901 PSNR_train: 29.3690\n",
      "[epoch 1][227/834] loss: 1.2237 PSNR_train: 29.5733\n",
      "[epoch 1][228/834] loss: 1.2501 PSNR_train: 29.4989\n",
      "[epoch 1][229/834] loss: 1.2527 PSNR_train: 29.4525\n",
      "[epoch 1][230/834] loss: 1.2458 PSNR_train: 29.4745\n",
      "[epoch 1][231/834] loss: 1.3108 PSNR_train: 29.1898\n",
      "[epoch 1][232/834] loss: 1.2605 PSNR_train: 29.3894\n",
      "[epoch 1][233/834] loss: 1.2643 PSNR_train: 29.3492\n",
      "[epoch 1][234/834] loss: 1.2393 PSNR_train: 29.5595\n",
      "[epoch 1][235/834] loss: 1.2051 PSNR_train: 29.5648\n",
      "[epoch 1][236/834] loss: 1.2799 PSNR_train: 29.3228\n",
      "[epoch 1][237/834] loss: 1.2375 PSNR_train: 29.4796\n",
      "[epoch 1][238/834] loss: 1.2956 PSNR_train: 29.3075\n",
      "[epoch 1][239/834] loss: 1.1887 PSNR_train: 29.7522\n",
      "[epoch 1][240/834] loss: 1.1986 PSNR_train: 29.6974\n",
      "[epoch 1][241/834] loss: 1.1732 PSNR_train: 29.7980\n",
      "[epoch 1][242/834] loss: 1.2295 PSNR_train: 29.6152\n",
      "[epoch 1][243/834] loss: 1.1840 PSNR_train: 29.7636\n",
      "[epoch 1][244/834] loss: 1.1631 PSNR_train: 29.8577\n",
      "[epoch 1][245/834] loss: 1.1983 PSNR_train: 29.7178\n",
      "[epoch 1][246/834] loss: 1.1480 PSNR_train: 29.9731\n",
      "[epoch 1][247/834] loss: 1.1898 PSNR_train: 29.7806\n",
      "[epoch 1][248/834] loss: 1.2119 PSNR_train: 29.6579\n",
      "[epoch 1][249/834] loss: 1.1423 PSNR_train: 29.9761\n",
      "[epoch 1][250/834] loss: 1.1729 PSNR_train: 29.8153\n",
      "[epoch 1][251/834] loss: 1.1310 PSNR_train: 30.0307\n",
      "[epoch 1][252/834] loss: 1.1537 PSNR_train: 29.9323\n",
      "[epoch 1][253/834] loss: 1.1487 PSNR_train: 29.9269\n",
      "[epoch 1][254/834] loss: 1.1224 PSNR_train: 30.0623\n",
      "[epoch 1][255/834] loss: 1.1460 PSNR_train: 29.9669\n",
      "[epoch 1][256/834] loss: 1.1187 PSNR_train: 30.0494\n",
      "[epoch 1][257/834] loss: 1.1473 PSNR_train: 29.9470\n",
      "[epoch 1][258/834] loss: 1.1598 PSNR_train: 29.8407\n",
      "[epoch 1][259/834] loss: 1.1275 PSNR_train: 30.0408\n",
      "[epoch 1][260/834] loss: 1.1009 PSNR_train: 30.0857\n",
      "[epoch 1][261/834] loss: 1.0851 PSNR_train: 30.1699\n",
      "[epoch 1][262/834] loss: 1.1209 PSNR_train: 30.0285\n",
      "[epoch 1][263/834] loss: 1.1517 PSNR_train: 29.8929\n",
      "[epoch 1][264/834] loss: 1.0945 PSNR_train: 30.1219\n",
      "[epoch 1][265/834] loss: 1.1224 PSNR_train: 30.0039\n",
      "[epoch 1][266/834] loss: 1.0397 PSNR_train: 30.3744\n",
      "[epoch 1][267/834] loss: 1.0967 PSNR_train: 30.1791\n",
      "[epoch 1][268/834] loss: 1.0795 PSNR_train: 30.1946\n",
      "[epoch 1][269/834] loss: 1.0644 PSNR_train: 30.3836\n",
      "[epoch 1][270/834] loss: 1.0531 PSNR_train: 30.3446\n",
      "[epoch 1][271/834] loss: 1.0553 PSNR_train: 30.4627\n",
      "[epoch 1][272/834] loss: 1.0602 PSNR_train: 30.3785\n",
      "[epoch 1][273/834] loss: 1.0872 PSNR_train: 30.2673\n",
      "[epoch 1][274/834] loss: 1.0625 PSNR_train: 30.3432\n",
      "[epoch 1][275/834] loss: 1.0644 PSNR_train: 30.3442\n",
      "[epoch 1][276/834] loss: 0.9981 PSNR_train: 30.7502\n",
      "[epoch 1][277/834] loss: 1.0438 PSNR_train: 30.4993\n",
      "[epoch 1][278/834] loss: 1.0553 PSNR_train: 30.3429\n",
      "[epoch 1][279/834] loss: 1.0819 PSNR_train: 30.2236\n",
      "[epoch 1][280/834] loss: 1.0025 PSNR_train: 30.6302\n",
      "[epoch 1][281/834] loss: 0.9999 PSNR_train: 30.5761\n",
      "[epoch 1][282/834] loss: 1.0473 PSNR_train: 30.4101\n",
      "[epoch 1][283/834] loss: 1.0567 PSNR_train: 30.2897\n",
      "[epoch 1][284/834] loss: 1.0391 PSNR_train: 30.3351\n",
      "[epoch 1][285/834] loss: 1.0246 PSNR_train: 30.5120\n",
      "[epoch 1][286/834] loss: 1.0361 PSNR_train: 30.3702\n",
      "[epoch 1][287/834] loss: 1.0071 PSNR_train: 30.5069\n",
      "[epoch 1][288/834] loss: 1.0061 PSNR_train: 30.5853\n",
      "[epoch 1][289/834] loss: 1.0097 PSNR_train: 30.4586\n",
      "[epoch 1][290/834] loss: 0.9900 PSNR_train: 30.4771\n",
      "[epoch 1][291/834] loss: 0.9652 PSNR_train: 30.7230\n",
      "[epoch 1][292/834] loss: 1.0392 PSNR_train: 30.3652\n",
      "[epoch 1][293/834] loss: 1.0044 PSNR_train: 30.4778\n",
      "[epoch 1][294/834] loss: 1.0155 PSNR_train: 30.3528\n",
      "[epoch 1][295/834] loss: 0.9995 PSNR_train: 30.5133\n",
      "[epoch 1][296/834] loss: 0.9716 PSNR_train: 30.7640\n",
      "[epoch 1][297/834] loss: 1.0005 PSNR_train: 30.4886\n",
      "[epoch 1][298/834] loss: 0.9638 PSNR_train: 30.7770\n",
      "[epoch 1][299/834] loss: 0.9649 PSNR_train: 30.7309\n",
      "[epoch 1][300/834] loss: 1.0082 PSNR_train: 30.6653\n",
      "[epoch 1][301/834] loss: 1.0179 PSNR_train: 30.5696\n",
      "[epoch 1][302/834] loss: 0.9738 PSNR_train: 30.8378\n",
      "[epoch 1][303/834] loss: 0.9852 PSNR_train: 30.8117\n",
      "[epoch 1][304/834] loss: 0.9781 PSNR_train: 30.7960\n",
      "[epoch 1][305/834] loss: 0.9583 PSNR_train: 30.9167\n",
      "[epoch 1][306/834] loss: 0.9491 PSNR_train: 31.0074\n",
      "[epoch 1][307/834] loss: 0.9368 PSNR_train: 31.0600\n",
      "[epoch 1][308/834] loss: 0.9970 PSNR_train: 30.6871\n",
      "[epoch 1][309/834] loss: 1.0240 PSNR_train: 30.4955\n",
      "[epoch 1][310/834] loss: 0.9971 PSNR_train: 30.6230\n",
      "[epoch 1][311/834] loss: 0.9578 PSNR_train: 30.8155\n",
      "[epoch 1][312/834] loss: 0.9668 PSNR_train: 30.6585\n",
      "[epoch 1][313/834] loss: 0.9184 PSNR_train: 30.9503\n",
      "[epoch 1][314/834] loss: 0.9196 PSNR_train: 30.9315\n",
      "[epoch 1][315/834] loss: 0.9450 PSNR_train: 30.8392\n",
      "[epoch 1][316/834] loss: 0.9542 PSNR_train: 30.8702\n",
      "[epoch 1][317/834] loss: 0.9557 PSNR_train: 30.7937\n",
      "[epoch 1][318/834] loss: 0.9743 PSNR_train: 30.7232\n",
      "[epoch 1][319/834] loss: 0.9423 PSNR_train: 30.8616\n",
      "[epoch 1][320/834] loss: 0.9617 PSNR_train: 30.8329\n",
      "[epoch 1][321/834] loss: 0.9283 PSNR_train: 30.9397\n",
      "[epoch 1][322/834] loss: 0.9905 PSNR_train: 30.6051\n",
      "[epoch 1][323/834] loss: 0.9331 PSNR_train: 30.7521\n",
      "[epoch 1][324/834] loss: 0.9416 PSNR_train: 30.6543\n",
      "[epoch 1][325/834] loss: 0.9367 PSNR_train: 30.7406\n",
      "[epoch 1][326/834] loss: 0.9504 PSNR_train: 30.6282\n",
      "[epoch 1][327/834] loss: 0.9538 PSNR_train: 30.6400\n",
      "[epoch 1][328/834] loss: 0.9579 PSNR_train: 30.6764\n",
      "[epoch 1][329/834] loss: 0.8786 PSNR_train: 31.0701\n",
      "[epoch 1][330/834] loss: 0.9730 PSNR_train: 30.5920\n",
      "[epoch 1][331/834] loss: 0.9049 PSNR_train: 31.0069\n",
      "[epoch 1][332/834] loss: 0.9327 PSNR_train: 30.8169\n",
      "[epoch 1][333/834] loss: 0.9361 PSNR_train: 30.9784\n",
      "[epoch 1][334/834] loss: 0.9163 PSNR_train: 31.0288\n",
      "[epoch 1][335/834] loss: 0.9670 PSNR_train: 30.7980\n",
      "[epoch 1][336/834] loss: 0.9628 PSNR_train: 30.8680\n",
      "[epoch 1][337/834] loss: 0.9538 PSNR_train: 30.8527\n",
      "[epoch 1][338/834] loss: 0.9197 PSNR_train: 31.0680\n",
      "[epoch 1][339/834] loss: 0.9077 PSNR_train: 31.0391\n",
      "[epoch 1][340/834] loss: 0.8705 PSNR_train: 31.2623\n",
      "[epoch 1][341/834] loss: 0.9064 PSNR_train: 31.1332\n",
      "[epoch 1][342/834] loss: 0.9449 PSNR_train: 30.8584\n",
      "[epoch 1][343/834] loss: 0.9039 PSNR_train: 31.0595\n",
      "[epoch 1][344/834] loss: 0.9023 PSNR_train: 31.0973\n",
      "[epoch 1][345/834] loss: 0.8878 PSNR_train: 30.9770\n",
      "[epoch 1][346/834] loss: 0.9313 PSNR_train: 30.8087\n",
      "[epoch 1][347/834] loss: 0.9107 PSNR_train: 30.8997\n",
      "[epoch 1][348/834] loss: 0.8735 PSNR_train: 31.0949\n",
      "[epoch 1][349/834] loss: 0.8642 PSNR_train: 31.1673\n",
      "[epoch 1][350/834] loss: 0.8921 PSNR_train: 31.0368\n",
      "[epoch 1][351/834] loss: 0.9007 PSNR_train: 31.1079\n",
      "[epoch 1][352/834] loss: 0.8725 PSNR_train: 31.1606\n",
      "[epoch 1][353/834] loss: 0.9157 PSNR_train: 30.8773\n",
      "[epoch 1][354/834] loss: 0.8688 PSNR_train: 31.1357\n",
      "[epoch 1][355/834] loss: 0.8576 PSNR_train: 31.1473\n",
      "[epoch 1][356/834] loss: 0.9519 PSNR_train: 30.7150\n",
      "[epoch 1][357/834] loss: 0.8670 PSNR_train: 31.1008\n",
      "[epoch 1][358/834] loss: 0.8661 PSNR_train: 31.0354\n",
      "[epoch 1][359/834] loss: 0.8786 PSNR_train: 31.0532\n",
      "[epoch 1][360/834] loss: 0.8484 PSNR_train: 31.2701\n",
      "[epoch 1][361/834] loss: 0.8323 PSNR_train: 31.3727\n",
      "[epoch 1][362/834] loss: 0.8445 PSNR_train: 31.3659\n",
      "[epoch 1][363/834] loss: 0.8691 PSNR_train: 31.2503\n",
      "[epoch 1][364/834] loss: 0.9056 PSNR_train: 31.0462\n",
      "[epoch 1][365/834] loss: 0.8540 PSNR_train: 31.2490\n",
      "[epoch 1][366/834] loss: 0.8860 PSNR_train: 31.1355\n",
      "[epoch 1][367/834] loss: 0.8899 PSNR_train: 31.0652\n",
      "[epoch 1][368/834] loss: 0.8671 PSNR_train: 31.1073\n",
      "[epoch 1][369/834] loss: 0.9020 PSNR_train: 31.0154\n",
      "[epoch 1][370/834] loss: 0.9135 PSNR_train: 30.9450\n",
      "[epoch 1][371/834] loss: 0.9166 PSNR_train: 30.8469\n",
      "[epoch 1][372/834] loss: 0.8696 PSNR_train: 31.1165\n",
      "[epoch 1][373/834] loss: 0.8545 PSNR_train: 31.2352\n",
      "[epoch 1][374/834] loss: 0.8635 PSNR_train: 31.2650\n",
      "[epoch 1][375/834] loss: 0.8862 PSNR_train: 31.0182\n",
      "[epoch 1][376/834] loss: 0.8890 PSNR_train: 31.1665\n",
      "[epoch 1][377/834] loss: 0.8447 PSNR_train: 31.2415\n",
      "[epoch 1][378/834] loss: 0.8779 PSNR_train: 30.9770\n",
      "[epoch 1][379/834] loss: 0.8488 PSNR_train: 31.2645\n",
      "[epoch 1][380/834] loss: 0.8736 PSNR_train: 31.0488\n",
      "[epoch 1][381/834] loss: 0.8697 PSNR_train: 31.1174\n",
      "[epoch 1][382/834] loss: 0.8101 PSNR_train: 31.5039\n",
      "[epoch 1][383/834] loss: 0.8403 PSNR_train: 31.2014\n",
      "[epoch 1][384/834] loss: 0.8452 PSNR_train: 31.3233\n",
      "[epoch 1][385/834] loss: 0.8182 PSNR_train: 31.4291\n",
      "[epoch 1][386/834] loss: 0.8377 PSNR_train: 31.3642\n",
      "[epoch 1][387/834] loss: 0.8471 PSNR_train: 31.2804\n",
      "[epoch 1][388/834] loss: 0.8402 PSNR_train: 31.4034\n",
      "[epoch 1][389/834] loss: 0.8470 PSNR_train: 31.1831\n",
      "[epoch 1][390/834] loss: 0.7795 PSNR_train: 31.5971\n",
      "[epoch 1][391/834] loss: 0.9072 PSNR_train: 30.8862\n",
      "[epoch 1][392/834] loss: 0.8253 PSNR_train: 31.2838\n",
      "[epoch 1][393/834] loss: 0.8740 PSNR_train: 31.0797\n",
      "[epoch 1][394/834] loss: 0.8775 PSNR_train: 30.9734\n",
      "[epoch 1][395/834] loss: 0.8066 PSNR_train: 31.3682\n",
      "[epoch 1][396/834] loss: 0.8659 PSNR_train: 31.1196\n",
      "[epoch 1][397/834] loss: 0.8209 PSNR_train: 31.3046\n",
      "[epoch 1][398/834] loss: 0.8749 PSNR_train: 31.0814\n",
      "[epoch 1][399/834] loss: 0.8473 PSNR_train: 31.2078\n",
      "[epoch 1][400/834] loss: 0.8492 PSNR_train: 31.2737\n",
      "[epoch 1][401/834] loss: 0.8449 PSNR_train: 31.3519\n",
      "[epoch 1][402/834] loss: 0.8381 PSNR_train: 31.4234\n",
      "[epoch 1][403/834] loss: 0.8082 PSNR_train: 31.5466\n",
      "[epoch 1][404/834] loss: 0.8057 PSNR_train: 31.4706\n",
      "[epoch 1][405/834] loss: 0.8594 PSNR_train: 31.2023\n",
      "[epoch 1][406/834] loss: 0.8389 PSNR_train: 31.2969\n",
      "[epoch 1][407/834] loss: 0.8393 PSNR_train: 31.2491\n",
      "[epoch 1][408/834] loss: 0.8257 PSNR_train: 31.3147\n",
      "[epoch 1][409/834] loss: 0.8458 PSNR_train: 31.1598\n",
      "[epoch 1][410/834] loss: 0.8528 PSNR_train: 31.3087\n",
      "[epoch 1][411/834] loss: 0.8324 PSNR_train: 31.2962\n",
      "[epoch 1][412/834] loss: 0.8062 PSNR_train: 31.5068\n",
      "[epoch 1][413/834] loss: 0.8607 PSNR_train: 31.0716\n",
      "[epoch 1][414/834] loss: 0.8049 PSNR_train: 31.4950\n",
      "[epoch 1][415/834] loss: 0.7963 PSNR_train: 31.5600\n",
      "[epoch 1][416/834] loss: 0.8491 PSNR_train: 31.1533\n",
      "[epoch 1][417/834] loss: 0.8564 PSNR_train: 31.2317\n",
      "[epoch 1][418/834] loss: 0.8392 PSNR_train: 31.2205\n",
      "[epoch 1][419/834] loss: 0.8218 PSNR_train: 31.2796\n",
      "[epoch 1][420/834] loss: 0.8359 PSNR_train: 31.2289\n",
      "[epoch 1][421/834] loss: 0.8107 PSNR_train: 31.3203\n",
      "[epoch 1][422/834] loss: 0.8578 PSNR_train: 31.0960\n",
      "[epoch 1][423/834] loss: 0.7963 PSNR_train: 31.5665\n",
      "[epoch 1][424/834] loss: 0.7826 PSNR_train: 31.5972\n",
      "[epoch 1][425/834] loss: 0.8241 PSNR_train: 31.3922\n",
      "[epoch 1][426/834] loss: 0.8156 PSNR_train: 31.3493\n",
      "[epoch 1][427/834] loss: 0.7633 PSNR_train: 31.8104\n",
      "[epoch 1][428/834] loss: 0.7693 PSNR_train: 31.7113\n",
      "[epoch 1][429/834] loss: 0.7766 PSNR_train: 31.5725\n",
      "[epoch 1][430/834] loss: 0.7917 PSNR_train: 31.5345\n",
      "[epoch 1][431/834] loss: 0.8100 PSNR_train: 31.2845\n",
      "[epoch 1][432/834] loss: 0.7916 PSNR_train: 31.4688\n",
      "[epoch 1][433/834] loss: 0.7519 PSNR_train: 31.6606\n",
      "[epoch 1][434/834] loss: 0.7530 PSNR_train: 31.7057\n",
      "[epoch 1][435/834] loss: 0.8075 PSNR_train: 31.2566\n",
      "[epoch 1][436/834] loss: 0.7796 PSNR_train: 31.4587\n",
      "[epoch 1][437/834] loss: 0.8360 PSNR_train: 31.3451\n",
      "[epoch 1][438/834] loss: 0.8029 PSNR_train: 31.5388\n",
      "[epoch 1][439/834] loss: 0.7664 PSNR_train: 31.6232\n",
      "[epoch 1][440/834] loss: 0.7853 PSNR_train: 31.5221\n",
      "[epoch 1][441/834] loss: 0.8203 PSNR_train: 31.3172\n",
      "[epoch 1][442/834] loss: 0.8177 PSNR_train: 31.2583\n",
      "[epoch 1][443/834] loss: 0.7699 PSNR_train: 31.5564\n",
      "[epoch 1][444/834] loss: 0.8571 PSNR_train: 31.1435\n",
      "[epoch 1][445/834] loss: 0.7861 PSNR_train: 31.4594\n",
      "[epoch 1][446/834] loss: 0.7860 PSNR_train: 31.5005\n",
      "[epoch 1][447/834] loss: 0.8317 PSNR_train: 31.2281\n",
      "[epoch 1][448/834] loss: 0.8096 PSNR_train: 31.4129\n",
      "[epoch 1][449/834] loss: 0.7694 PSNR_train: 31.6383\n",
      "[epoch 1][450/834] loss: 0.7929 PSNR_train: 31.6028\n",
      "[epoch 1][451/834] loss: 0.8152 PSNR_train: 31.4775\n",
      "[epoch 1][452/834] loss: 0.8539 PSNR_train: 31.1942\n",
      "[epoch 1][453/834] loss: 0.7663 PSNR_train: 31.6953\n",
      "[epoch 1][454/834] loss: 0.8103 PSNR_train: 31.5127\n",
      "[epoch 1][455/834] loss: 0.7623 PSNR_train: 31.6203\n",
      "[epoch 1][456/834] loss: 0.7532 PSNR_train: 31.7300\n",
      "[epoch 1][457/834] loss: 0.7838 PSNR_train: 31.5078\n",
      "[epoch 1][458/834] loss: 0.7688 PSNR_train: 31.5702\n",
      "[epoch 1][459/834] loss: 0.8410 PSNR_train: 31.1365\n",
      "[epoch 1][460/834] loss: 0.7935 PSNR_train: 31.4786\n",
      "[epoch 1][461/834] loss: 0.7952 PSNR_train: 31.3594\n",
      "[epoch 1][462/834] loss: 0.8496 PSNR_train: 31.1689\n",
      "[epoch 1][463/834] loss: 0.7478 PSNR_train: 31.7186\n",
      "[epoch 1][464/834] loss: 0.8346 PSNR_train: 31.1933\n",
      "[epoch 1][465/834] loss: 0.7881 PSNR_train: 31.5373\n",
      "[epoch 1][466/834] loss: 0.7842 PSNR_train: 31.5025\n",
      "[epoch 1][467/834] loss: 0.7797 PSNR_train: 31.6018\n",
      "[epoch 1][468/834] loss: 0.7744 PSNR_train: 31.5365\n",
      "[epoch 1][469/834] loss: 0.7986 PSNR_train: 31.4517\n",
      "[epoch 1][470/834] loss: 0.7926 PSNR_train: 31.4321\n",
      "[epoch 1][471/834] loss: 0.7640 PSNR_train: 31.5169\n",
      "[epoch 1][472/834] loss: 0.7978 PSNR_train: 31.3128\n",
      "[epoch 1][473/834] loss: 0.7622 PSNR_train: 31.6261\n",
      "[epoch 1][474/834] loss: 0.7816 PSNR_train: 31.4033\n",
      "[epoch 1][475/834] loss: 0.7288 PSNR_train: 31.7090\n",
      "[epoch 1][476/834] loss: 0.7818 PSNR_train: 31.5260\n",
      "[epoch 1][477/834] loss: 0.7917 PSNR_train: 31.4086\n",
      "[epoch 1][478/834] loss: 0.7438 PSNR_train: 31.6963\n",
      "[epoch 1][479/834] loss: 0.7324 PSNR_train: 31.8135\n",
      "[epoch 1][480/834] loss: 0.7352 PSNR_train: 31.7344\n",
      "[epoch 1][481/834] loss: 0.7768 PSNR_train: 31.4827\n",
      "[epoch 1][482/834] loss: 0.7774 PSNR_train: 31.5979\n",
      "[epoch 1][483/834] loss: 0.7660 PSNR_train: 31.5922\n",
      "[epoch 1][484/834] loss: 0.7504 PSNR_train: 31.8408\n",
      "[epoch 1][485/834] loss: 0.7808 PSNR_train: 31.4711\n",
      "[epoch 1][486/834] loss: 0.7683 PSNR_train: 31.5672\n",
      "[epoch 1][487/834] loss: 0.7512 PSNR_train: 31.6881\n",
      "[epoch 1][488/834] loss: 0.7395 PSNR_train: 31.7869\n",
      "[epoch 1][489/834] loss: 0.7873 PSNR_train: 31.4237\n",
      "[epoch 1][490/834] loss: 0.7605 PSNR_train: 31.6932\n",
      "[epoch 1][491/834] loss: 0.7470 PSNR_train: 31.7470\n",
      "[epoch 1][492/834] loss: 0.7366 PSNR_train: 31.9663\n",
      "[epoch 1][493/834] loss: 0.7629 PSNR_train: 31.6307\n",
      "[epoch 1][494/834] loss: 0.7886 PSNR_train: 31.4299\n",
      "[epoch 1][495/834] loss: 0.7778 PSNR_train: 31.4745\n",
      "[epoch 1][496/834] loss: 0.7504 PSNR_train: 31.6543\n",
      "[epoch 1][497/834] loss: 0.7809 PSNR_train: 31.5311\n",
      "[epoch 1][498/834] loss: 0.7218 PSNR_train: 31.8774\n",
      "[epoch 1][499/834] loss: 0.7250 PSNR_train: 31.8510\n",
      "[epoch 1][500/834] loss: 0.7806 PSNR_train: 31.5422\n",
      "[epoch 1][501/834] loss: 0.7373 PSNR_train: 31.8036\n",
      "[epoch 1][502/834] loss: 0.7841 PSNR_train: 31.5323\n",
      "[epoch 1][503/834] loss: 0.7567 PSNR_train: 31.6898\n",
      "[epoch 1][504/834] loss: 0.7509 PSNR_train: 31.6785\n",
      "[epoch 1][505/834] loss: 0.7917 PSNR_train: 31.2891\n",
      "[epoch 1][506/834] loss: 0.7590 PSNR_train: 31.5508\n",
      "[epoch 1][507/834] loss: 0.7233 PSNR_train: 31.6979\n",
      "[epoch 1][508/834] loss: 0.7838 PSNR_train: 31.3414\n",
      "[epoch 1][509/834] loss: 0.7485 PSNR_train: 31.5978\n",
      "[epoch 1][510/834] loss: 0.7377 PSNR_train: 31.6774\n",
      "[epoch 1][511/834] loss: 0.7462 PSNR_train: 31.7203\n",
      "[epoch 1][512/834] loss: 0.7353 PSNR_train: 31.7877\n",
      "[epoch 1][513/834] loss: 0.7257 PSNR_train: 31.8588\n",
      "[epoch 1][514/834] loss: 0.7678 PSNR_train: 31.6141\n",
      "[epoch 1][515/834] loss: 0.7453 PSNR_train: 31.6533\n",
      "[epoch 1][516/834] loss: 0.7484 PSNR_train: 31.7587\n",
      "[epoch 1][517/834] loss: 0.6883 PSNR_train: 32.0020\n",
      "[epoch 1][518/834] loss: 0.7470 PSNR_train: 31.6567\n",
      "[epoch 1][519/834] loss: 0.8020 PSNR_train: 31.4453\n",
      "[epoch 1][520/834] loss: 0.7615 PSNR_train: 31.6101\n",
      "[epoch 1][521/834] loss: 0.7122 PSNR_train: 31.9488\n",
      "[epoch 1][522/834] loss: 0.7744 PSNR_train: 31.5508\n",
      "[epoch 1][523/834] loss: 0.7849 PSNR_train: 31.5849\n",
      "[epoch 1][524/834] loss: 0.7569 PSNR_train: 31.6415\n",
      "[epoch 1][525/834] loss: 0.7190 PSNR_train: 31.9882\n",
      "[epoch 1][526/834] loss: 0.7405 PSNR_train: 31.7547\n",
      "[epoch 1][527/834] loss: 0.7566 PSNR_train: 31.7234\n",
      "[epoch 1][528/834] loss: 0.7726 PSNR_train: 31.5888\n",
      "[epoch 1][529/834] loss: 0.7233 PSNR_train: 31.9769\n",
      "[epoch 1][530/834] loss: 0.7902 PSNR_train: 31.6000\n",
      "[epoch 1][531/834] loss: 0.7841 PSNR_train: 31.4748\n",
      "[epoch 1][532/834] loss: 0.7361 PSNR_train: 31.8873\n",
      "[epoch 1][533/834] loss: 0.7335 PSNR_train: 31.8294\n",
      "[epoch 1][534/834] loss: 0.7324 PSNR_train: 31.7996\n",
      "[epoch 1][535/834] loss: 0.7701 PSNR_train: 31.5692\n",
      "[epoch 1][536/834] loss: 0.7251 PSNR_train: 31.8774\n",
      "[epoch 1][537/834] loss: 0.7319 PSNR_train: 31.9309\n",
      "[epoch 1][538/834] loss: 0.6894 PSNR_train: 31.9962\n",
      "[epoch 1][539/834] loss: 0.7151 PSNR_train: 31.8422\n",
      "[epoch 1][540/834] loss: 0.8048 PSNR_train: 31.2849\n",
      "[epoch 1][541/834] loss: 0.7592 PSNR_train: 31.5325\n",
      "[epoch 1][542/834] loss: 0.7222 PSNR_train: 31.6874\n",
      "[epoch 1][543/834] loss: 0.7223 PSNR_train: 31.7601\n",
      "[epoch 1][544/834] loss: 0.7663 PSNR_train: 31.4725\n",
      "[epoch 1][545/834] loss: 0.7227 PSNR_train: 31.8545\n",
      "[epoch 1][546/834] loss: 0.7223 PSNR_train: 32.0131\n",
      "[epoch 1][547/834] loss: 0.7159 PSNR_train: 31.8442\n",
      "[epoch 1][548/834] loss: 0.7573 PSNR_train: 31.6842\n",
      "[epoch 1][549/834] loss: 0.7158 PSNR_train: 31.8430\n",
      "[epoch 1][550/834] loss: 0.7378 PSNR_train: 31.8631\n",
      "[epoch 1][551/834] loss: 0.7580 PSNR_train: 31.6485\n",
      "[epoch 1][552/834] loss: 0.7449 PSNR_train: 31.7592\n",
      "[epoch 1][553/834] loss: 0.7306 PSNR_train: 31.7251\n",
      "[epoch 1][554/834] loss: 0.7228 PSNR_train: 31.9472\n",
      "[epoch 1][555/834] loss: 0.6929 PSNR_train: 32.0709\n",
      "[epoch 1][556/834] loss: 0.7330 PSNR_train: 31.8830\n",
      "[epoch 1][557/834] loss: 0.7677 PSNR_train: 31.6729\n",
      "[epoch 1][558/834] loss: 0.7088 PSNR_train: 31.9845\n",
      "[epoch 1][559/834] loss: 0.7152 PSNR_train: 31.9135\n",
      "[epoch 1][560/834] loss: 0.7175 PSNR_train: 31.8857\n",
      "[epoch 1][561/834] loss: 0.7218 PSNR_train: 31.7662\n",
      "[epoch 1][562/834] loss: 0.7175 PSNR_train: 31.9139\n",
      "[epoch 1][563/834] loss: 0.7298 PSNR_train: 31.9071\n",
      "[epoch 1][564/834] loss: 0.7094 PSNR_train: 31.8336\n",
      "[epoch 1][565/834] loss: 0.7250 PSNR_train: 31.8175\n",
      "[epoch 1][566/834] loss: 0.7651 PSNR_train: 31.5495\n",
      "[epoch 1][567/834] loss: 0.7069 PSNR_train: 31.8864\n",
      "[epoch 1][568/834] loss: 0.7032 PSNR_train: 31.8598\n",
      "[epoch 1][569/834] loss: 0.7596 PSNR_train: 31.6894\n",
      "[epoch 1][570/834] loss: 0.7035 PSNR_train: 31.8737\n",
      "[epoch 1][571/834] loss: 0.7334 PSNR_train: 31.6591\n",
      "[epoch 1][572/834] loss: 0.7312 PSNR_train: 31.6469\n",
      "[epoch 1][573/834] loss: 0.7210 PSNR_train: 31.6558\n",
      "[epoch 1][574/834] loss: 0.7055 PSNR_train: 31.6597\n",
      "[epoch 1][575/834] loss: 0.7520 PSNR_train: 31.6210\n",
      "[epoch 1][576/834] loss: 0.6980 PSNR_train: 32.0468\n",
      "[epoch 1][577/834] loss: 0.7287 PSNR_train: 31.8314\n",
      "[epoch 1][578/834] loss: 0.7046 PSNR_train: 31.9433\n",
      "[epoch 1][579/834] loss: 0.7085 PSNR_train: 31.9102\n",
      "[epoch 1][580/834] loss: 0.7257 PSNR_train: 31.8344\n",
      "[epoch 1][581/834] loss: 0.7242 PSNR_train: 31.7840\n",
      "[epoch 1][582/834] loss: 0.7176 PSNR_train: 31.8152\n",
      "[epoch 1][583/834] loss: 0.6833 PSNR_train: 32.0743\n",
      "[epoch 1][584/834] loss: 0.7454 PSNR_train: 31.6845\n",
      "[epoch 1][585/834] loss: 0.7023 PSNR_train: 31.8420\n",
      "[epoch 1][586/834] loss: 0.7319 PSNR_train: 31.7400\n",
      "[epoch 1][587/834] loss: 0.7604 PSNR_train: 31.4571\n",
      "[epoch 1][588/834] loss: 0.7058 PSNR_train: 31.7971\n",
      "[epoch 1][589/834] loss: 0.7091 PSNR_train: 31.7717\n",
      "[epoch 1][590/834] loss: 0.7124 PSNR_train: 31.7933\n",
      "[epoch 1][591/834] loss: 0.6948 PSNR_train: 31.9053\n",
      "[epoch 1][592/834] loss: 0.6907 PSNR_train: 32.0693\n",
      "[epoch 1][593/834] loss: 0.7557 PSNR_train: 31.7009\n",
      "[epoch 1][594/834] loss: 0.6802 PSNR_train: 32.2051\n",
      "[epoch 1][595/834] loss: 0.7362 PSNR_train: 31.8016\n",
      "[epoch 1][596/834] loss: 0.6821 PSNR_train: 32.0644\n",
      "[epoch 1][597/834] loss: 0.7178 PSNR_train: 31.9755\n",
      "[epoch 1][598/834] loss: 0.6819 PSNR_train: 32.1574\n",
      "[epoch 1][599/834] loss: 0.7026 PSNR_train: 31.9864\n",
      "[epoch 1][600/834] loss: 0.7344 PSNR_train: 31.8447\n",
      "[epoch 1][601/834] loss: 0.6959 PSNR_train: 32.0888\n",
      "[epoch 1][602/834] loss: 0.6860 PSNR_train: 32.3145\n",
      "[epoch 1][603/834] loss: 0.7190 PSNR_train: 32.1376\n",
      "[epoch 1][604/834] loss: 0.7458 PSNR_train: 31.8421\n",
      "[epoch 1][605/834] loss: 0.7164 PSNR_train: 31.9930\n",
      "[epoch 1][606/834] loss: 0.7371 PSNR_train: 31.6963\n",
      "[epoch 1][607/834] loss: 0.6953 PSNR_train: 31.9512\n",
      "[epoch 1][608/834] loss: 0.6923 PSNR_train: 31.8294\n",
      "[epoch 1][609/834] loss: 0.7245 PSNR_train: 31.6211\n",
      "[epoch 1][610/834] loss: 0.7219 PSNR_train: 31.6466\n",
      "[epoch 1][611/834] loss: 0.7239 PSNR_train: 31.6447\n",
      "[epoch 1][612/834] loss: 0.7127 PSNR_train: 31.7406\n",
      "[epoch 1][613/834] loss: 0.6993 PSNR_train: 31.9370\n",
      "[epoch 1][614/834] loss: 0.7168 PSNR_train: 31.8214\n",
      "[epoch 1][615/834] loss: 0.7398 PSNR_train: 31.6870\n",
      "[epoch 1][616/834] loss: 0.6870 PSNR_train: 32.1740\n",
      "[epoch 1][617/834] loss: 0.6963 PSNR_train: 32.0641\n",
      "[epoch 1][618/834] loss: 0.6831 PSNR_train: 32.1108\n",
      "[epoch 1][619/834] loss: 0.7157 PSNR_train: 31.8725\n",
      "[epoch 1][620/834] loss: 0.7118 PSNR_train: 31.9378\n",
      "[epoch 1][621/834] loss: 0.7103 PSNR_train: 31.9622\n",
      "[epoch 1][622/834] loss: 0.6867 PSNR_train: 32.1458\n",
      "[epoch 1][623/834] loss: 0.6821 PSNR_train: 32.2331\n",
      "[epoch 1][624/834] loss: 0.7115 PSNR_train: 31.8819\n",
      "[epoch 1][625/834] loss: 0.6914 PSNR_train: 31.9773\n",
      "[epoch 1][626/834] loss: 0.7176 PSNR_train: 31.8620\n",
      "[epoch 1][627/834] loss: 0.7383 PSNR_train: 31.7112\n",
      "[epoch 1][628/834] loss: 0.6778 PSNR_train: 32.0960\n",
      "[epoch 1][629/834] loss: 0.6627 PSNR_train: 32.0913\n",
      "[epoch 1][630/834] loss: 0.6759 PSNR_train: 32.0147\n",
      "[epoch 1][631/834] loss: 0.6749 PSNR_train: 31.9624\n",
      "[epoch 1][632/834] loss: 0.6462 PSNR_train: 32.2965\n",
      "[epoch 1][633/834] loss: 0.7136 PSNR_train: 31.8009\n",
      "[epoch 1][634/834] loss: 0.6826 PSNR_train: 31.9724\n",
      "[epoch 1][635/834] loss: 0.6879 PSNR_train: 32.0264\n",
      "[epoch 1][636/834] loss: 0.6581 PSNR_train: 32.1914\n",
      "[epoch 1][637/834] loss: 0.7309 PSNR_train: 31.8129\n",
      "[epoch 1][638/834] loss: 0.6905 PSNR_train: 32.0498\n",
      "[epoch 1][639/834] loss: 0.7052 PSNR_train: 31.9434\n",
      "[epoch 1][640/834] loss: 0.7190 PSNR_train: 31.8591\n",
      "[epoch 1][641/834] loss: 0.7027 PSNR_train: 31.9797\n",
      "[epoch 1][642/834] loss: 0.6847 PSNR_train: 32.1391\n",
      "[epoch 1][643/834] loss: 0.6897 PSNR_train: 32.0597\n",
      "[epoch 1][644/834] loss: 0.7026 PSNR_train: 31.9962\n",
      "[epoch 1][645/834] loss: 0.7235 PSNR_train: 31.8731\n",
      "[epoch 1][646/834] loss: 0.6622 PSNR_train: 32.1246\n",
      "[epoch 1][647/834] loss: 0.6597 PSNR_train: 32.1755\n",
      "[epoch 1][648/834] loss: 0.6878 PSNR_train: 32.0764\n",
      "[epoch 1][649/834] loss: 0.6837 PSNR_train: 32.1260\n",
      "[epoch 1][650/834] loss: 0.6983 PSNR_train: 32.0276\n",
      "[epoch 1][651/834] loss: 0.7085 PSNR_train: 31.9660\n",
      "[epoch 1][652/834] loss: 0.7173 PSNR_train: 31.8964\n",
      "[epoch 1][653/834] loss: 0.6469 PSNR_train: 32.4705\n",
      "[epoch 1][654/834] loss: 0.7000 PSNR_train: 31.9914\n",
      "[epoch 1][655/834] loss: 0.6950 PSNR_train: 31.9609\n",
      "[epoch 1][656/834] loss: 0.6770 PSNR_train: 32.1188\n",
      "[epoch 1][657/834] loss: 0.7043 PSNR_train: 31.8728\n",
      "[epoch 1][658/834] loss: 0.6607 PSNR_train: 32.0735\n",
      "[epoch 1][659/834] loss: 0.6524 PSNR_train: 32.1792\n",
      "[epoch 1][660/834] loss: 0.6607 PSNR_train: 32.0191\n",
      "[epoch 1][661/834] loss: 0.7249 PSNR_train: 31.6007\n",
      "[epoch 1][662/834] loss: 0.7181 PSNR_train: 31.7591\n",
      "[epoch 1][663/834] loss: 0.7443 PSNR_train: 31.7892\n",
      "[epoch 1][664/834] loss: 0.7124 PSNR_train: 31.8489\n",
      "[epoch 1][665/834] loss: 0.6613 PSNR_train: 32.2481\n",
      "[epoch 1][666/834] loss: 0.7061 PSNR_train: 32.0785\n",
      "[epoch 1][667/834] loss: 0.6475 PSNR_train: 32.3464\n",
      "[epoch 1][668/834] loss: 0.6659 PSNR_train: 32.3332\n",
      "[epoch 1][669/834] loss: 0.6717 PSNR_train: 32.2035\n",
      "[epoch 1][670/834] loss: 0.6738 PSNR_train: 32.2127\n",
      "[epoch 1][671/834] loss: 0.6739 PSNR_train: 32.2621\n",
      "[epoch 1][672/834] loss: 0.6953 PSNR_train: 32.0693\n",
      "[epoch 1][673/834] loss: 0.6738 PSNR_train: 32.2408\n",
      "[epoch 1][674/834] loss: 0.6786 PSNR_train: 32.1551\n",
      "[epoch 1][675/834] loss: 0.6410 PSNR_train: 32.4069\n",
      "[epoch 1][676/834] loss: 0.6860 PSNR_train: 32.0791\n",
      "[epoch 1][677/834] loss: 0.6767 PSNR_train: 32.1174\n",
      "[epoch 1][678/834] loss: 0.6739 PSNR_train: 32.0833\n",
      "[epoch 1][679/834] loss: 0.6859 PSNR_train: 31.8829\n",
      "[epoch 1][680/834] loss: 0.6874 PSNR_train: 31.9813\n",
      "[epoch 1][681/834] loss: 0.6516 PSNR_train: 32.0291\n",
      "[epoch 1][682/834] loss: 0.7023 PSNR_train: 31.8176\n",
      "[epoch 1][683/834] loss: 0.6934 PSNR_train: 31.9190\n",
      "[epoch 1][684/834] loss: 0.6889 PSNR_train: 31.9711\n",
      "[epoch 1][685/834] loss: 0.6763 PSNR_train: 32.0033\n",
      "[epoch 1][686/834] loss: 0.6863 PSNR_train: 32.0963\n",
      "[epoch 1][687/834] loss: 0.6788 PSNR_train: 32.1220\n",
      "[epoch 1][688/834] loss: 0.7108 PSNR_train: 32.0216\n",
      "[epoch 1][689/834] loss: 0.6263 PSNR_train: 32.4903\n",
      "[epoch 1][690/834] loss: 0.6993 PSNR_train: 31.9304\n",
      "[epoch 1][691/834] loss: 0.7126 PSNR_train: 31.8540\n",
      "[epoch 1][692/834] loss: 0.6586 PSNR_train: 32.3667\n",
      "[epoch 1][693/834] loss: 0.6946 PSNR_train: 32.0052\n",
      "[epoch 1][694/834] loss: 0.6595 PSNR_train: 32.2602\n",
      "[epoch 1][695/834] loss: 0.6875 PSNR_train: 32.2615\n",
      "[epoch 1][696/834] loss: 0.6434 PSNR_train: 32.3356\n",
      "[epoch 1][697/834] loss: 0.6463 PSNR_train: 32.4243\n",
      "[epoch 1][698/834] loss: 0.6751 PSNR_train: 32.1296\n",
      "[epoch 1][699/834] loss: 0.6822 PSNR_train: 32.0926\n",
      "[epoch 1][700/834] loss: 0.6744 PSNR_train: 32.1790\n",
      "[epoch 1][701/834] loss: 0.6638 PSNR_train: 32.2918\n",
      "[epoch 1][702/834] loss: 0.7010 PSNR_train: 31.9340\n",
      "[epoch 1][703/834] loss: 0.6935 PSNR_train: 32.0925\n",
      "[epoch 1][704/834] loss: 0.6792 PSNR_train: 32.2483\n",
      "[epoch 1][705/834] loss: 0.6609 PSNR_train: 32.3518\n",
      "[epoch 1][706/834] loss: 0.6976 PSNR_train: 32.0057\n",
      "[epoch 1][707/834] loss: 0.6593 PSNR_train: 32.2719\n",
      "[epoch 1][708/834] loss: 0.6686 PSNR_train: 32.3028\n",
      "[epoch 1][709/834] loss: 0.6394 PSNR_train: 32.4792\n",
      "[epoch 1][710/834] loss: 0.6951 PSNR_train: 31.9697\n",
      "[epoch 1][711/834] loss: 0.5972 PSNR_train: 32.6212\n",
      "[epoch 1][712/834] loss: 0.6972 PSNR_train: 31.9798\n",
      "[epoch 1][713/834] loss: 0.6749 PSNR_train: 32.0486\n",
      "[epoch 1][714/834] loss: 0.6655 PSNR_train: 32.3127\n",
      "[epoch 1][715/834] loss: 0.6654 PSNR_train: 32.3980\n",
      "[epoch 1][716/834] loss: 0.6347 PSNR_train: 32.4587\n",
      "[epoch 1][717/834] loss: 0.7168 PSNR_train: 32.0044\n",
      "[epoch 1][718/834] loss: 0.6439 PSNR_train: 32.4129\n",
      "[epoch 1][719/834] loss: 0.6458 PSNR_train: 32.4605\n",
      "[epoch 1][720/834] loss: 0.6683 PSNR_train: 32.2793\n",
      "[epoch 1][721/834] loss: 0.6874 PSNR_train: 31.9856\n",
      "[epoch 1][722/834] loss: 0.6408 PSNR_train: 32.2621\n",
      "[epoch 1][723/834] loss: 0.6902 PSNR_train: 32.0671\n",
      "[epoch 1][724/834] loss: 0.6444 PSNR_train: 32.3703\n",
      "[epoch 1][725/834] loss: 0.7354 PSNR_train: 31.7032\n",
      "[epoch 1][726/834] loss: 0.6184 PSNR_train: 32.5454\n",
      "[epoch 1][727/834] loss: 0.6428 PSNR_train: 32.4828\n",
      "[epoch 1][728/834] loss: 0.6221 PSNR_train: 32.6157\n",
      "[epoch 1][729/834] loss: 0.6341 PSNR_train: 32.5088\n",
      "[epoch 1][730/834] loss: 0.6323 PSNR_train: 32.4937\n",
      "[epoch 1][731/834] loss: 0.6262 PSNR_train: 32.6308\n",
      "[epoch 1][732/834] loss: 0.6867 PSNR_train: 32.1187\n",
      "[epoch 1][733/834] loss: 0.6632 PSNR_train: 32.3267\n",
      "[epoch 1][734/834] loss: 0.6159 PSNR_train: 32.5700\n",
      "[epoch 1][735/834] loss: 0.6225 PSNR_train: 32.5342\n",
      "[epoch 1][736/834] loss: 0.6555 PSNR_train: 32.2936\n",
      "[epoch 1][737/834] loss: 0.6534 PSNR_train: 32.1610\n",
      "[epoch 1][738/834] loss: 0.6815 PSNR_train: 31.8587\n",
      "[epoch 1][739/834] loss: 0.6053 PSNR_train: 32.3357\n",
      "[epoch 1][740/834] loss: 0.6823 PSNR_train: 31.9043\n",
      "[epoch 1][741/834] loss: 0.6993 PSNR_train: 31.9267\n",
      "[epoch 1][742/834] loss: 0.6628 PSNR_train: 32.1949\n",
      "[epoch 1][743/834] loss: 0.6392 PSNR_train: 32.3669\n",
      "[epoch 1][744/834] loss: 0.7195 PSNR_train: 31.9706\n",
      "[epoch 1][745/834] loss: 0.6335 PSNR_train: 32.4022\n",
      "[epoch 1][746/834] loss: 0.6640 PSNR_train: 32.2851\n",
      "[epoch 1][747/834] loss: 0.6681 PSNR_train: 32.3445\n",
      "[epoch 1][748/834] loss: 0.6319 PSNR_train: 32.6234\n",
      "[epoch 1][749/834] loss: 0.7061 PSNR_train: 32.0880\n",
      "[epoch 1][750/834] loss: 0.6782 PSNR_train: 32.3329\n",
      "[epoch 1][751/834] loss: 0.6488 PSNR_train: 32.6187\n",
      "[epoch 1][752/834] loss: 0.6786 PSNR_train: 32.2297\n",
      "[epoch 1][753/834] loss: 0.6746 PSNR_train: 32.2115\n",
      "[epoch 1][754/834] loss: 0.6808 PSNR_train: 32.1531\n",
      "[epoch 1][755/834] loss: 0.6731 PSNR_train: 32.2141\n",
      "[epoch 1][756/834] loss: 0.6248 PSNR_train: 32.6243\n",
      "[epoch 1][757/834] loss: 0.6411 PSNR_train: 32.4005\n",
      "[epoch 1][758/834] loss: 0.6618 PSNR_train: 32.4622\n",
      "[epoch 1][759/834] loss: 0.6900 PSNR_train: 32.1058\n",
      "[epoch 1][760/834] loss: 0.6908 PSNR_train: 32.0925\n",
      "[epoch 1][761/834] loss: 0.6278 PSNR_train: 32.4744\n",
      "[epoch 1][762/834] loss: 0.6698 PSNR_train: 32.1816\n",
      "[epoch 1][763/834] loss: 0.6808 PSNR_train: 32.1122\n",
      "[epoch 1][764/834] loss: 0.6479 PSNR_train: 32.1538\n",
      "[epoch 1][765/834] loss: 0.6457 PSNR_train: 32.1397\n",
      "[epoch 1][766/834] loss: 0.6303 PSNR_train: 32.2110\n",
      "[epoch 1][767/834] loss: 0.6755 PSNR_train: 31.9212\n",
      "[epoch 1][768/834] loss: 0.6412 PSNR_train: 32.1239\n",
      "[epoch 1][769/834] loss: 0.6970 PSNR_train: 31.8783\n",
      "[epoch 1][770/834] loss: 0.6900 PSNR_train: 31.8849\n",
      "[epoch 1][771/834] loss: 0.6513 PSNR_train: 32.0377\n",
      "[epoch 1][772/834] loss: 0.6783 PSNR_train: 31.8130\n",
      "[epoch 1][773/834] loss: 0.6465 PSNR_train: 32.0979\n",
      "[epoch 1][774/834] loss: 0.6896 PSNR_train: 32.0030\n",
      "[epoch 1][775/834] loss: 0.5894 PSNR_train: 32.6095\n",
      "[epoch 1][776/834] loss: 0.6383 PSNR_train: 32.2472\n",
      "[epoch 1][777/834] loss: 0.6297 PSNR_train: 32.1711\n",
      "[epoch 1][778/834] loss: 0.6609 PSNR_train: 31.8641\n",
      "[epoch 1][779/834] loss: 0.5882 PSNR_train: 32.3101\n",
      "[epoch 1][780/834] loss: 0.6539 PSNR_train: 31.9141\n",
      "[epoch 1][781/834] loss: 0.6439 PSNR_train: 31.9944\n",
      "[epoch 1][782/834] loss: 0.6663 PSNR_train: 31.9291\n",
      "[epoch 1][783/834] loss: 0.6449 PSNR_train: 32.1103\n",
      "[epoch 1][784/834] loss: 0.6751 PSNR_train: 32.1196\n",
      "[epoch 1][785/834] loss: 0.6265 PSNR_train: 32.4261\n",
      "[epoch 1][786/834] loss: 0.6670 PSNR_train: 32.1414\n",
      "[epoch 1][787/834] loss: 0.6391 PSNR_train: 32.3877\n",
      "[epoch 1][788/834] loss: 0.6228 PSNR_train: 32.5523\n",
      "[epoch 1][789/834] loss: 0.6551 PSNR_train: 32.2375\n",
      "[epoch 1][790/834] loss: 0.6455 PSNR_train: 32.4852\n",
      "[epoch 1][791/834] loss: 0.6243 PSNR_train: 32.5155\n",
      "[epoch 1][792/834] loss: 0.6275 PSNR_train: 32.5432\n",
      "[epoch 1][793/834] loss: 0.6897 PSNR_train: 32.0892\n",
      "[epoch 1][794/834] loss: 0.6717 PSNR_train: 32.2654\n",
      "[epoch 1][795/834] loss: 0.6326 PSNR_train: 32.5115\n",
      "[epoch 1][796/834] loss: 0.6091 PSNR_train: 32.6162\n",
      "[epoch 1][797/834] loss: 0.6455 PSNR_train: 32.2269\n",
      "[epoch 1][798/834] loss: 0.6381 PSNR_train: 32.3571\n",
      "[epoch 1][799/834] loss: 0.6446 PSNR_train: 32.2765\n",
      "[epoch 1][800/834] loss: 0.6589 PSNR_train: 32.1816\n",
      "[epoch 1][801/834] loss: 0.6454 PSNR_train: 32.2517\n",
      "[epoch 1][802/834] loss: 0.6582 PSNR_train: 32.4494\n",
      "[epoch 1][803/834] loss: 0.6282 PSNR_train: 32.5601\n",
      "[epoch 1][804/834] loss: 0.6712 PSNR_train: 32.2857\n",
      "[epoch 1][805/834] loss: 0.6691 PSNR_train: 32.3292\n",
      "[epoch 1][806/834] loss: 0.6395 PSNR_train: 32.6982\n",
      "[epoch 1][807/834] loss: 0.6037 PSNR_train: 32.7127\n",
      "[epoch 1][808/834] loss: 0.6633 PSNR_train: 32.1364\n",
      "[epoch 1][809/834] loss: 0.6691 PSNR_train: 32.0593\n",
      "[epoch 1][810/834] loss: 0.6684 PSNR_train: 32.2122\n",
      "[epoch 1][811/834] loss: 0.7048 PSNR_train: 31.8783\n",
      "[epoch 1][812/834] loss: 0.6611 PSNR_train: 32.2285\n",
      "[epoch 1][813/834] loss: 0.6537 PSNR_train: 32.3639\n",
      "[epoch 1][814/834] loss: 0.6509 PSNR_train: 32.3286\n",
      "[epoch 1][815/834] loss: 0.6542 PSNR_train: 32.2827\n",
      "[epoch 1][816/834] loss: 0.6553 PSNR_train: 32.2351\n",
      "[epoch 1][817/834] loss: 0.6668 PSNR_train: 32.0948\n",
      "[epoch 1][818/834] loss: 0.6248 PSNR_train: 32.2776\n",
      "[epoch 1][819/834] loss: 0.6091 PSNR_train: 32.4912\n",
      "[epoch 1][820/834] loss: 0.6125 PSNR_train: 32.5217\n",
      "[epoch 1][821/834] loss: 0.6327 PSNR_train: 32.3130\n",
      "[epoch 1][822/834] loss: 0.6421 PSNR_train: 32.3052\n",
      "[epoch 1][823/834] loss: 0.6147 PSNR_train: 32.5213\n",
      "[epoch 1][824/834] loss: 0.7160 PSNR_train: 31.8416\n",
      "[epoch 1][825/834] loss: 0.6448 PSNR_train: 32.4811\n",
      "[epoch 1][826/834] loss: 0.6786 PSNR_train: 32.1235\n",
      "[epoch 1][827/834] loss: 0.6445 PSNR_train: 32.3087\n",
      "[epoch 1][828/834] loss: 0.6924 PSNR_train: 32.0312\n",
      "[epoch 1][829/834] loss: 0.6441 PSNR_train: 32.3621\n",
      "[epoch 1][830/834] loss: 0.6297 PSNR_train: 32.4645\n",
      "[epoch 1][831/834] loss: 0.6469 PSNR_train: 32.3151\n",
      "[epoch 1][832/834] loss: 0.6733 PSNR_train: 32.2437\n",
      "[epoch 1][833/834] loss: 0.7052 PSNR_train: 31.7262\n",
      "[epoch 1][834/834] loss: 0.5860 PSNR_train: 32.6418\n",
      "\n",
      "[epoch 1] PSNR_val: 35.3416\n",
      "learning rate 0.000010\n",
      "[epoch 2][1/834] loss: 0.6486 PSNR_train: 32.2141\n",
      "[epoch 2][2/834] loss: 0.6523 PSNR_train: 32.2415\n",
      "[epoch 2][3/834] loss: 0.6889 PSNR_train: 32.0020\n",
      "[epoch 2][4/834] loss: 0.6141 PSNR_train: 32.5654\n",
      "[epoch 2][5/834] loss: 0.6414 PSNR_train: 32.3591\n",
      "[epoch 2][6/834] loss: 0.6344 PSNR_train: 32.3497\n",
      "[epoch 2][7/834] loss: 0.6289 PSNR_train: 32.4737\n",
      "[epoch 2][8/834] loss: 0.6130 PSNR_train: 32.5491\n",
      "[epoch 2][9/834] loss: 0.6161 PSNR_train: 32.5767\n",
      "[epoch 2][10/834] loss: 0.6405 PSNR_train: 32.3381\n",
      "[epoch 2][11/834] loss: 0.6306 PSNR_train: 32.5250\n",
      "[epoch 2][12/834] loss: 0.6449 PSNR_train: 32.2806\n",
      "[epoch 2][13/834] loss: 0.6265 PSNR_train: 32.5407\n",
      "[epoch 2][14/834] loss: 0.6792 PSNR_train: 32.1325\n",
      "[epoch 2][15/834] loss: 0.6098 PSNR_train: 32.7133\n",
      "[epoch 2][16/834] loss: 0.6215 PSNR_train: 32.6093\n",
      "[epoch 2][17/834] loss: 0.6780 PSNR_train: 32.1479\n",
      "[epoch 2][18/834] loss: 0.6344 PSNR_train: 32.4763\n",
      "[epoch 2][19/834] loss: 0.6380 PSNR_train: 32.3715\n",
      "[epoch 2][20/834] loss: 0.6094 PSNR_train: 32.6523\n",
      "[epoch 2][21/834] loss: 0.6425 PSNR_train: 32.3914\n",
      "[epoch 2][22/834] loss: 0.6460 PSNR_train: 32.4336\n",
      "[epoch 2][23/834] loss: 0.5969 PSNR_train: 32.6355\n",
      "[epoch 2][24/834] loss: 0.6511 PSNR_train: 32.2599\n",
      "[epoch 2][25/834] loss: 0.6110 PSNR_train: 32.6707\n",
      "[epoch 2][26/834] loss: 0.6517 PSNR_train: 32.2664\n",
      "[epoch 2][27/834] loss: 0.6703 PSNR_train: 32.2549\n",
      "[epoch 2][28/834] loss: 0.6342 PSNR_train: 32.3458\n",
      "[epoch 2][29/834] loss: 0.6538 PSNR_train: 32.2925\n",
      "[epoch 2][30/834] loss: 0.6394 PSNR_train: 32.3838\n",
      "[epoch 2][31/834] loss: 0.6177 PSNR_train: 32.5311\n",
      "[epoch 2][32/834] loss: 0.6225 PSNR_train: 32.4805\n",
      "[epoch 2][33/834] loss: 0.6108 PSNR_train: 32.6123\n",
      "[epoch 2][34/834] loss: 0.7153 PSNR_train: 32.0315\n",
      "[epoch 2][35/834] loss: 0.6202 PSNR_train: 32.4723\n",
      "[epoch 2][36/834] loss: 0.6281 PSNR_train: 32.5266\n",
      "[epoch 2][37/834] loss: 0.6070 PSNR_train: 32.6498\n",
      "[epoch 2][38/834] loss: 0.6196 PSNR_train: 32.5508\n",
      "[epoch 2][39/834] loss: 0.6727 PSNR_train: 32.1682\n",
      "[epoch 2][40/834] loss: 0.6539 PSNR_train: 32.2849\n",
      "[epoch 2][41/834] loss: 0.6202 PSNR_train: 32.4161\n",
      "[epoch 2][42/834] loss: 0.6643 PSNR_train: 32.2801\n",
      "[epoch 2][43/834] loss: 0.6514 PSNR_train: 32.2823\n",
      "[epoch 2][44/834] loss: 0.6134 PSNR_train: 32.6142\n",
      "[epoch 2][45/834] loss: 0.6607 PSNR_train: 32.2615\n",
      "[epoch 2][46/834] loss: 0.6415 PSNR_train: 32.4120\n",
      "[epoch 2][47/834] loss: 0.6387 PSNR_train: 32.3427\n",
      "[epoch 2][48/834] loss: 0.6272 PSNR_train: 32.4634\n",
      "[epoch 2][49/834] loss: 0.6814 PSNR_train: 32.1323\n",
      "[epoch 2][50/834] loss: 0.7037 PSNR_train: 31.8985\n",
      "[epoch 2][51/834] loss: 0.6485 PSNR_train: 32.4166\n",
      "[epoch 2][52/834] loss: 0.6578 PSNR_train: 32.3896\n",
      "[epoch 2][53/834] loss: 0.6164 PSNR_train: 32.4934\n",
      "[epoch 2][54/834] loss: 0.6359 PSNR_train: 32.4658\n",
      "[epoch 2][55/834] loss: 0.6643 PSNR_train: 32.1663\n",
      "[epoch 2][56/834] loss: 0.6030 PSNR_train: 32.7535\n",
      "[epoch 2][57/834] loss: 0.6553 PSNR_train: 32.3060\n",
      "[epoch 2][58/834] loss: 0.6136 PSNR_train: 32.5913\n",
      "[epoch 2][59/834] loss: 0.6061 PSNR_train: 32.7106\n",
      "[epoch 2][60/834] loss: 0.6552 PSNR_train: 32.3630\n",
      "[epoch 2][61/834] loss: 0.6604 PSNR_train: 32.3247\n",
      "[epoch 2][62/834] loss: 0.6444 PSNR_train: 32.5525\n",
      "[epoch 2][63/834] loss: 0.6366 PSNR_train: 32.4817\n",
      "[epoch 2][64/834] loss: 0.6847 PSNR_train: 32.1674\n",
      "[epoch 2][65/834] loss: 0.6649 PSNR_train: 32.3619\n",
      "[epoch 2][66/834] loss: 0.6537 PSNR_train: 32.4024\n",
      "[epoch 2][67/834] loss: 0.6583 PSNR_train: 32.3271\n",
      "[epoch 2][68/834] loss: 0.6217 PSNR_train: 32.5596\n",
      "[epoch 2][69/834] loss: 0.6310 PSNR_train: 32.4687\n",
      "[epoch 2][70/834] loss: 0.6451 PSNR_train: 32.4250\n",
      "[epoch 2][71/834] loss: 0.6345 PSNR_train: 32.5516\n",
      "[epoch 2][72/834] loss: 0.6187 PSNR_train: 32.6009\n",
      "[epoch 2][73/834] loss: 0.6414 PSNR_train: 32.4184\n",
      "[epoch 2][74/834] loss: 0.6543 PSNR_train: 32.3037\n",
      "[epoch 2][75/834] loss: 0.5927 PSNR_train: 32.8516\n",
      "[epoch 2][76/834] loss: 0.6348 PSNR_train: 32.3971\n",
      "[epoch 2][77/834] loss: 0.6343 PSNR_train: 32.4247\n",
      "[epoch 2][78/834] loss: 0.6052 PSNR_train: 32.6164\n",
      "[epoch 2][79/834] loss: 0.6370 PSNR_train: 32.4458\n",
      "[epoch 2][80/834] loss: 0.6724 PSNR_train: 32.2211\n",
      "[epoch 2][81/834] loss: 0.6034 PSNR_train: 32.7726\n",
      "[epoch 2][82/834] loss: 0.6234 PSNR_train: 32.4153\n",
      "[epoch 2][83/834] loss: 0.5842 PSNR_train: 32.8129\n",
      "[epoch 2][84/834] loss: 0.6100 PSNR_train: 32.6534\n",
      "[epoch 2][85/834] loss: 0.6562 PSNR_train: 32.2911\n",
      "[epoch 2][86/834] loss: 0.6717 PSNR_train: 32.2364\n",
      "[epoch 2][87/834] loss: 0.6514 PSNR_train: 32.3967\n",
      "[epoch 2][88/834] loss: 0.6038 PSNR_train: 32.7696\n",
      "[epoch 2][89/834] loss: 0.6187 PSNR_train: 32.6057\n",
      "[epoch 2][90/834] loss: 0.6479 PSNR_train: 32.3035\n",
      "[epoch 2][91/834] loss: 0.6085 PSNR_train: 32.6703\n",
      "[epoch 2][92/834] loss: 0.6458 PSNR_train: 32.2748\n",
      "[epoch 2][93/834] loss: 0.6500 PSNR_train: 32.3067\n",
      "[epoch 2][94/834] loss: 0.6455 PSNR_train: 32.3718\n",
      "[epoch 2][95/834] loss: 0.6352 PSNR_train: 32.5043\n",
      "[epoch 2][96/834] loss: 0.6247 PSNR_train: 32.5715\n",
      "[epoch 2][97/834] loss: 0.6285 PSNR_train: 32.4119\n",
      "[epoch 2][98/834] loss: 0.6115 PSNR_train: 32.5542\n",
      "[epoch 2][99/834] loss: 0.6298 PSNR_train: 32.5533\n",
      "[epoch 2][100/834] loss: 0.6630 PSNR_train: 32.1675\n",
      "[epoch 2][101/834] loss: 0.6096 PSNR_train: 32.7300\n",
      "[epoch 2][102/834] loss: 0.6254 PSNR_train: 32.5481\n",
      "[epoch 2][103/834] loss: 0.6183 PSNR_train: 32.6388\n",
      "[epoch 2][104/834] loss: 0.6353 PSNR_train: 32.4277\n",
      "[epoch 2][105/834] loss: 0.6337 PSNR_train: 32.4191\n",
      "[epoch 2][106/834] loss: 0.6076 PSNR_train: 32.6963\n",
      "[epoch 2][107/834] loss: 0.5925 PSNR_train: 32.5787\n",
      "[epoch 2][108/834] loss: 0.6344 PSNR_train: 32.3882\n",
      "[epoch 2][109/834] loss: 0.5968 PSNR_train: 32.6496\n",
      "[epoch 2][110/834] loss: 0.7217 PSNR_train: 31.8926\n",
      "[epoch 2][111/834] loss: 0.5864 PSNR_train: 32.7033\n",
      "[epoch 2][112/834] loss: 0.6343 PSNR_train: 32.3362\n",
      "[epoch 2][113/834] loss: 0.6727 PSNR_train: 32.1499\n",
      "[epoch 2][114/834] loss: 0.6296 PSNR_train: 32.4719\n",
      "[epoch 2][115/834] loss: 0.6496 PSNR_train: 32.3814\n",
      "[epoch 2][116/834] loss: 0.6005 PSNR_train: 32.6969\n",
      "[epoch 2][117/834] loss: 0.5990 PSNR_train: 32.7086\n",
      "[epoch 2][118/834] loss: 0.6114 PSNR_train: 32.6242\n",
      "[epoch 2][119/834] loss: 0.6450 PSNR_train: 32.3502\n",
      "[epoch 2][120/834] loss: 0.6323 PSNR_train: 32.3872\n",
      "[epoch 2][121/834] loss: 0.6625 PSNR_train: 32.1287\n",
      "[epoch 2][122/834] loss: 0.6713 PSNR_train: 32.2776\n",
      "[epoch 2][123/834] loss: 0.5955 PSNR_train: 32.7718\n",
      "[epoch 2][124/834] loss: 0.5902 PSNR_train: 32.8851\n",
      "[epoch 2][125/834] loss: 0.6792 PSNR_train: 32.0543\n",
      "[epoch 2][126/834] loss: 0.6225 PSNR_train: 32.4949\n",
      "[epoch 2][127/834] loss: 0.6444 PSNR_train: 32.2767\n",
      "[epoch 2][128/834] loss: 0.6745 PSNR_train: 32.1146\n",
      "[epoch 2][129/834] loss: 0.6393 PSNR_train: 32.3781\n",
      "[epoch 2][130/834] loss: 0.6389 PSNR_train: 32.5486\n",
      "[epoch 2][131/834] loss: 0.6527 PSNR_train: 32.4007\n",
      "[epoch 2][132/834] loss: 0.6034 PSNR_train: 32.6423\n",
      "[epoch 2][133/834] loss: 0.6388 PSNR_train: 32.4518\n",
      "[epoch 2][134/834] loss: 0.6395 PSNR_train: 32.5066\n",
      "[epoch 2][135/834] loss: 0.5999 PSNR_train: 32.8096\n",
      "[epoch 2][136/834] loss: 0.6275 PSNR_train: 32.5028\n",
      "[epoch 2][137/834] loss: 0.6193 PSNR_train: 32.5109\n",
      "[epoch 2][138/834] loss: 0.6352 PSNR_train: 32.3460\n",
      "[epoch 2][139/834] loss: 0.6092 PSNR_train: 32.7451\n",
      "[epoch 2][140/834] loss: 0.5850 PSNR_train: 32.8596\n",
      "[epoch 2][141/834] loss: 0.6641 PSNR_train: 32.2313\n",
      "[epoch 2][142/834] loss: 0.6494 PSNR_train: 32.3995\n",
      "[epoch 2][143/834] loss: 0.6588 PSNR_train: 32.2435\n",
      "[epoch 2][144/834] loss: 0.6243 PSNR_train: 32.4766\n",
      "[epoch 2][145/834] loss: 0.6376 PSNR_train: 32.3922\n",
      "[epoch 2][146/834] loss: 0.6569 PSNR_train: 32.2624\n",
      "[epoch 2][147/834] loss: 0.6055 PSNR_train: 32.5419\n",
      "[epoch 2][148/834] loss: 0.6587 PSNR_train: 32.2368\n",
      "[epoch 2][149/834] loss: 0.6316 PSNR_train: 32.4477\n",
      "[epoch 2][150/834] loss: 0.6536 PSNR_train: 32.3329\n",
      "[epoch 2][151/834] loss: 0.6200 PSNR_train: 32.5367\n",
      "[epoch 2][152/834] loss: 0.6398 PSNR_train: 32.5343\n",
      "[epoch 2][153/834] loss: 0.6352 PSNR_train: 32.3916\n",
      "[epoch 2][154/834] loss: 0.6264 PSNR_train: 32.6607\n",
      "[epoch 2][155/834] loss: 0.6474 PSNR_train: 32.4509\n",
      "[epoch 2][156/834] loss: 0.6382 PSNR_train: 32.3708\n",
      "[epoch 2][157/834] loss: 0.6061 PSNR_train: 32.7101\n",
      "[epoch 2][158/834] loss: 0.6145 PSNR_train: 32.4645\n",
      "[epoch 2][159/834] loss: 0.6730 PSNR_train: 32.2163\n",
      "[epoch 2][160/834] loss: 0.6293 PSNR_train: 32.6314\n",
      "[epoch 2][161/834] loss: 0.6349 PSNR_train: 32.4457\n",
      "[epoch 2][162/834] loss: 0.6858 PSNR_train: 32.1324\n",
      "[epoch 2][163/834] loss: 0.6635 PSNR_train: 32.1674\n",
      "[epoch 2][164/834] loss: 0.6306 PSNR_train: 32.5537\n",
      "[epoch 2][165/834] loss: 0.6009 PSNR_train: 32.7894\n",
      "[epoch 2][166/834] loss: 0.6496 PSNR_train: 32.2816\n",
      "[epoch 2][167/834] loss: 0.6235 PSNR_train: 32.5584\n",
      "[epoch 2][168/834] loss: 0.6513 PSNR_train: 32.2839\n",
      "[epoch 2][169/834] loss: 0.6004 PSNR_train: 32.6422\n",
      "[epoch 2][170/834] loss: 0.6686 PSNR_train: 32.2836\n",
      "[epoch 2][171/834] loss: 0.6322 PSNR_train: 32.5556\n",
      "[epoch 2][172/834] loss: 0.6472 PSNR_train: 32.4050\n",
      "[epoch 2][173/834] loss: 0.6345 PSNR_train: 32.6011\n",
      "[epoch 2][174/834] loss: 0.6373 PSNR_train: 32.5391\n",
      "[epoch 2][175/834] loss: 0.6243 PSNR_train: 32.4844\n",
      "[epoch 2][176/834] loss: 0.6241 PSNR_train: 32.4942\n",
      "[epoch 2][177/834] loss: 0.6327 PSNR_train: 32.4636\n",
      "[epoch 2][178/834] loss: 0.7245 PSNR_train: 31.9856\n",
      "[epoch 2][179/834] loss: 0.6232 PSNR_train: 32.5287\n",
      "[epoch 2][180/834] loss: 0.6336 PSNR_train: 32.4126\n",
      "[epoch 2][181/834] loss: 0.6266 PSNR_train: 32.4696\n",
      "[epoch 2][182/834] loss: 0.5898 PSNR_train: 32.7090\n",
      "[epoch 2][183/834] loss: 0.6217 PSNR_train: 32.4811\n",
      "[epoch 2][184/834] loss: 0.6430 PSNR_train: 32.4015\n",
      "[epoch 2][185/834] loss: 0.6514 PSNR_train: 32.2033\n",
      "[epoch 2][186/834] loss: 0.6159 PSNR_train: 32.5765\n",
      "[epoch 2][187/834] loss: 0.6199 PSNR_train: 32.5679\n",
      "[epoch 2][188/834] loss: 0.5879 PSNR_train: 32.7308\n",
      "[epoch 2][189/834] loss: 0.6219 PSNR_train: 32.5483\n",
      "[epoch 2][190/834] loss: 0.6420 PSNR_train: 32.3209\n",
      "[epoch 2][191/834] loss: 0.6499 PSNR_train: 32.2156\n",
      "[epoch 2][192/834] loss: 0.6515 PSNR_train: 32.2805\n",
      "[epoch 2][193/834] loss: 0.6453 PSNR_train: 32.3091\n",
      "[epoch 2][194/834] loss: 0.6170 PSNR_train: 32.5475\n",
      "[epoch 2][195/834] loss: 0.6495 PSNR_train: 32.2442\n",
      "[epoch 2][196/834] loss: 0.6586 PSNR_train: 32.2936\n",
      "[epoch 2][197/834] loss: 0.6324 PSNR_train: 32.4638\n",
      "[epoch 2][198/834] loss: 0.6553 PSNR_train: 32.3369\n",
      "[epoch 2][199/834] loss: 0.6312 PSNR_train: 32.4320\n",
      "[epoch 2][200/834] loss: 0.6531 PSNR_train: 32.3971\n",
      "[epoch 2][201/834] loss: 0.6691 PSNR_train: 32.2335\n",
      "[epoch 2][202/834] loss: 0.6220 PSNR_train: 32.5412\n",
      "[epoch 2][203/834] loss: 0.6177 PSNR_train: 32.6300\n",
      "[epoch 2][204/834] loss: 0.6342 PSNR_train: 32.5176\n",
      "[epoch 2][205/834] loss: 0.6146 PSNR_train: 32.7160\n",
      "[epoch 2][206/834] loss: 0.6184 PSNR_train: 32.6390\n",
      "[epoch 2][207/834] loss: 0.6733 PSNR_train: 32.2163\n",
      "[epoch 2][208/834] loss: 0.6288 PSNR_train: 32.4704\n",
      "[epoch 2][209/834] loss: 0.6181 PSNR_train: 32.5635\n",
      "[epoch 2][210/834] loss: 0.6244 PSNR_train: 32.4775\n",
      "[epoch 2][211/834] loss: 0.6227 PSNR_train: 32.4033\n",
      "[epoch 2][212/834] loss: 0.6075 PSNR_train: 32.6632\n",
      "[epoch 2][213/834] loss: 0.5989 PSNR_train: 32.7134\n",
      "[epoch 2][214/834] loss: 0.6796 PSNR_train: 32.0413\n",
      "[epoch 2][215/834] loss: 0.6576 PSNR_train: 32.3661\n",
      "[epoch 2][216/834] loss: 0.6016 PSNR_train: 32.7139\n",
      "[epoch 2][217/834] loss: 0.6509 PSNR_train: 32.1721\n",
      "[epoch 2][218/834] loss: 0.6440 PSNR_train: 32.4259\n",
      "[epoch 2][219/834] loss: 0.6351 PSNR_train: 32.4899\n",
      "[epoch 2][220/834] loss: 0.6294 PSNR_train: 32.5378\n",
      "[epoch 2][221/834] loss: 0.6071 PSNR_train: 32.6565\n",
      "[epoch 2][222/834] loss: 0.6563 PSNR_train: 32.4289\n",
      "[epoch 2][223/834] loss: 0.6220 PSNR_train: 32.4787\n",
      "[epoch 2][224/834] loss: 0.5992 PSNR_train: 32.6749\n",
      "[epoch 2][225/834] loss: 0.6405 PSNR_train: 32.3782\n",
      "[epoch 2][226/834] loss: 0.6193 PSNR_train: 32.7334\n",
      "[epoch 2][227/834] loss: 0.6044 PSNR_train: 32.6487\n",
      "[epoch 2][228/834] loss: 0.6104 PSNR_train: 32.6036\n",
      "[epoch 2][229/834] loss: 0.6302 PSNR_train: 32.4694\n",
      "[epoch 2][230/834] loss: 0.6112 PSNR_train: 32.6686\n",
      "[epoch 2][231/834] loss: 0.6397 PSNR_train: 32.4934\n",
      "[epoch 2][232/834] loss: 0.6479 PSNR_train: 32.3323\n",
      "[epoch 2][233/834] loss: 0.6437 PSNR_train: 32.4517\n",
      "[epoch 2][234/834] loss: 0.6041 PSNR_train: 32.6743\n",
      "[epoch 2][235/834] loss: 0.6346 PSNR_train: 32.4442\n",
      "[epoch 2][236/834] loss: 0.6290 PSNR_train: 32.4052\n",
      "[epoch 2][237/834] loss: 0.5976 PSNR_train: 32.7008\n",
      "[epoch 2][238/834] loss: 0.5768 PSNR_train: 32.7793\n",
      "[epoch 2][239/834] loss: 0.6393 PSNR_train: 32.4267\n",
      "[epoch 2][240/834] loss: 0.6453 PSNR_train: 32.4125\n",
      "[epoch 2][241/834] loss: 0.6539 PSNR_train: 32.3085\n",
      "[epoch 2][242/834] loss: 0.5958 PSNR_train: 32.7337\n",
      "[epoch 2][243/834] loss: 0.6080 PSNR_train: 32.5826\n",
      "[epoch 2][244/834] loss: 0.6665 PSNR_train: 32.1405\n",
      "[epoch 2][245/834] loss: 0.6594 PSNR_train: 32.3094\n",
      "[epoch 2][246/834] loss: 0.6020 PSNR_train: 32.5127\n",
      "[epoch 2][247/834] loss: 0.6536 PSNR_train: 32.2419\n",
      "[epoch 2][248/834] loss: 0.6744 PSNR_train: 32.1943\n",
      "[epoch 2][249/834] loss: 0.6535 PSNR_train: 32.2719\n",
      "[epoch 2][250/834] loss: 0.6131 PSNR_train: 32.6369\n",
      "[epoch 2][251/834] loss: 0.6110 PSNR_train: 32.6054\n",
      "[epoch 2][252/834] loss: 0.6144 PSNR_train: 32.5359\n",
      "[epoch 2][253/834] loss: 0.6086 PSNR_train: 32.5607\n",
      "[epoch 2][254/834] loss: 0.5971 PSNR_train: 32.6952\n",
      "[epoch 2][255/834] loss: 0.6333 PSNR_train: 32.5004\n",
      "[epoch 2][256/834] loss: 0.6546 PSNR_train: 32.2956\n",
      "[epoch 2][257/834] loss: 0.6344 PSNR_train: 32.5059\n",
      "[epoch 2][258/834] loss: 0.6461 PSNR_train: 32.4184\n",
      "[epoch 2][259/834] loss: 0.6187 PSNR_train: 32.6044\n",
      "[epoch 2][260/834] loss: 0.6479 PSNR_train: 32.5000\n",
      "[epoch 2][261/834] loss: 0.5970 PSNR_train: 32.8726\n",
      "[epoch 2][262/834] loss: 0.6273 PSNR_train: 32.5656\n",
      "[epoch 2][263/834] loss: 0.6503 PSNR_train: 32.3693\n",
      "[epoch 2][264/834] loss: 0.6114 PSNR_train: 32.5795\n",
      "[epoch 2][265/834] loss: 0.6386 PSNR_train: 32.4242\n",
      "[epoch 2][266/834] loss: 0.5952 PSNR_train: 32.7362\n",
      "[epoch 2][267/834] loss: 0.6468 PSNR_train: 32.4687\n",
      "[epoch 2][268/834] loss: 0.6492 PSNR_train: 32.3154\n",
      "[epoch 2][269/834] loss: 0.7005 PSNR_train: 32.0860\n",
      "[epoch 2][270/834] loss: 0.6026 PSNR_train: 32.7233\n",
      "[epoch 2][271/834] loss: 0.6275 PSNR_train: 32.5232\n",
      "[epoch 2][272/834] loss: 0.6530 PSNR_train: 32.4190\n",
      "[epoch 2][273/834] loss: 0.6477 PSNR_train: 32.3446\n",
      "[epoch 2][274/834] loss: 0.6299 PSNR_train: 32.4606\n",
      "[epoch 2][275/834] loss: 0.6554 PSNR_train: 32.4490\n",
      "[epoch 2][276/834] loss: 0.6333 PSNR_train: 32.4758\n",
      "[epoch 2][277/834] loss: 0.6301 PSNR_train: 32.4163\n",
      "[epoch 2][278/834] loss: 0.6271 PSNR_train: 32.6860\n",
      "[epoch 2][279/834] loss: 0.6689 PSNR_train: 32.3757\n",
      "[epoch 2][280/834] loss: 0.6569 PSNR_train: 32.3408\n",
      "[epoch 2][281/834] loss: 0.6286 PSNR_train: 32.5085\n",
      "[epoch 2][282/834] loss: 0.6197 PSNR_train: 32.6440\n",
      "[epoch 2][283/834] loss: 0.6413 PSNR_train: 32.4582\n",
      "[epoch 2][284/834] loss: 0.6401 PSNR_train: 32.4758\n",
      "[epoch 2][285/834] loss: 0.6341 PSNR_train: 32.5039\n",
      "[epoch 2][286/834] loss: 0.6357 PSNR_train: 32.4319\n",
      "[epoch 2][287/834] loss: 0.6201 PSNR_train: 32.5508\n",
      "[epoch 2][288/834] loss: 0.6165 PSNR_train: 32.5015\n",
      "[epoch 2][289/834] loss: 0.5908 PSNR_train: 32.7431\n",
      "[epoch 2][290/834] loss: 0.6386 PSNR_train: 32.5437\n",
      "[epoch 2][291/834] loss: 0.6486 PSNR_train: 32.4727\n",
      "[epoch 2][292/834] loss: 0.6222 PSNR_train: 32.5208\n",
      "[epoch 2][293/834] loss: 0.5835 PSNR_train: 32.7650\n",
      "[epoch 2][294/834] loss: 0.6549 PSNR_train: 32.3568\n",
      "[epoch 2][295/834] loss: 0.6575 PSNR_train: 32.3816\n",
      "[epoch 2][296/834] loss: 0.6256 PSNR_train: 32.5944\n",
      "[epoch 2][297/834] loss: 0.6325 PSNR_train: 32.5084\n",
      "[epoch 2][298/834] loss: 0.6541 PSNR_train: 32.3720\n",
      "[epoch 2][299/834] loss: 0.6374 PSNR_train: 32.3254\n",
      "[epoch 2][300/834] loss: 0.6396 PSNR_train: 32.3634\n",
      "[epoch 2][301/834] loss: 0.6518 PSNR_train: 32.3289\n",
      "[epoch 2][302/834] loss: 0.6205 PSNR_train: 32.4604\n",
      "[epoch 2][303/834] loss: 0.6244 PSNR_train: 32.4638\n",
      "[epoch 2][304/834] loss: 0.6158 PSNR_train: 32.6257\n",
      "[epoch 2][305/834] loss: 0.6625 PSNR_train: 32.1550\n",
      "[epoch 2][306/834] loss: 0.6778 PSNR_train: 32.1496\n",
      "[epoch 2][307/834] loss: 0.6692 PSNR_train: 32.1250\n",
      "[epoch 2][308/834] loss: 0.6174 PSNR_train: 32.5019\n",
      "[epoch 2][309/834] loss: 0.6563 PSNR_train: 32.1642\n",
      "[epoch 2][310/834] loss: 0.6120 PSNR_train: 32.7230\n",
      "[epoch 2][311/834] loss: 0.6375 PSNR_train: 32.4192\n",
      "[epoch 2][312/834] loss: 0.6315 PSNR_train: 32.4531\n",
      "[epoch 2][313/834] loss: 0.6238 PSNR_train: 32.3972\n",
      "[epoch 2][314/834] loss: 0.6374 PSNR_train: 32.4363\n",
      "[epoch 2][315/834] loss: 0.6338 PSNR_train: 32.5097\n",
      "[epoch 2][316/834] loss: 0.6805 PSNR_train: 32.2342\n",
      "[epoch 2][317/834] loss: 0.6370 PSNR_train: 32.4609\n",
      "[epoch 2][318/834] loss: 0.6448 PSNR_train: 32.3050\n",
      "[epoch 2][319/834] loss: 0.6551 PSNR_train: 32.2689\n",
      "[epoch 2][320/834] loss: 0.6363 PSNR_train: 32.4871\n",
      "[epoch 2][321/834] loss: 0.6213 PSNR_train: 32.5818\n",
      "[epoch 2][322/834] loss: 0.6467 PSNR_train: 32.3437\n",
      "[epoch 2][323/834] loss: 0.6268 PSNR_train: 32.6449\n",
      "[epoch 2][324/834] loss: 0.6292 PSNR_train: 32.6468\n",
      "[epoch 2][325/834] loss: 0.6181 PSNR_train: 32.5199\n",
      "[epoch 2][326/834] loss: 0.6607 PSNR_train: 32.2460\n",
      "[epoch 2][327/834] loss: 0.6355 PSNR_train: 32.4326\n",
      "[epoch 2][328/834] loss: 0.6106 PSNR_train: 32.5677\n",
      "[epoch 2][329/834] loss: 0.6097 PSNR_train: 32.6347\n",
      "[epoch 2][330/834] loss: 0.5934 PSNR_train: 32.6748\n",
      "[epoch 2][331/834] loss: 0.6294 PSNR_train: 32.5413\n",
      "[epoch 2][332/834] loss: 0.6295 PSNR_train: 32.4216\n",
      "[epoch 2][333/834] loss: 0.6468 PSNR_train: 32.3257\n",
      "[epoch 2][334/834] loss: 0.6027 PSNR_train: 32.6578\n",
      "[epoch 2][335/834] loss: 0.6691 PSNR_train: 32.2215\n",
      "[epoch 2][336/834] loss: 0.6072 PSNR_train: 32.5977\n",
      "[epoch 2][337/834] loss: 0.5794 PSNR_train: 32.8954\n",
      "[epoch 2][338/834] loss: 0.6626 PSNR_train: 32.2947\n",
      "[epoch 2][339/834] loss: 0.6151 PSNR_train: 32.5075\n",
      "[epoch 2][340/834] loss: 0.6365 PSNR_train: 32.3617\n",
      "[epoch 2][341/834] loss: 0.6249 PSNR_train: 32.4317\n",
      "[epoch 2][342/834] loss: 0.6236 PSNR_train: 32.5147\n",
      "[epoch 2][343/834] loss: 0.5812 PSNR_train: 32.9074\n",
      "[epoch 2][344/834] loss: 0.6430 PSNR_train: 32.4389\n",
      "[epoch 2][345/834] loss: 0.6399 PSNR_train: 32.4183\n",
      "[epoch 2][346/834] loss: 0.6566 PSNR_train: 32.1507\n",
      "[epoch 2][347/834] loss: 0.6462 PSNR_train: 32.3763\n",
      "[epoch 2][348/834] loss: 0.6351 PSNR_train: 32.4773\n",
      "[epoch 2][349/834] loss: 0.6644 PSNR_train: 32.2328\n",
      "[epoch 2][350/834] loss: 0.6021 PSNR_train: 32.6774\n",
      "[epoch 2][351/834] loss: 0.5972 PSNR_train: 32.7491\n",
      "[epoch 2][352/834] loss: 0.6628 PSNR_train: 32.2260\n",
      "[epoch 2][353/834] loss: 0.6311 PSNR_train: 32.4868\n",
      "[epoch 2][354/834] loss: 0.6365 PSNR_train: 32.3481\n",
      "[epoch 2][355/834] loss: 0.5943 PSNR_train: 32.7414\n",
      "[epoch 2][356/834] loss: 0.6393 PSNR_train: 32.4887\n",
      "[epoch 2][357/834] loss: 0.6103 PSNR_train: 32.7709\n",
      "[epoch 2][358/834] loss: 0.6445 PSNR_train: 32.2849\n",
      "[epoch 2][359/834] loss: 0.6394 PSNR_train: 32.4354\n",
      "[epoch 2][360/834] loss: 0.6024 PSNR_train: 32.7446\n",
      "[epoch 2][361/834] loss: 0.6264 PSNR_train: 32.4160\n",
      "[epoch 2][362/834] loss: 0.6007 PSNR_train: 32.6670\n",
      "[epoch 2][363/834] loss: 0.6316 PSNR_train: 32.5501\n",
      "[epoch 2][364/834] loss: 0.6077 PSNR_train: 32.5965\n",
      "[epoch 2][365/834] loss: 0.6547 PSNR_train: 32.2048\n",
      "[epoch 2][366/834] loss: 0.6572 PSNR_train: 32.2917\n",
      "[epoch 2][367/834] loss: 0.6393 PSNR_train: 32.3503\n",
      "[epoch 2][368/834] loss: 0.6304 PSNR_train: 32.4904\n",
      "[epoch 2][369/834] loss: 0.6216 PSNR_train: 32.6790\n",
      "[epoch 2][370/834] loss: 0.6260 PSNR_train: 32.4190\n",
      "[epoch 2][371/834] loss: 0.6244 PSNR_train: 32.4857\n",
      "[epoch 2][372/834] loss: 0.6602 PSNR_train: 32.2790\n",
      "[epoch 2][373/834] loss: 0.6144 PSNR_train: 32.5827\n",
      "[epoch 2][374/834] loss: 0.6418 PSNR_train: 32.3892\n",
      "[epoch 2][375/834] loss: 0.6615 PSNR_train: 32.2316\n",
      "[epoch 2][376/834] loss: 0.6433 PSNR_train: 32.3886\n",
      "[epoch 2][377/834] loss: 0.6200 PSNR_train: 32.6126\n",
      "[epoch 2][378/834] loss: 0.6678 PSNR_train: 32.2146\n",
      "[epoch 2][379/834] loss: 0.6452 PSNR_train: 32.4047\n",
      "[epoch 2][380/834] loss: 0.6277 PSNR_train: 32.6184\n",
      "[epoch 2][381/834] loss: 0.6013 PSNR_train: 32.6734\n",
      "[epoch 2][382/834] loss: 0.6618 PSNR_train: 32.3569\n",
      "[epoch 2][383/834] loss: 0.5854 PSNR_train: 32.8268\n",
      "[epoch 2][384/834] loss: 0.6401 PSNR_train: 32.4285\n",
      "[epoch 2][385/834] loss: 0.6310 PSNR_train: 32.5444\n",
      "[epoch 2][386/834] loss: 0.6340 PSNR_train: 32.5302\n",
      "[epoch 2][387/834] loss: 0.5935 PSNR_train: 32.7453\n",
      "[epoch 2][388/834] loss: 0.6456 PSNR_train: 32.4933\n",
      "[epoch 2][389/834] loss: 0.6138 PSNR_train: 32.6369\n",
      "[epoch 2][390/834] loss: 0.6475 PSNR_train: 32.3245\n",
      "[epoch 2][391/834] loss: 0.6186 PSNR_train: 32.5158\n",
      "[epoch 2][392/834] loss: 0.6331 PSNR_train: 32.4235\n",
      "[epoch 2][393/834] loss: 0.6080 PSNR_train: 32.7686\n",
      "[epoch 2][394/834] loss: 0.5963 PSNR_train: 32.8091\n",
      "[epoch 2][395/834] loss: 0.5977 PSNR_train: 32.7418\n",
      "[epoch 2][396/834] loss: 0.6239 PSNR_train: 32.6793\n",
      "[epoch 2][397/834] loss: 0.6070 PSNR_train: 32.7028\n",
      "[epoch 2][398/834] loss: 0.6624 PSNR_train: 32.3067\n",
      "[epoch 2][399/834] loss: 0.6725 PSNR_train: 32.2345\n",
      "[epoch 2][400/834] loss: 0.6763 PSNR_train: 32.1123\n",
      "[epoch 2][401/834] loss: 0.6369 PSNR_train: 32.4108\n",
      "[epoch 2][402/834] loss: 0.6364 PSNR_train: 32.3417\n",
      "[epoch 2][403/834] loss: 0.5930 PSNR_train: 32.6923\n",
      "[epoch 2][404/834] loss: 0.6566 PSNR_train: 32.3009\n",
      "[epoch 2][405/834] loss: 0.6111 PSNR_train: 32.5473\n",
      "[epoch 2][406/834] loss: 0.5986 PSNR_train: 32.7373\n",
      "[epoch 2][407/834] loss: 0.6336 PSNR_train: 32.6305\n",
      "[epoch 2][408/834] loss: 0.6272 PSNR_train: 32.4916\n",
      "[epoch 2][409/834] loss: 0.6295 PSNR_train: 32.4873\n",
      "[epoch 2][410/834] loss: 0.6082 PSNR_train: 32.6608\n",
      "[epoch 2][411/834] loss: 0.6472 PSNR_train: 32.4277\n",
      "[epoch 2][412/834] loss: 0.6156 PSNR_train: 32.4225\n",
      "[epoch 2][413/834] loss: 0.6417 PSNR_train: 32.4328\n",
      "[epoch 2][414/834] loss: 0.6161 PSNR_train: 32.5530\n",
      "[epoch 2][415/834] loss: 0.6787 PSNR_train: 32.1252\n",
      "[epoch 2][416/834] loss: 0.6194 PSNR_train: 32.5162\n",
      "[epoch 2][417/834] loss: 0.6089 PSNR_train: 32.5916\n",
      "[epoch 2][418/834] loss: 0.6342 PSNR_train: 32.3890\n",
      "[epoch 2][419/834] loss: 0.6362 PSNR_train: 32.4474\n",
      "[epoch 2][420/834] loss: 0.6526 PSNR_train: 32.3533\n",
      "[epoch 2][421/834] loss: 0.6818 PSNR_train: 32.0366\n",
      "[epoch 2][422/834] loss: 0.6381 PSNR_train: 32.4559\n",
      "[epoch 2][423/834] loss: 0.6526 PSNR_train: 32.4723\n",
      "[epoch 2][424/834] loss: 0.6029 PSNR_train: 32.6364\n",
      "[epoch 2][425/834] loss: 0.6058 PSNR_train: 32.7539\n",
      "[epoch 2][426/834] loss: 0.6212 PSNR_train: 32.6846\n",
      "[epoch 2][427/834] loss: 0.6538 PSNR_train: 32.4338\n",
      "[epoch 2][428/834] loss: 0.6105 PSNR_train: 32.6260\n",
      "[epoch 2][429/834] loss: 0.6179 PSNR_train: 32.5885\n",
      "[epoch 2][430/834] loss: 0.6396 PSNR_train: 32.3513\n",
      "[epoch 2][431/834] loss: 0.6348 PSNR_train: 32.5048\n",
      "[epoch 2][432/834] loss: 0.5587 PSNR_train: 33.1134\n",
      "[epoch 2][433/834] loss: 0.6321 PSNR_train: 32.4357\n",
      "[epoch 2][434/834] loss: 0.5896 PSNR_train: 32.6828\n",
      "[epoch 2][435/834] loss: 0.5924 PSNR_train: 32.6798\n",
      "[epoch 2][436/834] loss: 0.6243 PSNR_train: 32.4757\n",
      "[epoch 2][437/834] loss: 0.6065 PSNR_train: 32.6705\n",
      "[epoch 2][438/834] loss: 0.5755 PSNR_train: 32.9184\n",
      "[epoch 2][439/834] loss: 0.6500 PSNR_train: 32.3972\n",
      "[epoch 2][440/834] loss: 0.6242 PSNR_train: 32.4185\n",
      "[epoch 2][441/834] loss: 0.6276 PSNR_train: 32.5061\n",
      "[epoch 2][442/834] loss: 0.5887 PSNR_train: 32.8506\n",
      "[epoch 2][443/834] loss: 0.6400 PSNR_train: 32.4102\n",
      "[epoch 2][444/834] loss: 0.6505 PSNR_train: 32.2904\n",
      "[epoch 2][445/834] loss: 0.6075 PSNR_train: 32.6596\n",
      "[epoch 2][446/834] loss: 0.6344 PSNR_train: 32.3560\n",
      "[epoch 2][447/834] loss: 0.6351 PSNR_train: 32.3532\n",
      "[epoch 2][448/834] loss: 0.6360 PSNR_train: 32.3498\n",
      "[epoch 2][449/834] loss: 0.5928 PSNR_train: 32.6262\n",
      "[epoch 2][450/834] loss: 0.6392 PSNR_train: 32.3626\n",
      "[epoch 2][451/834] loss: 0.6097 PSNR_train: 32.4832\n",
      "[epoch 2][452/834] loss: 0.6299 PSNR_train: 32.4985\n",
      "[epoch 2][453/834] loss: 0.6213 PSNR_train: 32.4587\n",
      "[epoch 2][454/834] loss: 0.6484 PSNR_train: 32.3250\n",
      "[epoch 2][455/834] loss: 0.6765 PSNR_train: 32.0867\n",
      "[epoch 2][456/834] loss: 0.6671 PSNR_train: 32.3678\n",
      "[epoch 2][457/834] loss: 0.6112 PSNR_train: 32.6086\n",
      "[epoch 2][458/834] loss: 0.6144 PSNR_train: 32.5962\n",
      "[epoch 2][459/834] loss: 0.6053 PSNR_train: 32.5826\n",
      "[epoch 2][460/834] loss: 0.6155 PSNR_train: 32.4737\n",
      "[epoch 2][461/834] loss: 0.6999 PSNR_train: 32.0322\n",
      "[epoch 2][462/834] loss: 0.6340 PSNR_train: 32.5045\n",
      "[epoch 2][463/834] loss: 0.6044 PSNR_train: 32.6921\n",
      "[epoch 2][464/834] loss: 0.6441 PSNR_train: 32.4010\n",
      "[epoch 2][465/834] loss: 0.5970 PSNR_train: 32.7033\n",
      "[epoch 2][466/834] loss: 0.6022 PSNR_train: 32.8063\n",
      "[epoch 2][467/834] loss: 0.6264 PSNR_train: 32.4765\n",
      "[epoch 2][468/834] loss: 0.6382 PSNR_train: 32.4789\n",
      "[epoch 2][469/834] loss: 0.6375 PSNR_train: 32.4192\n",
      "[epoch 2][470/834] loss: 0.6356 PSNR_train: 32.3761\n",
      "[epoch 2][471/834] loss: 0.6161 PSNR_train: 32.6000\n",
      "[epoch 2][472/834] loss: 0.6438 PSNR_train: 32.5030\n",
      "[epoch 2][473/834] loss: 0.6220 PSNR_train: 32.4210\n",
      "[epoch 2][474/834] loss: 0.6545 PSNR_train: 32.1794\n",
      "[epoch 2][475/834] loss: 0.5966 PSNR_train: 32.8041\n",
      "[epoch 2][476/834] loss: 0.6406 PSNR_train: 32.3288\n",
      "[epoch 2][477/834] loss: 0.6328 PSNR_train: 32.4119\n",
      "[epoch 2][478/834] loss: 0.6795 PSNR_train: 32.2677\n",
      "[epoch 2][479/834] loss: 0.5851 PSNR_train: 32.8988\n",
      "[epoch 2][480/834] loss: 0.6724 PSNR_train: 32.1975\n",
      "[epoch 2][481/834] loss: 0.6453 PSNR_train: 32.3149\n",
      "[epoch 2][482/834] loss: 0.6347 PSNR_train: 32.5135\n",
      "[epoch 2][483/834] loss: 0.6492 PSNR_train: 32.3666\n",
      "[epoch 2][484/834] loss: 0.6412 PSNR_train: 32.3900\n",
      "[epoch 2][485/834] loss: 0.6235 PSNR_train: 32.5569\n",
      "[epoch 2][486/834] loss: 0.6196 PSNR_train: 32.5330\n",
      "[epoch 2][487/834] loss: 0.6492 PSNR_train: 32.3934\n",
      "[epoch 2][488/834] loss: 0.5982 PSNR_train: 32.6601\n",
      "[epoch 2][489/834] loss: 0.6650 PSNR_train: 32.3150\n",
      "[epoch 2][490/834] loss: 0.6513 PSNR_train: 32.3376\n",
      "[epoch 2][491/834] loss: 0.6463 PSNR_train: 32.3403\n",
      "[epoch 2][492/834] loss: 0.6280 PSNR_train: 32.7128\n",
      "[epoch 2][493/834] loss: 0.6201 PSNR_train: 32.5547\n",
      "[epoch 2][494/834] loss: 0.6520 PSNR_train: 32.3169\n",
      "[epoch 2][495/834] loss: 0.5977 PSNR_train: 32.7995\n",
      "[epoch 2][496/834] loss: 0.6424 PSNR_train: 32.4164\n",
      "[epoch 2][497/834] loss: 0.6196 PSNR_train: 32.6371\n",
      "[epoch 2][498/834] loss: 0.6172 PSNR_train: 32.6109\n",
      "[epoch 2][499/834] loss: 0.6424 PSNR_train: 32.3720\n",
      "[epoch 2][500/834] loss: 0.6154 PSNR_train: 32.6079\n",
      "[epoch 2][501/834] loss: 0.6094 PSNR_train: 32.5796\n",
      "[epoch 2][502/834] loss: 0.6325 PSNR_train: 32.4559\n",
      "[epoch 2][503/834] loss: 0.6298 PSNR_train: 32.4285\n",
      "[epoch 2][504/834] loss: 0.6544 PSNR_train: 32.4105\n",
      "[epoch 2][505/834] loss: 0.6282 PSNR_train: 32.3648\n",
      "[epoch 2][506/834] loss: 0.6239 PSNR_train: 32.4945\n",
      "[epoch 2][507/834] loss: 0.6665 PSNR_train: 32.3088\n",
      "[epoch 2][508/834] loss: 0.6013 PSNR_train: 32.6847\n",
      "[epoch 2][509/834] loss: 0.5958 PSNR_train: 32.8011\n",
      "[epoch 2][510/834] loss: 0.6092 PSNR_train: 32.7138\n",
      "[epoch 2][511/834] loss: 0.6347 PSNR_train: 32.4203\n",
      "[epoch 2][512/834] loss: 0.6458 PSNR_train: 32.4144\n",
      "[epoch 2][513/834] loss: 0.6043 PSNR_train: 32.8671\n",
      "[epoch 2][514/834] loss: 0.5966 PSNR_train: 32.6514\n",
      "[epoch 2][515/834] loss: 0.6355 PSNR_train: 32.4674\n",
      "[epoch 2][516/834] loss: 0.6056 PSNR_train: 32.5229\n",
      "[epoch 2][517/834] loss: 0.5975 PSNR_train: 32.6121\n",
      "[epoch 2][518/834] loss: 0.6067 PSNR_train: 32.6462\n",
      "[epoch 2][519/834] loss: 0.6412 PSNR_train: 32.4485\n",
      "[epoch 2][520/834] loss: 0.5974 PSNR_train: 32.8325\n",
      "[epoch 2][521/834] loss: 0.6567 PSNR_train: 32.4522\n",
      "[epoch 2][522/834] loss: 0.6670 PSNR_train: 32.2338\n",
      "[epoch 2][523/834] loss: 0.6271 PSNR_train: 32.5018\n",
      "[epoch 2][524/834] loss: 0.6179 PSNR_train: 32.5949\n",
      "[epoch 2][525/834] loss: 0.6506 PSNR_train: 32.2648\n",
      "[epoch 2][526/834] loss: 0.6493 PSNR_train: 32.4092\n",
      "[epoch 2][527/834] loss: 0.6026 PSNR_train: 32.6563\n",
      "[epoch 2][528/834] loss: 0.6191 PSNR_train: 32.6111\n",
      "[epoch 2][529/834] loss: 0.6013 PSNR_train: 32.6970\n",
      "[epoch 2][530/834] loss: 0.6325 PSNR_train: 32.4965\n",
      "[epoch 2][531/834] loss: 0.6599 PSNR_train: 32.2064\n",
      "[epoch 2][532/834] loss: 0.6457 PSNR_train: 32.2222\n",
      "[epoch 2][533/834] loss: 0.6058 PSNR_train: 32.6666\n",
      "[epoch 2][534/834] loss: 0.6061 PSNR_train: 32.5152\n",
      "[epoch 2][535/834] loss: 0.6265 PSNR_train: 32.5197\n",
      "[epoch 2][536/834] loss: 0.6050 PSNR_train: 32.5885\n",
      "[epoch 2][537/834] loss: 0.5992 PSNR_train: 32.5261\n",
      "[epoch 2][538/834] loss: 0.6201 PSNR_train: 32.6167\n",
      "[epoch 2][539/834] loss: 0.5658 PSNR_train: 32.9392\n",
      "[epoch 2][540/834] loss: 0.6126 PSNR_train: 32.6803\n",
      "[epoch 2][541/834] loss: 0.6432 PSNR_train: 32.4102\n",
      "[epoch 2][542/834] loss: 0.6117 PSNR_train: 32.7393\n",
      "[epoch 2][543/834] loss: 0.6403 PSNR_train: 32.4666\n",
      "[epoch 2][544/834] loss: 0.5897 PSNR_train: 32.8710\n",
      "[epoch 2][545/834] loss: 0.5749 PSNR_train: 32.8475\n",
      "[epoch 2][546/834] loss: 0.6374 PSNR_train: 32.6226\n",
      "[epoch 2][547/834] loss: 0.6195 PSNR_train: 32.6077\n",
      "[epoch 2][548/834] loss: 0.6504 PSNR_train: 32.3455\n",
      "[epoch 2][549/834] loss: 0.6276 PSNR_train: 32.3874\n",
      "[epoch 2][550/834] loss: 0.5905 PSNR_train: 32.7873\n",
      "[epoch 2][551/834] loss: 0.6078 PSNR_train: 32.7347\n",
      "[epoch 2][552/834] loss: 0.6366 PSNR_train: 32.2744\n",
      "[epoch 2][553/834] loss: 0.6072 PSNR_train: 32.4948\n",
      "[epoch 2][554/834] loss: 0.6209 PSNR_train: 32.4283\n",
      "[epoch 2][555/834] loss: 0.6195 PSNR_train: 32.6873\n",
      "[epoch 2][556/834] loss: 0.6243 PSNR_train: 32.4079\n",
      "[epoch 2][557/834] loss: 0.5997 PSNR_train: 32.5614\n",
      "[epoch 2][558/834] loss: 0.6355 PSNR_train: 32.5858\n",
      "[epoch 2][559/834] loss: 0.6673 PSNR_train: 32.1232\n",
      "[epoch 2][560/834] loss: 0.6619 PSNR_train: 32.1924\n",
      "[epoch 2][561/834] loss: 0.6549 PSNR_train: 32.2932\n",
      "[epoch 2][562/834] loss: 0.6419 PSNR_train: 32.2874\n",
      "[epoch 2][563/834] loss: 0.6205 PSNR_train: 32.4400\n",
      "[epoch 2][564/834] loss: 0.6306 PSNR_train: 32.5548\n",
      "[epoch 2][565/834] loss: 0.6197 PSNR_train: 32.4411\n",
      "[epoch 2][566/834] loss: 0.6131 PSNR_train: 32.4668\n",
      "[epoch 2][567/834] loss: 0.6201 PSNR_train: 32.5980\n",
      "[epoch 2][568/834] loss: 0.6322 PSNR_train: 32.5196\n",
      "[epoch 2][569/834] loss: 0.6186 PSNR_train: 32.5808\n",
      "[epoch 2][570/834] loss: 0.5880 PSNR_train: 32.7858\n",
      "[epoch 2][571/834] loss: 0.6380 PSNR_train: 32.2510\n",
      "[epoch 2][572/834] loss: 0.6373 PSNR_train: 32.3326\n",
      "[epoch 2][573/834] loss: 0.6114 PSNR_train: 32.5272\n",
      "[epoch 2][574/834] loss: 0.6436 PSNR_train: 32.5119\n",
      "[epoch 2][575/834] loss: 0.5875 PSNR_train: 32.6993\n",
      "[epoch 2][576/834] loss: 0.6533 PSNR_train: 32.3442\n",
      "[epoch 2][577/834] loss: 0.6565 PSNR_train: 32.4424\n",
      "[epoch 2][578/834] loss: 0.5935 PSNR_train: 32.7161\n",
      "[epoch 2][579/834] loss: 0.6030 PSNR_train: 32.6846\n",
      "[epoch 2][580/834] loss: 0.6471 PSNR_train: 32.3984\n",
      "[epoch 2][581/834] loss: 0.5902 PSNR_train: 32.8258\n",
      "[epoch 2][582/834] loss: 0.5908 PSNR_train: 32.8878\n",
      "[epoch 2][583/834] loss: 0.6819 PSNR_train: 32.1987\n",
      "[epoch 2][584/834] loss: 0.6325 PSNR_train: 32.5424\n",
      "[epoch 2][585/834] loss: 0.6085 PSNR_train: 32.7251\n",
      "[epoch 2][586/834] loss: 0.6238 PSNR_train: 32.6179\n",
      "[epoch 2][587/834] loss: 0.6222 PSNR_train: 32.5966\n",
      "[epoch 2][588/834] loss: 0.6059 PSNR_train: 32.6810\n",
      "[epoch 2][589/834] loss: 0.5886 PSNR_train: 32.8614\n",
      "[epoch 2][590/834] loss: 0.6231 PSNR_train: 32.5669\n",
      "[epoch 2][591/834] loss: 0.6249 PSNR_train: 32.5235\n",
      "[epoch 2][592/834] loss: 0.6218 PSNR_train: 32.4019\n",
      "[epoch 2][593/834] loss: 0.6308 PSNR_train: 32.4325\n",
      "[epoch 2][594/834] loss: 0.6276 PSNR_train: 32.4904\n",
      "[epoch 2][595/834] loss: 0.6795 PSNR_train: 32.3150\n",
      "[epoch 2][596/834] loss: 0.6434 PSNR_train: 32.4401\n",
      "[epoch 2][597/834] loss: 0.6143 PSNR_train: 32.5627\n",
      "[epoch 2][598/834] loss: 0.6440 PSNR_train: 32.3810\n",
      "[epoch 2][599/834] loss: 0.5950 PSNR_train: 32.6237\n",
      "[epoch 2][600/834] loss: 0.5608 PSNR_train: 32.8615\n",
      "[epoch 2][601/834] loss: 0.6183 PSNR_train: 32.5095\n",
      "[epoch 2][602/834] loss: 0.6109 PSNR_train: 32.5358\n",
      "[epoch 2][603/834] loss: 0.6038 PSNR_train: 32.6320\n",
      "[epoch 2][604/834] loss: 0.6167 PSNR_train: 32.5170\n",
      "[epoch 2][605/834] loss: 0.6265 PSNR_train: 32.3593\n",
      "[epoch 2][606/834] loss: 0.6655 PSNR_train: 32.1709\n",
      "[epoch 2][607/834] loss: 0.6366 PSNR_train: 32.2386\n",
      "[epoch 2][608/834] loss: 0.6561 PSNR_train: 32.2171\n",
      "[epoch 2][609/834] loss: 0.6279 PSNR_train: 32.4819\n",
      "[epoch 2][610/834] loss: 0.6258 PSNR_train: 32.5235\n",
      "[epoch 2][611/834] loss: 0.6305 PSNR_train: 32.5241\n",
      "[epoch 2][612/834] loss: 0.6067 PSNR_train: 32.5397\n",
      "[epoch 2][613/834] loss: 0.6260 PSNR_train: 32.5320\n",
      "[epoch 2][614/834] loss: 0.6557 PSNR_train: 32.2896\n",
      "[epoch 2][615/834] loss: 0.6066 PSNR_train: 32.7203\n",
      "[epoch 2][616/834] loss: 0.6299 PSNR_train: 32.4749\n",
      "[epoch 2][617/834] loss: 0.5849 PSNR_train: 32.8563\n",
      "[epoch 2][618/834] loss: 0.5874 PSNR_train: 32.8018\n",
      "[epoch 2][619/834] loss: 0.6324 PSNR_train: 32.4389\n",
      "[epoch 2][620/834] loss: 0.5888 PSNR_train: 32.8064\n",
      "[epoch 2][621/834] loss: 0.6844 PSNR_train: 32.2950\n",
      "[epoch 2][622/834] loss: 0.6155 PSNR_train: 32.5671\n",
      "[epoch 2][623/834] loss: 0.6467 PSNR_train: 32.3292\n",
      "[epoch 2][624/834] loss: 0.6177 PSNR_train: 32.5044\n",
      "[epoch 2][625/834] loss: 0.6448 PSNR_train: 32.5580\n",
      "[epoch 2][626/834] loss: 0.6037 PSNR_train: 32.6945\n",
      "[epoch 2][627/834] loss: 0.6246 PSNR_train: 32.5723\n",
      "[epoch 2][628/834] loss: 0.6240 PSNR_train: 32.5007\n",
      "[epoch 2][629/834] loss: 0.6125 PSNR_train: 32.5528\n",
      "[epoch 2][630/834] loss: 0.6605 PSNR_train: 32.1822\n",
      "[epoch 2][631/834] loss: 0.6279 PSNR_train: 32.5555\n",
      "[epoch 2][632/834] loss: 0.6318 PSNR_train: 32.3589\n",
      "[epoch 2][633/834] loss: 0.6223 PSNR_train: 32.4296\n",
      "[epoch 2][634/834] loss: 0.6457 PSNR_train: 32.3852\n",
      "[epoch 2][635/834] loss: 0.6327 PSNR_train: 32.3514\n",
      "[epoch 2][636/834] loss: 0.5832 PSNR_train: 32.7404\n",
      "[epoch 2][637/834] loss: 0.5928 PSNR_train: 32.6905\n",
      "[epoch 2][638/834] loss: 0.5873 PSNR_train: 32.7956\n",
      "[epoch 2][639/834] loss: 0.5605 PSNR_train: 32.9061\n",
      "[epoch 2][640/834] loss: 0.6403 PSNR_train: 32.3652\n",
      "[epoch 2][641/834] loss: 0.6362 PSNR_train: 32.4146\n",
      "[epoch 2][642/834] loss: 0.6117 PSNR_train: 32.7026\n",
      "[epoch 2][643/834] loss: 0.6370 PSNR_train: 32.4515\n",
      "[epoch 2][644/834] loss: 0.5852 PSNR_train: 32.7842\n",
      "[epoch 2][645/834] loss: 0.6274 PSNR_train: 32.4982\n",
      "[epoch 2][646/834] loss: 0.5779 PSNR_train: 32.8530\n",
      "[epoch 2][647/834] loss: 0.6798 PSNR_train: 32.2099\n",
      "[epoch 2][648/834] loss: 0.6115 PSNR_train: 32.6009\n",
      "[epoch 2][649/834] loss: 0.5678 PSNR_train: 32.9059\n",
      "[epoch 2][650/834] loss: 0.6154 PSNR_train: 32.6146\n",
      "[epoch 2][651/834] loss: 0.5848 PSNR_train: 32.8213\n",
      "[epoch 2][652/834] loss: 0.5979 PSNR_train: 32.6905\n",
      "[epoch 2][653/834] loss: 0.6180 PSNR_train: 32.6716\n",
      "[epoch 2][654/834] loss: 0.6332 PSNR_train: 32.5914\n",
      "[epoch 2][655/834] loss: 0.6453 PSNR_train: 32.3494\n",
      "[epoch 2][656/834] loss: 0.6038 PSNR_train: 32.7811\n",
      "[epoch 2][657/834] loss: 0.5795 PSNR_train: 32.8724\n",
      "[epoch 2][658/834] loss: 0.6591 PSNR_train: 32.3987\n",
      "[epoch 2][659/834] loss: 0.6994 PSNR_train: 32.0812\n",
      "[epoch 2][660/834] loss: 0.6325 PSNR_train: 32.4898\n",
      "[epoch 2][661/834] loss: 0.6656 PSNR_train: 32.2391\n",
      "[epoch 2][662/834] loss: 0.5797 PSNR_train: 32.8540\n",
      "[epoch 2][663/834] loss: 0.6138 PSNR_train: 32.4489\n",
      "[epoch 2][664/834] loss: 0.6097 PSNR_train: 32.7055\n",
      "[epoch 2][665/834] loss: 0.6008 PSNR_train: 32.7275\n",
      "[epoch 2][666/834] loss: 0.5756 PSNR_train: 32.9270\n",
      "[epoch 2][667/834] loss: 0.6021 PSNR_train: 32.7054\n",
      "[epoch 2][668/834] loss: 0.5964 PSNR_train: 32.7866\n",
      "[epoch 2][669/834] loss: 0.6376 PSNR_train: 32.4872\n",
      "[epoch 2][670/834] loss: 0.6063 PSNR_train: 32.5911\n",
      "[epoch 2][671/834] loss: 0.6370 PSNR_train: 32.4373\n",
      "[epoch 2][672/834] loss: 0.6758 PSNR_train: 32.1989\n",
      "[epoch 2][673/834] loss: 0.5938 PSNR_train: 32.7089\n",
      "[epoch 2][674/834] loss: 0.6426 PSNR_train: 32.2497\n",
      "[epoch 2][675/834] loss: 0.6328 PSNR_train: 32.5508\n",
      "[epoch 2][676/834] loss: 0.6448 PSNR_train: 32.3780\n",
      "[epoch 2][677/834] loss: 0.6557 PSNR_train: 32.3833\n",
      "[epoch 2][678/834] loss: 0.6338 PSNR_train: 32.5357\n",
      "[epoch 2][679/834] loss: 0.5879 PSNR_train: 32.9580\n",
      "[epoch 2][680/834] loss: 0.6747 PSNR_train: 32.3018\n",
      "[epoch 2][681/834] loss: 0.6150 PSNR_train: 32.6773\n",
      "[epoch 2][682/834] loss: 0.6302 PSNR_train: 32.4930\n",
      "[epoch 2][683/834] loss: 0.6582 PSNR_train: 32.2363\n",
      "[epoch 2][684/834] loss: 0.6001 PSNR_train: 32.5897\n",
      "[epoch 2][685/834] loss: 0.6329 PSNR_train: 32.4053\n",
      "[epoch 2][686/834] loss: 0.6202 PSNR_train: 32.4062\n",
      "[epoch 2][687/834] loss: 0.6371 PSNR_train: 32.4219\n",
      "[epoch 2][688/834] loss: 0.6509 PSNR_train: 32.3768\n",
      "[epoch 2][689/834] loss: 0.6115 PSNR_train: 32.4948\n",
      "[epoch 2][690/834] loss: 0.5949 PSNR_train: 32.6803\n",
      "[epoch 2][691/834] loss: 0.6207 PSNR_train: 32.4379\n",
      "[epoch 2][692/834] loss: 0.6531 PSNR_train: 32.2574\n",
      "[epoch 2][693/834] loss: 0.6273 PSNR_train: 32.4506\n",
      "[epoch 2][694/834] loss: 0.6515 PSNR_train: 32.3779\n",
      "[epoch 2][695/834] loss: 0.6454 PSNR_train: 32.3857\n",
      "[epoch 2][696/834] loss: 0.6812 PSNR_train: 32.1430\n",
      "[epoch 2][697/834] loss: 0.6241 PSNR_train: 32.5021\n",
      "[epoch 2][698/834] loss: 0.6420 PSNR_train: 32.3643\n",
      "[epoch 2][699/834] loss: 0.6367 PSNR_train: 32.5097\n",
      "[epoch 2][700/834] loss: 0.6066 PSNR_train: 32.7327\n",
      "[epoch 2][701/834] loss: 0.6338 PSNR_train: 32.4952\n",
      "[epoch 2][702/834] loss: 0.6272 PSNR_train: 32.5533\n",
      "[epoch 2][703/834] loss: 0.5977 PSNR_train: 32.6957\n",
      "[epoch 2][704/834] loss: 0.6537 PSNR_train: 32.2091\n",
      "[epoch 2][705/834] loss: 0.5935 PSNR_train: 32.7508\n",
      "[epoch 2][706/834] loss: 0.6516 PSNR_train: 32.3433\n",
      "[epoch 2][707/834] loss: 0.6298 PSNR_train: 32.4406\n",
      "[epoch 2][708/834] loss: 0.6257 PSNR_train: 32.5344\n",
      "[epoch 2][709/834] loss: 0.6121 PSNR_train: 32.5600\n",
      "[epoch 2][710/834] loss: 0.6164 PSNR_train: 32.6292\n",
      "[epoch 2][711/834] loss: 0.6395 PSNR_train: 32.4246\n",
      "[epoch 2][712/834] loss: 0.6116 PSNR_train: 32.5995\n",
      "[epoch 2][713/834] loss: 0.6264 PSNR_train: 32.4397\n",
      "[epoch 2][714/834] loss: 0.6455 PSNR_train: 32.3402\n",
      "[epoch 2][715/834] loss: 0.6331 PSNR_train: 32.3791\n",
      "[epoch 2][716/834] loss: 0.6219 PSNR_train: 32.4378\n",
      "[epoch 2][717/834] loss: 0.6643 PSNR_train: 32.1676\n",
      "[epoch 2][718/834] loss: 0.5654 PSNR_train: 32.8416\n",
      "[epoch 2][719/834] loss: 0.6232 PSNR_train: 32.4088\n",
      "[epoch 2][720/834] loss: 0.5983 PSNR_train: 32.6366\n",
      "[epoch 2][721/834] loss: 0.5910 PSNR_train: 32.6696\n",
      "[epoch 2][722/834] loss: 0.6373 PSNR_train: 32.2894\n",
      "[epoch 2][723/834] loss: 0.5988 PSNR_train: 32.7466\n",
      "[epoch 2][724/834] loss: 0.6895 PSNR_train: 31.9638\n",
      "[epoch 2][725/834] loss: 0.5787 PSNR_train: 32.8914\n",
      "[epoch 2][726/834] loss: 0.5983 PSNR_train: 32.7225\n",
      "[epoch 2][727/834] loss: 0.6236 PSNR_train: 32.5273\n",
      "[epoch 2][728/834] loss: 0.6171 PSNR_train: 32.5793\n",
      "[epoch 2][729/834] loss: 0.5937 PSNR_train: 32.7988\n",
      "[epoch 2][730/834] loss: 0.5890 PSNR_train: 32.8913\n",
      "[epoch 2][731/834] loss: 0.5720 PSNR_train: 32.9636\n",
      "[epoch 2][732/834] loss: 0.6056 PSNR_train: 32.6780\n",
      "[epoch 2][733/834] loss: 0.6394 PSNR_train: 32.4554\n",
      "[epoch 2][734/834] loss: 0.6369 PSNR_train: 32.2969\n",
      "[epoch 2][735/834] loss: 0.6102 PSNR_train: 32.4816\n",
      "[epoch 2][736/834] loss: 0.6113 PSNR_train: 32.6020\n",
      "[epoch 2][737/834] loss: 0.6169 PSNR_train: 32.6090\n",
      "[epoch 2][738/834] loss: 0.6461 PSNR_train: 32.5896\n",
      "[epoch 2][739/834] loss: 0.6203 PSNR_train: 32.4946\n",
      "[epoch 2][740/834] loss: 0.5799 PSNR_train: 32.7905\n",
      "[epoch 2][741/834] loss: 0.6036 PSNR_train: 32.6927\n",
      "[epoch 2][742/834] loss: 0.6576 PSNR_train: 32.2929\n",
      "[epoch 2][743/834] loss: 0.6093 PSNR_train: 32.5931\n",
      "[epoch 2][744/834] loss: 0.5921 PSNR_train: 32.7952\n",
      "[epoch 2][745/834] loss: 0.5868 PSNR_train: 32.6766\n",
      "[epoch 2][746/834] loss: 0.6323 PSNR_train: 32.2932\n",
      "[epoch 2][747/834] loss: 0.6393 PSNR_train: 32.4707\n",
      "[epoch 2][748/834] loss: 0.5796 PSNR_train: 32.7994\n",
      "[epoch 2][749/834] loss: 0.6566 PSNR_train: 32.2629\n",
      "[epoch 2][750/834] loss: 0.5871 PSNR_train: 32.6994\n",
      "[epoch 2][751/834] loss: 0.5735 PSNR_train: 32.8648\n",
      "[epoch 2][752/834] loss: 0.6089 PSNR_train: 32.4528\n",
      "[epoch 2][753/834] loss: 0.6066 PSNR_train: 32.5790\n",
      "[epoch 2][754/834] loss: 0.6101 PSNR_train: 32.5632\n",
      "[epoch 2][755/834] loss: 0.5985 PSNR_train: 32.6426\n",
      "[epoch 2][756/834] loss: 0.6187 PSNR_train: 32.5809\n",
      "[epoch 2][757/834] loss: 0.6128 PSNR_train: 32.5947\n",
      "[epoch 2][758/834] loss: 0.6415 PSNR_train: 32.3531\n",
      "[epoch 2][759/834] loss: 0.6646 PSNR_train: 32.2287\n",
      "[epoch 2][760/834] loss: 0.6789 PSNR_train: 32.1596\n",
      "[epoch 2][761/834] loss: 0.6061 PSNR_train: 32.6975\n",
      "[epoch 2][762/834] loss: 0.6348 PSNR_train: 32.3330\n",
      "[epoch 2][763/834] loss: 0.6366 PSNR_train: 32.3505\n",
      "[epoch 2][764/834] loss: 0.5955 PSNR_train: 32.7821\n",
      "[epoch 2][765/834] loss: 0.6195 PSNR_train: 32.6085\n",
      "[epoch 2][766/834] loss: 0.6224 PSNR_train: 32.5603\n",
      "[epoch 2][767/834] loss: 0.6135 PSNR_train: 32.7193\n",
      "[epoch 2][768/834] loss: 0.5904 PSNR_train: 32.6589\n",
      "[epoch 2][769/834] loss: 0.6119 PSNR_train: 32.6643\n",
      "[epoch 2][770/834] loss: 0.5715 PSNR_train: 32.8501\n",
      "[epoch 2][771/834] loss: 0.6530 PSNR_train: 32.4817\n",
      "[epoch 2][772/834] loss: 0.6114 PSNR_train: 32.5980\n",
      "[epoch 2][773/834] loss: 0.6263 PSNR_train: 32.5381\n",
      "[epoch 2][774/834] loss: 0.5978 PSNR_train: 32.7162\n",
      "[epoch 2][775/834] loss: 0.7074 PSNR_train: 31.9384\n",
      "[epoch 2][776/834] loss: 0.6395 PSNR_train: 32.4236\n",
      "[epoch 2][777/834] loss: 0.6206 PSNR_train: 32.4940\n",
      "[epoch 2][778/834] loss: 0.6313 PSNR_train: 32.4957\n",
      "[epoch 2][779/834] loss: 0.5660 PSNR_train: 33.0592\n",
      "[epoch 2][780/834] loss: 0.6339 PSNR_train: 32.4816\n",
      "[epoch 2][781/834] loss: 0.5718 PSNR_train: 32.7816\n",
      "[epoch 2][782/834] loss: 0.6100 PSNR_train: 32.6125\n",
      "[epoch 2][783/834] loss: 0.5773 PSNR_train: 32.7731\n",
      "[epoch 2][784/834] loss: 0.6224 PSNR_train: 32.4015\n",
      "[epoch 2][785/834] loss: 0.6295 PSNR_train: 32.5147\n",
      "[epoch 2][786/834] loss: 0.6770 PSNR_train: 32.1633\n",
      "[epoch 2][787/834] loss: 0.5883 PSNR_train: 32.6404\n",
      "[epoch 2][788/834] loss: 0.6025 PSNR_train: 32.6172\n",
      "[epoch 2][789/834] loss: 0.6377 PSNR_train: 32.5392\n",
      "[epoch 2][790/834] loss: 0.6035 PSNR_train: 32.5738\n",
      "[epoch 2][791/834] loss: 0.6356 PSNR_train: 32.3955\n",
      "[epoch 2][792/834] loss: 0.6292 PSNR_train: 32.5549\n",
      "[epoch 2][793/834] loss: 0.6273 PSNR_train: 32.4695\n",
      "[epoch 2][794/834] loss: 0.5830 PSNR_train: 32.8754\n",
      "[epoch 2][795/834] loss: 0.6459 PSNR_train: 32.4884\n",
      "[epoch 2][796/834] loss: 0.6339 PSNR_train: 32.5216\n",
      "[epoch 2][797/834] loss: 0.6318 PSNR_train: 32.5619\n",
      "[epoch 2][798/834] loss: 0.6179 PSNR_train: 32.6823\n",
      "[epoch 2][799/834] loss: 0.6432 PSNR_train: 32.3148\n",
      "[epoch 2][800/834] loss: 0.6032 PSNR_train: 32.7736\n",
      "[epoch 2][801/834] loss: 0.5821 PSNR_train: 32.8272\n",
      "[epoch 2][802/834] loss: 0.5872 PSNR_train: 32.7118\n",
      "[epoch 2][803/834] loss: 0.5844 PSNR_train: 32.7234\n",
      "[epoch 2][804/834] loss: 0.6251 PSNR_train: 32.4959\n",
      "[epoch 2][805/834] loss: 0.5968 PSNR_train: 32.6811\n",
      "[epoch 2][806/834] loss: 0.5944 PSNR_train: 32.6700\n",
      "[epoch 2][807/834] loss: 0.6274 PSNR_train: 32.4055\n",
      "[epoch 2][808/834] loss: 0.6090 PSNR_train: 32.5297\n",
      "[epoch 2][809/834] loss: 0.6232 PSNR_train: 32.5487\n",
      "[epoch 2][810/834] loss: 0.6271 PSNR_train: 32.6897\n",
      "[epoch 2][811/834] loss: 0.6225 PSNR_train: 32.4941\n",
      "[epoch 2][812/834] loss: 0.5862 PSNR_train: 32.8971\n",
      "[epoch 2][813/834] loss: 0.5895 PSNR_train: 32.8205\n",
      "[epoch 2][814/834] loss: 0.6046 PSNR_train: 32.8445\n",
      "[epoch 2][815/834] loss: 0.6064 PSNR_train: 32.6800\n",
      "[epoch 2][816/834] loss: 0.6355 PSNR_train: 32.3834\n",
      "[epoch 2][817/834] loss: 0.6016 PSNR_train: 32.4939\n",
      "[epoch 2][818/834] loss: 0.6667 PSNR_train: 32.2600\n",
      "[epoch 2][819/834] loss: 0.5869 PSNR_train: 32.7572\n",
      "[epoch 2][820/834] loss: 0.6492 PSNR_train: 32.4243\n",
      "[epoch 2][821/834] loss: 0.5966 PSNR_train: 32.6326\n",
      "[epoch 2][822/834] loss: 0.6344 PSNR_train: 32.5038\n",
      "[epoch 2][823/834] loss: 0.6383 PSNR_train: 32.3719\n",
      "[epoch 2][824/834] loss: 0.5969 PSNR_train: 32.7924\n",
      "[epoch 2][825/834] loss: 0.5933 PSNR_train: 32.7835\n",
      "[epoch 2][826/834] loss: 0.6173 PSNR_train: 32.4814\n",
      "[epoch 2][827/834] loss: 0.6425 PSNR_train: 32.4101\n",
      "[epoch 2][828/834] loss: 0.6279 PSNR_train: 32.4923\n",
      "[epoch 2][829/834] loss: 0.6325 PSNR_train: 32.4463\n",
      "[epoch 2][830/834] loss: 0.5849 PSNR_train: 32.8271\n",
      "[epoch 2][831/834] loss: 0.6161 PSNR_train: 32.5285\n",
      "[epoch 2][832/834] loss: 0.6224 PSNR_train: 32.4770\n",
      "[epoch 2][833/834] loss: 0.6205 PSNR_train: 32.5083\n",
      "[epoch 2][834/834] loss: 0.6150 PSNR_train: 32.4581\n",
      "\n",
      "[epoch 2] PSNR_val: 35.7454\n",
      "learning rate 0.000010\n",
      "[epoch 3][1/834] loss: 0.6175 PSNR_train: 32.6931\n",
      "[epoch 3][2/834] loss: 0.5940 PSNR_train: 32.7339\n",
      "[epoch 3][3/834] loss: 0.6859 PSNR_train: 32.1363\n",
      "[epoch 3][4/834] loss: 0.6211 PSNR_train: 32.6195\n",
      "[epoch 3][5/834] loss: 0.6372 PSNR_train: 32.4488\n",
      "[epoch 3][6/834] loss: 0.6025 PSNR_train: 32.7180\n",
      "[epoch 3][7/834] loss: 0.6014 PSNR_train: 32.7164\n",
      "[epoch 3][8/834] loss: 0.6444 PSNR_train: 32.3905\n",
      "[epoch 3][9/834] loss: 0.5885 PSNR_train: 32.7428\n",
      "[epoch 3][10/834] loss: 0.6127 PSNR_train: 32.5363\n",
      "[epoch 3][11/834] loss: 0.6171 PSNR_train: 32.6100\n",
      "[epoch 3][12/834] loss: 0.6543 PSNR_train: 32.3958\n",
      "[epoch 3][13/834] loss: 0.5985 PSNR_train: 32.6456\n",
      "[epoch 3][14/834] loss: 0.5892 PSNR_train: 32.8452\n",
      "[epoch 3][15/834] loss: 0.5840 PSNR_train: 32.7931\n",
      "[epoch 3][16/834] loss: 0.6141 PSNR_train: 32.5990\n",
      "[epoch 3][17/834] loss: 0.6090 PSNR_train: 32.5684\n",
      "[epoch 3][18/834] loss: 0.6129 PSNR_train: 32.7152\n",
      "[epoch 3][19/834] loss: 0.6012 PSNR_train: 32.6626\n",
      "[epoch 3][20/834] loss: 0.6107 PSNR_train: 32.6057\n",
      "[epoch 3][21/834] loss: 0.6435 PSNR_train: 32.1874\n",
      "[epoch 3][22/834] loss: 0.6530 PSNR_train: 32.3313\n",
      "[epoch 3][23/834] loss: 0.6063 PSNR_train: 32.5311\n",
      "[epoch 3][24/834] loss: 0.6081 PSNR_train: 32.7239\n",
      "[epoch 3][25/834] loss: 0.6552 PSNR_train: 32.2990\n",
      "[epoch 3][26/834] loss: 0.5825 PSNR_train: 32.8899\n",
      "[epoch 3][27/834] loss: 0.6229 PSNR_train: 32.4954\n",
      "[epoch 3][28/834] loss: 0.5812 PSNR_train: 32.8441\n",
      "[epoch 3][29/834] loss: 0.6487 PSNR_train: 32.2987\n",
      "[epoch 3][30/834] loss: 0.6418 PSNR_train: 32.3704\n",
      "[epoch 3][31/834] loss: 0.5885 PSNR_train: 32.7767\n",
      "[epoch 3][32/834] loss: 0.6172 PSNR_train: 32.7153\n",
      "[epoch 3][33/834] loss: 0.5858 PSNR_train: 32.8290\n",
      "[epoch 3][34/834] loss: 0.6012 PSNR_train: 32.7678\n",
      "[epoch 3][35/834] loss: 0.6580 PSNR_train: 32.3542\n",
      "[epoch 3][36/834] loss: 0.6147 PSNR_train: 32.6711\n",
      "[epoch 3][37/834] loss: 0.6296 PSNR_train: 32.4739\n",
      "[epoch 3][38/834] loss: 0.6136 PSNR_train: 32.6462\n",
      "[epoch 3][39/834] loss: 0.6270 PSNR_train: 32.5096\n",
      "[epoch 3][40/834] loss: 0.6363 PSNR_train: 32.3170\n",
      "[epoch 3][41/834] loss: 0.6132 PSNR_train: 32.5872\n",
      "[epoch 3][42/834] loss: 0.5662 PSNR_train: 33.0922\n",
      "[epoch 3][43/834] loss: 0.5825 PSNR_train: 32.8613\n",
      "[epoch 3][44/834] loss: 0.6408 PSNR_train: 32.4256\n",
      "[epoch 3][45/834] loss: 0.6121 PSNR_train: 32.5745\n",
      "[epoch 3][46/834] loss: 0.6203 PSNR_train: 32.6341\n",
      "[epoch 3][47/834] loss: 0.6090 PSNR_train: 32.5447\n",
      "[epoch 3][48/834] loss: 0.6242 PSNR_train: 32.4448\n",
      "[epoch 3][49/834] loss: 0.6377 PSNR_train: 32.4574\n",
      "[epoch 3][50/834] loss: 0.6225 PSNR_train: 32.4474\n",
      "[epoch 3][51/834] loss: 0.6579 PSNR_train: 32.2177\n",
      "[epoch 3][52/834] loss: 0.6199 PSNR_train: 32.6208\n",
      "[epoch 3][53/834] loss: 0.6101 PSNR_train: 32.6793\n",
      "[epoch 3][54/834] loss: 0.6304 PSNR_train: 32.3730\n",
      "[epoch 3][55/834] loss: 0.6402 PSNR_train: 32.2185\n",
      "[epoch 3][56/834] loss: 0.6022 PSNR_train: 32.6636\n",
      "[epoch 3][57/834] loss: 0.5820 PSNR_train: 32.8268\n",
      "[epoch 3][58/834] loss: 0.5629 PSNR_train: 33.0762\n",
      "[epoch 3][59/834] loss: 0.6310 PSNR_train: 32.5265\n",
      "[epoch 3][60/834] loss: 0.6455 PSNR_train: 32.4414\n",
      "[epoch 3][61/834] loss: 0.6483 PSNR_train: 32.3865\n",
      "[epoch 3][62/834] loss: 0.6145 PSNR_train: 32.5564\n",
      "[epoch 3][63/834] loss: 0.6397 PSNR_train: 32.3333\n",
      "[epoch 3][64/834] loss: 0.5733 PSNR_train: 33.0875\n",
      "[epoch 3][65/834] loss: 0.6220 PSNR_train: 32.5881\n",
      "[epoch 3][66/834] loss: 0.6292 PSNR_train: 32.5879\n",
      "[epoch 3][67/834] loss: 0.6167 PSNR_train: 32.5917\n",
      "[epoch 3][68/834] loss: 0.6424 PSNR_train: 32.4217\n",
      "[epoch 3][69/834] loss: 0.6290 PSNR_train: 32.6226\n",
      "[epoch 3][70/834] loss: 0.6268 PSNR_train: 32.5496\n",
      "[epoch 3][71/834] loss: 0.6217 PSNR_train: 32.4554\n",
      "[epoch 3][72/834] loss: 0.6251 PSNR_train: 32.5736\n",
      "[epoch 3][73/834] loss: 0.5821 PSNR_train: 32.8169\n",
      "[epoch 3][74/834] loss: 0.5985 PSNR_train: 32.7762\n",
      "[epoch 3][75/834] loss: 0.5500 PSNR_train: 33.1929\n",
      "[epoch 3][76/834] loss: 0.6197 PSNR_train: 32.5981\n",
      "[epoch 3][77/834] loss: 0.5908 PSNR_train: 32.6920\n",
      "[epoch 3][78/834] loss: 0.5972 PSNR_train: 32.7646\n",
      "[epoch 3][79/834] loss: 0.6190 PSNR_train: 32.4796\n",
      "[epoch 3][80/834] loss: 0.6388 PSNR_train: 32.4525\n",
      "[epoch 3][81/834] loss: 0.6504 PSNR_train: 32.2899\n",
      "[epoch 3][82/834] loss: 0.6351 PSNR_train: 32.4889\n",
      "[epoch 3][83/834] loss: 0.6841 PSNR_train: 32.2507\n",
      "[epoch 3][84/834] loss: 0.6352 PSNR_train: 32.5028\n",
      "[epoch 3][85/834] loss: 0.6533 PSNR_train: 32.1866\n",
      "[epoch 3][86/834] loss: 0.6397 PSNR_train: 32.3285\n",
      "[epoch 3][87/834] loss: 0.6050 PSNR_train: 32.6146\n",
      "[epoch 3][88/834] loss: 0.6106 PSNR_train: 32.5137\n",
      "[epoch 3][89/834] loss: 0.6123 PSNR_train: 32.5815\n",
      "[epoch 3][90/834] loss: 0.6107 PSNR_train: 32.7870\n",
      "[epoch 3][91/834] loss: 0.6306 PSNR_train: 32.4896\n",
      "[epoch 3][92/834] loss: 0.6125 PSNR_train: 32.6778\n",
      "[epoch 3][93/834] loss: 0.5946 PSNR_train: 32.7818\n",
      "[epoch 3][94/834] loss: 0.6273 PSNR_train: 32.4442\n",
      "[epoch 3][95/834] loss: 0.6367 PSNR_train: 32.5219\n",
      "[epoch 3][96/834] loss: 0.6213 PSNR_train: 32.5008\n",
      "[epoch 3][97/834] loss: 0.6155 PSNR_train: 32.5650\n",
      "[epoch 3][98/834] loss: 0.6517 PSNR_train: 32.3659\n",
      "[epoch 3][99/834] loss: 0.6298 PSNR_train: 32.5134\n",
      "[epoch 3][100/834] loss: 0.5864 PSNR_train: 32.7101\n",
      "[epoch 3][101/834] loss: 0.6146 PSNR_train: 32.6180\n",
      "[epoch 3][102/834] loss: 0.6138 PSNR_train: 32.5136\n",
      "[epoch 3][103/834] loss: 0.5871 PSNR_train: 32.7669\n",
      "[epoch 3][104/834] loss: 0.6364 PSNR_train: 32.4706\n",
      "[epoch 3][105/834] loss: 0.5930 PSNR_train: 32.6989\n",
      "[epoch 3][106/834] loss: 0.5980 PSNR_train: 32.7092\n",
      "[epoch 3][107/834] loss: 0.5655 PSNR_train: 32.9900\n",
      "[epoch 3][108/834] loss: 0.6823 PSNR_train: 32.2807\n",
      "[epoch 3][109/834] loss: 0.5791 PSNR_train: 32.9235\n",
      "[epoch 3][110/834] loss: 0.5781 PSNR_train: 32.8126\n",
      "[epoch 3][111/834] loss: 0.5996 PSNR_train: 32.5861\n",
      "[epoch 3][112/834] loss: 0.5952 PSNR_train: 32.7551\n",
      "[epoch 3][113/834] loss: 0.6055 PSNR_train: 32.5999\n",
      "[epoch 3][114/834] loss: 0.6119 PSNR_train: 32.5544\n",
      "[epoch 3][115/834] loss: 0.6714 PSNR_train: 32.3829\n",
      "[epoch 3][116/834] loss: 0.5789 PSNR_train: 32.8214\n",
      "[epoch 3][117/834] loss: 0.6270 PSNR_train: 32.4686\n",
      "[epoch 3][118/834] loss: 0.6165 PSNR_train: 32.4877\n",
      "[epoch 3][119/834] loss: 0.6312 PSNR_train: 32.4406\n",
      "[epoch 3][120/834] loss: 0.5699 PSNR_train: 32.8067\n",
      "[epoch 3][121/834] loss: 0.5979 PSNR_train: 32.6461\n",
      "[epoch 3][122/834] loss: 0.5907 PSNR_train: 32.8702\n",
      "[epoch 3][123/834] loss: 0.6007 PSNR_train: 32.6771\n",
      "[epoch 3][124/834] loss: 0.6009 PSNR_train: 32.5796\n",
      "[epoch 3][125/834] loss: 0.6391 PSNR_train: 32.3848\n",
      "[epoch 3][126/834] loss: 0.6195 PSNR_train: 32.6097\n",
      "[epoch 3][127/834] loss: 0.6109 PSNR_train: 32.6032\n",
      "[epoch 3][128/834] loss: 0.6502 PSNR_train: 32.4990\n",
      "[epoch 3][129/834] loss: 0.5659 PSNR_train: 33.0376\n",
      "[epoch 3][130/834] loss: 0.6000 PSNR_train: 32.7910\n",
      "[epoch 3][131/834] loss: 0.5911 PSNR_train: 32.8354\n",
      "[epoch 3][132/834] loss: 0.5898 PSNR_train: 32.8994\n",
      "[epoch 3][133/834] loss: 0.6345 PSNR_train: 32.4214\n",
      "[epoch 3][134/834] loss: 0.6010 PSNR_train: 32.7227\n",
      "[epoch 3][135/834] loss: 0.5701 PSNR_train: 32.9365\n",
      "[epoch 3][136/834] loss: 0.6465 PSNR_train: 32.3787\n",
      "[epoch 3][137/834] loss: 0.6066 PSNR_train: 32.6365\n",
      "[epoch 3][138/834] loss: 0.6237 PSNR_train: 32.5960\n",
      "[epoch 3][139/834] loss: 0.5673 PSNR_train: 32.9366\n",
      "[epoch 3][140/834] loss: 0.6198 PSNR_train: 32.5613\n",
      "[epoch 3][141/834] loss: 0.6330 PSNR_train: 32.4492\n",
      "[epoch 3][142/834] loss: 0.6661 PSNR_train: 32.3616\n",
      "[epoch 3][143/834] loss: 0.6174 PSNR_train: 32.5478\n",
      "[epoch 3][144/834] loss: 0.6474 PSNR_train: 32.4257\n",
      "[epoch 3][145/834] loss: 0.6228 PSNR_train: 32.3931\n",
      "[epoch 3][146/834] loss: 0.5874 PSNR_train: 32.7933\n",
      "[epoch 3][147/834] loss: 0.6479 PSNR_train: 32.3347\n",
      "[epoch 3][148/834] loss: 0.6229 PSNR_train: 32.4914\n",
      "[epoch 3][149/834] loss: 0.6470 PSNR_train: 32.4100\n",
      "[epoch 3][150/834] loss: 0.5777 PSNR_train: 32.8868\n",
      "[epoch 3][151/834] loss: 0.5992 PSNR_train: 32.7024\n",
      "[epoch 3][152/834] loss: 0.5767 PSNR_train: 32.8997\n",
      "[epoch 3][153/834] loss: 0.6092 PSNR_train: 32.7682\n",
      "[epoch 3][154/834] loss: 0.6346 PSNR_train: 32.4523\n",
      "[epoch 3][155/834] loss: 0.6013 PSNR_train: 32.6201\n",
      "[epoch 3][156/834] loss: 0.5707 PSNR_train: 32.9929\n",
      "[epoch 3][157/834] loss: 0.5779 PSNR_train: 33.0448\n",
      "[epoch 3][158/834] loss: 0.5926 PSNR_train: 32.7498\n",
      "[epoch 3][159/834] loss: 0.6312 PSNR_train: 32.4557\n",
      "[epoch 3][160/834] loss: 0.6434 PSNR_train: 32.3641\n",
      "[epoch 3][161/834] loss: 0.5996 PSNR_train: 32.6757\n",
      "[epoch 3][162/834] loss: 0.5753 PSNR_train: 32.6838\n",
      "[epoch 3][163/834] loss: 0.6470 PSNR_train: 32.2342\n",
      "[epoch 3][164/834] loss: 0.5937 PSNR_train: 32.6012\n",
      "[epoch 3][165/834] loss: 0.6608 PSNR_train: 32.3072\n",
      "[epoch 3][166/834] loss: 0.6098 PSNR_train: 32.5642\n",
      "[epoch 3][167/834] loss: 0.6116 PSNR_train: 32.6939\n",
      "[epoch 3][168/834] loss: 0.6236 PSNR_train: 32.4338\n",
      "[epoch 3][169/834] loss: 0.6007 PSNR_train: 32.6648\n",
      "[epoch 3][170/834] loss: 0.6234 PSNR_train: 32.5544\n",
      "[epoch 3][171/834] loss: 0.5892 PSNR_train: 32.7694\n",
      "[epoch 3][172/834] loss: 0.5723 PSNR_train: 32.7960\n",
      "[epoch 3][173/834] loss: 0.6138 PSNR_train: 32.6751\n",
      "[epoch 3][174/834] loss: 0.6283 PSNR_train: 32.5795\n",
      "[epoch 3][175/834] loss: 0.6297 PSNR_train: 32.4558\n",
      "[epoch 3][176/834] loss: 0.6416 PSNR_train: 32.4026\n",
      "[epoch 3][177/834] loss: 0.6185 PSNR_train: 32.5230\n",
      "[epoch 3][178/834] loss: 0.6063 PSNR_train: 32.5979\n",
      "[epoch 3][179/834] loss: 0.6154 PSNR_train: 32.4991\n",
      "[epoch 3][180/834] loss: 0.5958 PSNR_train: 32.6906\n",
      "[epoch 3][181/834] loss: 0.6068 PSNR_train: 32.6133\n",
      "[epoch 3][182/834] loss: 0.6092 PSNR_train: 32.5365\n",
      "[epoch 3][183/834] loss: 0.6187 PSNR_train: 32.6082\n",
      "[epoch 3][184/834] loss: 0.6089 PSNR_train: 32.5936\n",
      "[epoch 3][185/834] loss: 0.6109 PSNR_train: 32.5866\n",
      "[epoch 3][186/834] loss: 0.6303 PSNR_train: 32.6037\n",
      "[epoch 3][187/834] loss: 0.6290 PSNR_train: 32.4218\n",
      "[epoch 3][188/834] loss: 0.5911 PSNR_train: 32.6719\n",
      "[epoch 3][189/834] loss: 0.6455 PSNR_train: 32.3701\n",
      "[epoch 3][190/834] loss: 0.6022 PSNR_train: 32.6680\n",
      "[epoch 3][191/834] loss: 0.5951 PSNR_train: 32.6419\n",
      "[epoch 3][192/834] loss: 0.5887 PSNR_train: 32.6416\n",
      "[epoch 3][193/834] loss: 0.5991 PSNR_train: 32.5942\n",
      "[epoch 3][194/834] loss: 0.5805 PSNR_train: 32.8349\n",
      "[epoch 3][195/834] loss: 0.6235 PSNR_train: 32.4758\n",
      "[epoch 3][196/834] loss: 0.6455 PSNR_train: 32.5546\n",
      "[epoch 3][197/834] loss: 0.6548 PSNR_train: 32.4868\n",
      "[epoch 3][198/834] loss: 0.6075 PSNR_train: 32.5900\n",
      "[epoch 3][199/834] loss: 0.6282 PSNR_train: 32.5065\n",
      "[epoch 3][200/834] loss: 0.5690 PSNR_train: 33.0890\n",
      "[epoch 3][201/834] loss: 0.6437 PSNR_train: 32.4140\n",
      "[epoch 3][202/834] loss: 0.6126 PSNR_train: 32.7745\n",
      "[epoch 3][203/834] loss: 0.6030 PSNR_train: 32.6006\n",
      "[epoch 3][204/834] loss: 0.6830 PSNR_train: 32.2569\n",
      "[epoch 3][205/834] loss: 0.5706 PSNR_train: 32.8749\n",
      "[epoch 3][206/834] loss: 0.6060 PSNR_train: 32.7802\n",
      "[epoch 3][207/834] loss: 0.5464 PSNR_train: 33.1945\n",
      "[epoch 3][208/834] loss: 0.6279 PSNR_train: 32.5662\n",
      "[epoch 3][209/834] loss: 0.5719 PSNR_train: 32.9468\n",
      "[epoch 3][210/834] loss: 0.6237 PSNR_train: 32.6021\n",
      "[epoch 3][211/834] loss: 0.6322 PSNR_train: 32.5207\n",
      "[epoch 3][212/834] loss: 0.6413 PSNR_train: 32.4637\n",
      "[epoch 3][213/834] loss: 0.6142 PSNR_train: 32.6395\n",
      "[epoch 3][214/834] loss: 0.6396 PSNR_train: 32.3848\n",
      "[epoch 3][215/834] loss: 0.5805 PSNR_train: 32.7581\n",
      "[epoch 3][216/834] loss: 0.6485 PSNR_train: 32.3891\n",
      "[epoch 3][217/834] loss: 0.5892 PSNR_train: 32.7553\n",
      "[epoch 3][218/834] loss: 0.6378 PSNR_train: 32.3996\n",
      "[epoch 3][219/834] loss: 0.5987 PSNR_train: 32.6901\n",
      "[epoch 3][220/834] loss: 0.6152 PSNR_train: 32.6055\n",
      "[epoch 3][221/834] loss: 0.6276 PSNR_train: 32.5420\n",
      "[epoch 3][222/834] loss: 0.6061 PSNR_train: 32.5965\n",
      "[epoch 3][223/834] loss: 0.6420 PSNR_train: 32.4720\n",
      "[epoch 3][224/834] loss: 0.6292 PSNR_train: 32.4210\n",
      "[epoch 3][225/834] loss: 0.5885 PSNR_train: 32.7785\n",
      "[epoch 3][226/834] loss: 0.6129 PSNR_train: 32.6568\n",
      "[epoch 3][227/834] loss: 0.6262 PSNR_train: 32.4674\n",
      "[epoch 3][228/834] loss: 0.6078 PSNR_train: 32.7580\n",
      "[epoch 3][229/834] loss: 0.5771 PSNR_train: 32.8193\n",
      "[epoch 3][230/834] loss: 0.6224 PSNR_train: 32.4050\n",
      "[epoch 3][231/834] loss: 0.6582 PSNR_train: 32.4031\n",
      "[epoch 3][232/834] loss: 0.6297 PSNR_train: 32.4422\n",
      "[epoch 3][233/834] loss: 0.6126 PSNR_train: 32.6261\n",
      "[epoch 3][234/834] loss: 0.6382 PSNR_train: 32.4110\n",
      "[epoch 3][235/834] loss: 0.6341 PSNR_train: 32.3108\n",
      "[epoch 3][236/834] loss: 0.5952 PSNR_train: 32.7259\n",
      "[epoch 3][237/834] loss: 0.6446 PSNR_train: 32.3553\n",
      "[epoch 3][238/834] loss: 0.6268 PSNR_train: 32.5548\n",
      "[epoch 3][239/834] loss: 0.6050 PSNR_train: 32.5416\n",
      "[epoch 3][240/834] loss: 0.6730 PSNR_train: 32.3105\n",
      "[epoch 3][241/834] loss: 0.6079 PSNR_train: 32.4903\n",
      "[epoch 3][242/834] loss: 0.6533 PSNR_train: 32.3155\n",
      "[epoch 3][243/834] loss: 0.5882 PSNR_train: 32.7581\n",
      "[epoch 3][244/834] loss: 0.6622 PSNR_train: 32.3006\n",
      "[epoch 3][245/834] loss: 0.6232 PSNR_train: 32.4673\n",
      "[epoch 3][246/834] loss: 0.6421 PSNR_train: 32.4686\n",
      "[epoch 3][247/834] loss: 0.6154 PSNR_train: 32.6222\n",
      "[epoch 3][248/834] loss: 0.6513 PSNR_train: 32.3740\n",
      "[epoch 3][249/834] loss: 0.5979 PSNR_train: 32.8763\n",
      "[epoch 3][250/834] loss: 0.5899 PSNR_train: 32.7149\n",
      "[epoch 3][251/834] loss: 0.6261 PSNR_train: 32.6663\n",
      "[epoch 3][252/834] loss: 0.5807 PSNR_train: 32.8650\n",
      "[epoch 3][253/834] loss: 0.6451 PSNR_train: 32.4136\n",
      "[epoch 3][254/834] loss: 0.5753 PSNR_train: 32.8182\n",
      "[epoch 3][255/834] loss: 0.6518 PSNR_train: 32.3162\n",
      "[epoch 3][256/834] loss: 0.6274 PSNR_train: 32.4979\n",
      "[epoch 3][257/834] loss: 0.6333 PSNR_train: 32.3945\n",
      "[epoch 3][258/834] loss: 0.5898 PSNR_train: 32.6876\n",
      "[epoch 3][259/834] loss: 0.6033 PSNR_train: 32.6168\n",
      "[epoch 3][260/834] loss: 0.6064 PSNR_train: 32.6641\n",
      "[epoch 3][261/834] loss: 0.5955 PSNR_train: 32.7596\n",
      "[epoch 3][262/834] loss: 0.5785 PSNR_train: 32.8759\n",
      "[epoch 3][263/834] loss: 0.5899 PSNR_train: 32.7685\n",
      "[epoch 3][264/834] loss: 0.6121 PSNR_train: 32.5073\n",
      "[epoch 3][265/834] loss: 0.6008 PSNR_train: 32.7302\n",
      "[epoch 3][266/834] loss: 0.6178 PSNR_train: 32.5566\n",
      "[epoch 3][267/834] loss: 0.6289 PSNR_train: 32.5899\n",
      "[epoch 3][268/834] loss: 0.6715 PSNR_train: 32.2085\n",
      "[epoch 3][269/834] loss: 0.5937 PSNR_train: 32.6440\n",
      "[epoch 3][270/834] loss: 0.6228 PSNR_train: 32.5192\n",
      "[epoch 3][271/834] loss: 0.6062 PSNR_train: 32.6089\n",
      "[epoch 3][272/834] loss: 0.6111 PSNR_train: 32.5165\n",
      "[epoch 3][273/834] loss: 0.5609 PSNR_train: 33.0128\n",
      "[epoch 3][274/834] loss: 0.5937 PSNR_train: 32.7452\n",
      "[epoch 3][275/834] loss: 0.6015 PSNR_train: 32.7124\n",
      "[epoch 3][276/834] loss: 0.6120 PSNR_train: 32.6920\n",
      "[epoch 3][277/834] loss: 0.6072 PSNR_train: 32.5415\n",
      "[epoch 3][278/834] loss: 0.6081 PSNR_train: 32.6193\n",
      "[epoch 3][279/834] loss: 0.6036 PSNR_train: 32.6860\n",
      "[epoch 3][280/834] loss: 0.5882 PSNR_train: 32.8131\n",
      "[epoch 3][281/834] loss: 0.6070 PSNR_train: 32.5448\n",
      "[epoch 3][282/834] loss: 0.6289 PSNR_train: 32.4806\n",
      "[epoch 3][283/834] loss: 0.6144 PSNR_train: 32.6750\n",
      "[epoch 3][284/834] loss: 0.6297 PSNR_train: 32.5577\n",
      "[epoch 3][285/834] loss: 0.5605 PSNR_train: 33.0646\n",
      "[epoch 3][286/834] loss: 0.5845 PSNR_train: 32.7383\n",
      "[epoch 3][287/834] loss: 0.6064 PSNR_train: 32.5022\n",
      "[epoch 3][288/834] loss: 0.6348 PSNR_train: 32.5228\n",
      "[epoch 3][289/834] loss: 0.6114 PSNR_train: 32.7189\n",
      "[epoch 3][290/834] loss: 0.6027 PSNR_train: 32.6804\n",
      "[epoch 3][291/834] loss: 0.6179 PSNR_train: 32.4863\n",
      "[epoch 3][292/834] loss: 0.5846 PSNR_train: 32.7758\n",
      "[epoch 3][293/834] loss: 0.5916 PSNR_train: 32.7045\n",
      "[epoch 3][294/834] loss: 0.5586 PSNR_train: 32.9444\n",
      "[epoch 3][295/834] loss: 0.5907 PSNR_train: 32.7529\n",
      "[epoch 3][296/834] loss: 0.6480 PSNR_train: 32.2908\n",
      "[epoch 3][297/834] loss: 0.6017 PSNR_train: 32.6744\n",
      "[epoch 3][298/834] loss: 0.6270 PSNR_train: 32.5178\n",
      "[epoch 3][299/834] loss: 0.5952 PSNR_train: 32.8490\n",
      "[epoch 3][300/834] loss: 0.6234 PSNR_train: 32.6938\n",
      "[epoch 3][301/834] loss: 0.6547 PSNR_train: 32.2967\n",
      "[epoch 3][302/834] loss: 0.6140 PSNR_train: 32.5470\n",
      "[epoch 3][303/834] loss: 0.5954 PSNR_train: 32.6596\n",
      "[epoch 3][304/834] loss: 0.5880 PSNR_train: 32.7849\n",
      "[epoch 3][305/834] loss: 0.5699 PSNR_train: 32.7744\n",
      "[epoch 3][306/834] loss: 0.6460 PSNR_train: 32.2803\n",
      "[epoch 3][307/834] loss: 0.6219 PSNR_train: 32.6072\n",
      "[epoch 3][308/834] loss: 0.5814 PSNR_train: 32.7459\n",
      "[epoch 3][309/834] loss: 0.5969 PSNR_train: 32.5107\n",
      "[epoch 3][310/834] loss: 0.5988 PSNR_train: 32.7451\n",
      "[epoch 3][311/834] loss: 0.5774 PSNR_train: 32.6989\n",
      "[epoch 3][312/834] loss: 0.6228 PSNR_train: 32.4839\n",
      "[epoch 3][313/834] loss: 0.5952 PSNR_train: 32.7008\n",
      "[epoch 3][314/834] loss: 0.6440 PSNR_train: 32.4124\n",
      "[epoch 3][315/834] loss: 0.6339 PSNR_train: 32.4793\n",
      "[epoch 3][316/834] loss: 0.5994 PSNR_train: 32.6602\n",
      "[epoch 3][317/834] loss: 0.6230 PSNR_train: 32.5381\n",
      "[epoch 3][318/834] loss: 0.5650 PSNR_train: 32.9423\n",
      "[epoch 3][319/834] loss: 0.5842 PSNR_train: 32.8038\n",
      "[epoch 3][320/834] loss: 0.6064 PSNR_train: 32.7227\n",
      "[epoch 3][321/834] loss: 0.5795 PSNR_train: 32.9840\n",
      "[epoch 3][322/834] loss: 0.6293 PSNR_train: 32.4941\n",
      "[epoch 3][323/834] loss: 0.6277 PSNR_train: 32.4458\n",
      "[epoch 3][324/834] loss: 0.6004 PSNR_train: 32.8137\n",
      "[epoch 3][325/834] loss: 0.6366 PSNR_train: 32.4999\n",
      "[epoch 3][326/834] loss: 0.6113 PSNR_train: 32.6207\n",
      "[epoch 3][327/834] loss: 0.6923 PSNR_train: 32.0827\n",
      "[epoch 3][328/834] loss: 0.6501 PSNR_train: 32.3197\n",
      "[epoch 3][329/834] loss: 0.6200 PSNR_train: 32.5518\n",
      "[epoch 3][330/834] loss: 0.6272 PSNR_train: 32.5272\n",
      "[epoch 3][331/834] loss: 0.5983 PSNR_train: 32.7604\n",
      "[epoch 3][332/834] loss: 0.6066 PSNR_train: 32.7219\n",
      "[epoch 3][333/834] loss: 0.6110 PSNR_train: 32.5728\n",
      "[epoch 3][334/834] loss: 0.6576 PSNR_train: 32.3825\n",
      "[epoch 3][335/834] loss: 0.5927 PSNR_train: 32.7742\n",
      "[epoch 3][336/834] loss: 0.6377 PSNR_train: 32.4728\n",
      "[epoch 3][337/834] loss: 0.6075 PSNR_train: 32.6143\n",
      "[epoch 3][338/834] loss: 0.6182 PSNR_train: 32.5286\n",
      "[epoch 3][339/834] loss: 0.5767 PSNR_train: 32.9649\n",
      "[epoch 3][340/834] loss: 0.5962 PSNR_train: 32.7019\n",
      "[epoch 3][341/834] loss: 0.5842 PSNR_train: 32.6989\n",
      "[epoch 3][342/834] loss: 0.6263 PSNR_train: 32.4252\n",
      "[epoch 3][343/834] loss: 0.5941 PSNR_train: 32.6926\n",
      "[epoch 3][344/834] loss: 0.6165 PSNR_train: 32.6620\n",
      "[epoch 3][345/834] loss: 0.6067 PSNR_train: 32.6162\n",
      "[epoch 3][346/834] loss: 0.6271 PSNR_train: 32.5682\n",
      "[epoch 3][347/834] loss: 0.5755 PSNR_train: 32.9559\n",
      "[epoch 3][348/834] loss: 0.5819 PSNR_train: 32.8394\n",
      "[epoch 3][349/834] loss: 0.5865 PSNR_train: 32.8481\n",
      "[epoch 3][350/834] loss: 0.5855 PSNR_train: 32.8829\n",
      "[epoch 3][351/834] loss: 0.6309 PSNR_train: 32.4375\n",
      "[epoch 3][352/834] loss: 0.5926 PSNR_train: 32.6147\n",
      "[epoch 3][353/834] loss: 0.6209 PSNR_train: 32.5916\n",
      "[epoch 3][354/834] loss: 0.6081 PSNR_train: 32.6251\n",
      "[epoch 3][355/834] loss: 0.5990 PSNR_train: 32.6776\n",
      "[epoch 3][356/834] loss: 0.6321 PSNR_train: 32.4208\n",
      "[epoch 3][357/834] loss: 0.5853 PSNR_train: 32.7629\n",
      "[epoch 3][358/834] loss: 0.6281 PSNR_train: 32.4543\n",
      "[epoch 3][359/834] loss: 0.5970 PSNR_train: 32.7303\n",
      "[epoch 3][360/834] loss: 0.6028 PSNR_train: 32.5750\n",
      "[epoch 3][361/834] loss: 0.5692 PSNR_train: 32.8280\n",
      "[epoch 3][362/834] loss: 0.6133 PSNR_train: 32.5034\n",
      "[epoch 3][363/834] loss: 0.5964 PSNR_train: 32.6528\n",
      "[epoch 3][364/834] loss: 0.6060 PSNR_train: 32.6496\n",
      "[epoch 3][365/834] loss: 0.6064 PSNR_train: 32.6194\n",
      "[epoch 3][366/834] loss: 0.6142 PSNR_train: 32.6407\n",
      "[epoch 3][367/834] loss: 0.5999 PSNR_train: 32.5665\n",
      "[epoch 3][368/834] loss: 0.6213 PSNR_train: 32.6068\n",
      "[epoch 3][369/834] loss: 0.6163 PSNR_train: 32.5792\n",
      "[epoch 3][370/834] loss: 0.5674 PSNR_train: 32.9283\n",
      "[epoch 3][371/834] loss: 0.6110 PSNR_train: 32.5477\n",
      "[epoch 3][372/834] loss: 0.5965 PSNR_train: 32.8162\n",
      "[epoch 3][373/834] loss: 0.5814 PSNR_train: 32.8290\n",
      "[epoch 3][374/834] loss: 0.6352 PSNR_train: 32.4180\n",
      "[epoch 3][375/834] loss: 0.5959 PSNR_train: 32.5477\n",
      "[epoch 3][376/834] loss: 0.5890 PSNR_train: 32.8616\n",
      "[epoch 3][377/834] loss: 0.6186 PSNR_train: 32.5444\n",
      "[epoch 3][378/834] loss: 0.5829 PSNR_train: 32.8495\n",
      "[epoch 3][379/834] loss: 0.5733 PSNR_train: 32.8428\n",
      "[epoch 3][380/834] loss: 0.5683 PSNR_train: 32.9392\n",
      "[epoch 3][381/834] loss: 0.5713 PSNR_train: 32.8896\n",
      "[epoch 3][382/834] loss: 0.5872 PSNR_train: 32.6681\n",
      "[epoch 3][383/834] loss: 0.6254 PSNR_train: 32.4972\n",
      "[epoch 3][384/834] loss: 0.6118 PSNR_train: 32.6105\n",
      "[epoch 3][385/834] loss: 0.6501 PSNR_train: 32.3017\n",
      "[epoch 3][386/834] loss: 0.6147 PSNR_train: 32.6035\n",
      "[epoch 3][387/834] loss: 0.5901 PSNR_train: 32.6970\n",
      "[epoch 3][388/834] loss: 0.6025 PSNR_train: 32.6217\n",
      "[epoch 3][389/834] loss: 0.5953 PSNR_train: 32.7431\n",
      "[epoch 3][390/834] loss: 0.5992 PSNR_train: 32.7694\n",
      "[epoch 3][391/834] loss: 0.5866 PSNR_train: 32.6703\n",
      "[epoch 3][392/834] loss: 0.6126 PSNR_train: 32.5976\n",
      "[epoch 3][393/834] loss: 0.5941 PSNR_train: 32.8231\n",
      "[epoch 3][394/834] loss: 0.5670 PSNR_train: 32.8996\n",
      "[epoch 3][395/834] loss: 0.6470 PSNR_train: 32.5133\n",
      "[epoch 3][396/834] loss: 0.6042 PSNR_train: 32.7021\n",
      "[epoch 3][397/834] loss: 0.6116 PSNR_train: 32.6035\n",
      "[epoch 3][398/834] loss: 0.6113 PSNR_train: 32.5836\n",
      "[epoch 3][399/834] loss: 0.6450 PSNR_train: 32.4329\n",
      "[epoch 3][400/834] loss: 0.6149 PSNR_train: 32.6220\n",
      "[epoch 3][401/834] loss: 0.6016 PSNR_train: 32.6857\n",
      "[epoch 3][402/834] loss: 0.6110 PSNR_train: 32.4508\n",
      "[epoch 3][403/834] loss: 0.6657 PSNR_train: 32.2064\n",
      "[epoch 3][404/834] loss: 0.5891 PSNR_train: 32.8642\n",
      "[epoch 3][405/834] loss: 0.6222 PSNR_train: 32.4509\n",
      "[epoch 3][406/834] loss: 0.5687 PSNR_train: 32.8134\n",
      "[epoch 3][407/834] loss: 0.5796 PSNR_train: 32.7599\n",
      "[epoch 3][408/834] loss: 0.6094 PSNR_train: 32.6056\n",
      "[epoch 3][409/834] loss: 0.5953 PSNR_train: 32.7033\n",
      "[epoch 3][410/834] loss: 0.6556 PSNR_train: 32.3771\n",
      "[epoch 3][411/834] loss: 0.5703 PSNR_train: 33.0447\n",
      "[epoch 3][412/834] loss: 0.5762 PSNR_train: 32.9570\n",
      "[epoch 3][413/834] loss: 0.6245 PSNR_train: 32.5243\n",
      "[epoch 3][414/834] loss: 0.6212 PSNR_train: 32.5611\n",
      "[epoch 3][415/834] loss: 0.5945 PSNR_train: 32.6727\n",
      "[epoch 3][416/834] loss: 0.6053 PSNR_train: 32.8007\n",
      "[epoch 3][417/834] loss: 0.5904 PSNR_train: 32.8032\n",
      "[epoch 3][418/834] loss: 0.6390 PSNR_train: 32.4366\n",
      "[epoch 3][419/834] loss: 0.6106 PSNR_train: 32.4944\n",
      "[epoch 3][420/834] loss: 0.6537 PSNR_train: 32.2924\n",
      "[epoch 3][421/834] loss: 0.5808 PSNR_train: 32.8923\n",
      "[epoch 3][422/834] loss: 0.6222 PSNR_train: 32.5744\n",
      "[epoch 3][423/834] loss: 0.5849 PSNR_train: 32.8204\n",
      "[epoch 3][424/834] loss: 0.6035 PSNR_train: 32.8914\n",
      "[epoch 3][425/834] loss: 0.5655 PSNR_train: 32.9369\n",
      "[epoch 3][426/834] loss: 0.6509 PSNR_train: 32.5458\n",
      "[epoch 3][427/834] loss: 0.5770 PSNR_train: 32.9375\n",
      "[epoch 3][428/834] loss: 0.6084 PSNR_train: 32.7374\n",
      "[epoch 3][429/834] loss: 0.5934 PSNR_train: 32.7920\n",
      "[epoch 3][430/834] loss: 0.6388 PSNR_train: 32.4428\n",
      "[epoch 3][431/834] loss: 0.6165 PSNR_train: 32.6459\n",
      "[epoch 3][432/834] loss: 0.5942 PSNR_train: 32.8908\n",
      "[epoch 3][433/834] loss: 0.5813 PSNR_train: 32.9990\n",
      "[epoch 3][434/834] loss: 0.5894 PSNR_train: 32.8863\n",
      "[epoch 3][435/834] loss: 0.6234 PSNR_train: 32.5550\n",
      "[epoch 3][436/834] loss: 0.6025 PSNR_train: 32.8725\n",
      "[epoch 3][437/834] loss: 0.5876 PSNR_train: 32.8060\n",
      "[epoch 3][438/834] loss: 0.5954 PSNR_train: 32.7341\n",
      "[epoch 3][439/834] loss: 0.5967 PSNR_train: 32.7285\n",
      "[epoch 3][440/834] loss: 0.5853 PSNR_train: 32.8014\n",
      "[epoch 3][441/834] loss: 0.5983 PSNR_train: 32.7672\n",
      "[epoch 3][442/834] loss: 0.5986 PSNR_train: 32.7045\n",
      "[epoch 3][443/834] loss: 0.6058 PSNR_train: 32.5236\n",
      "[epoch 3][444/834] loss: 0.6069 PSNR_train: 32.7462\n",
      "[epoch 3][445/834] loss: 0.5512 PSNR_train: 33.0500\n",
      "[epoch 3][446/834] loss: 0.6055 PSNR_train: 32.6058\n",
      "[epoch 3][447/834] loss: 0.6220 PSNR_train: 32.4189\n",
      "[epoch 3][448/834] loss: 0.5938 PSNR_train: 32.6713\n",
      "[epoch 3][449/834] loss: 0.6261 PSNR_train: 32.4622\n",
      "[epoch 3][450/834] loss: 0.5807 PSNR_train: 32.7120\n",
      "[epoch 3][451/834] loss: 0.5771 PSNR_train: 32.8262\n",
      "[epoch 3][452/834] loss: 0.5713 PSNR_train: 32.8724\n",
      "[epoch 3][453/834] loss: 0.5918 PSNR_train: 32.6387\n",
      "[epoch 3][454/834] loss: 0.6002 PSNR_train: 32.6332\n",
      "[epoch 3][455/834] loss: 0.5840 PSNR_train: 32.8836\n",
      "[epoch 3][456/834] loss: 0.6096 PSNR_train: 32.6077\n",
      "[epoch 3][457/834] loss: 0.6293 PSNR_train: 32.5538\n",
      "[epoch 3][458/834] loss: 0.5877 PSNR_train: 32.7636\n",
      "[epoch 3][459/834] loss: 0.6040 PSNR_train: 32.5716\n",
      "[epoch 3][460/834] loss: 0.6129 PSNR_train: 32.7198\n",
      "[epoch 3][461/834] loss: 0.6298 PSNR_train: 32.3648\n",
      "[epoch 3][462/834] loss: 0.5850 PSNR_train: 32.8226\n",
      "[epoch 3][463/834] loss: 0.6187 PSNR_train: 32.4487\n",
      "[epoch 3][464/834] loss: 0.6368 PSNR_train: 32.5205\n",
      "[epoch 3][465/834] loss: 0.6085 PSNR_train: 32.6955\n",
      "[epoch 3][466/834] loss: 0.6238 PSNR_train: 32.5119\n",
      "[epoch 3][467/834] loss: 0.5992 PSNR_train: 32.5380\n",
      "[epoch 3][468/834] loss: 0.5657 PSNR_train: 32.9158\n",
      "[epoch 3][469/834] loss: 0.5693 PSNR_train: 32.9566\n",
      "[epoch 3][470/834] loss: 0.5912 PSNR_train: 32.7447\n",
      "[epoch 3][471/834] loss: 0.5948 PSNR_train: 32.7575\n",
      "[epoch 3][472/834] loss: 0.6410 PSNR_train: 32.5384\n",
      "[epoch 3][473/834] loss: 0.6130 PSNR_train: 32.7568\n",
      "[epoch 3][474/834] loss: 0.6495 PSNR_train: 32.4034\n",
      "[epoch 3][475/834] loss: 0.6026 PSNR_train: 32.5526\n",
      "[epoch 3][476/834] loss: 0.6273 PSNR_train: 32.5171\n",
      "[epoch 3][477/834] loss: 0.6085 PSNR_train: 32.5666\n",
      "[epoch 3][478/834] loss: 0.6016 PSNR_train: 32.5707\n",
      "[epoch 3][479/834] loss: 0.5839 PSNR_train: 32.6728\n",
      "[epoch 3][480/834] loss: 0.5883 PSNR_train: 32.6979\n",
      "[epoch 3][481/834] loss: 0.5629 PSNR_train: 33.1272\n",
      "[epoch 3][482/834] loss: 0.6135 PSNR_train: 32.6578\n",
      "[epoch 3][483/834] loss: 0.6321 PSNR_train: 32.4409\n",
      "[epoch 3][484/834] loss: 0.6183 PSNR_train: 32.6449\n",
      "[epoch 3][485/834] loss: 0.6343 PSNR_train: 32.2811\n",
      "[epoch 3][486/834] loss: 0.6015 PSNR_train: 32.6595\n",
      "[epoch 3][487/834] loss: 0.5768 PSNR_train: 32.8423\n",
      "[epoch 3][488/834] loss: 0.6161 PSNR_train: 32.5962\n",
      "[epoch 3][489/834] loss: 0.6165 PSNR_train: 32.6317\n",
      "[epoch 3][490/834] loss: 0.6172 PSNR_train: 32.6067\n",
      "[epoch 3][491/834] loss: 0.5534 PSNR_train: 33.0917\n",
      "[epoch 3][492/834] loss: 0.6148 PSNR_train: 32.5492\n",
      "[epoch 3][493/834] loss: 0.6094 PSNR_train: 32.5648\n",
      "[epoch 3][494/834] loss: 0.5615 PSNR_train: 32.9686\n",
      "[epoch 3][495/834] loss: 0.5911 PSNR_train: 32.6448\n",
      "[epoch 3][496/834] loss: 0.5986 PSNR_train: 32.6070\n",
      "[epoch 3][497/834] loss: 0.5840 PSNR_train: 32.7064\n",
      "[epoch 3][498/834] loss: 0.6532 PSNR_train: 32.3224\n",
      "[epoch 3][499/834] loss: 0.5852 PSNR_train: 32.8034\n",
      "[epoch 3][500/834] loss: 0.6006 PSNR_train: 32.6575\n",
      "[epoch 3][501/834] loss: 0.5866 PSNR_train: 32.6864\n",
      "[epoch 3][502/834] loss: 0.6029 PSNR_train: 32.7426\n",
      "[epoch 3][503/834] loss: 0.6283 PSNR_train: 32.5471\n",
      "[epoch 3][504/834] loss: 0.6135 PSNR_train: 32.5113\n",
      "[epoch 3][505/834] loss: 0.5827 PSNR_train: 32.6990\n",
      "[epoch 3][506/834] loss: 0.6296 PSNR_train: 32.5248\n",
      "[epoch 3][507/834] loss: 0.5943 PSNR_train: 32.6050\n",
      "[epoch 3][508/834] loss: 0.6092 PSNR_train: 32.7262\n",
      "[epoch 3][509/834] loss: 0.6007 PSNR_train: 32.5143\n",
      "[epoch 3][510/834] loss: 0.6193 PSNR_train: 32.4368\n",
      "[epoch 3][511/834] loss: 0.5802 PSNR_train: 32.8329\n",
      "[epoch 3][512/834] loss: 0.6198 PSNR_train: 32.7089\n",
      "[epoch 3][513/834] loss: 0.6137 PSNR_train: 32.6617\n",
      "[epoch 3][514/834] loss: 0.5927 PSNR_train: 32.6815\n",
      "[epoch 3][515/834] loss: 0.6176 PSNR_train: 32.5035\n",
      "[epoch 3][516/834] loss: 0.5908 PSNR_train: 32.6348\n",
      "[epoch 3][517/834] loss: 0.6134 PSNR_train: 32.5871\n",
      "[epoch 3][518/834] loss: 0.5591 PSNR_train: 32.9921\n",
      "[epoch 3][519/834] loss: 0.6025 PSNR_train: 32.5555\n",
      "[epoch 3][520/834] loss: 0.5819 PSNR_train: 32.8073\n",
      "[epoch 3][521/834] loss: 0.5845 PSNR_train: 32.7533\n",
      "[epoch 3][522/834] loss: 0.5871 PSNR_train: 32.7522\n",
      "[epoch 3][523/834] loss: 0.6170 PSNR_train: 32.5588\n",
      "[epoch 3][524/834] loss: 0.6567 PSNR_train: 32.4327\n",
      "[epoch 3][525/834] loss: 0.5980 PSNR_train: 32.6914\n",
      "[epoch 3][526/834] loss: 0.5680 PSNR_train: 32.8811\n",
      "[epoch 3][527/834] loss: 0.5884 PSNR_train: 32.7773\n",
      "[epoch 3][528/834] loss: 0.5854 PSNR_train: 32.7551\n",
      "[epoch 3][529/834] loss: 0.5966 PSNR_train: 32.7259\n",
      "[epoch 3][530/834] loss: 0.5809 PSNR_train: 32.8505\n",
      "[epoch 3][531/834] loss: 0.5719 PSNR_train: 32.8114\n",
      "[epoch 3][532/834] loss: 0.5999 PSNR_train: 32.7195\n",
      "[epoch 3][533/834] loss: 0.6198 PSNR_train: 32.5557\n",
      "[epoch 3][534/834] loss: 0.6026 PSNR_train: 32.6921\n",
      "[epoch 3][535/834] loss: 0.5996 PSNR_train: 32.6885\n",
      "[epoch 3][536/834] loss: 0.6216 PSNR_train: 32.5600\n",
      "[epoch 3][537/834] loss: 0.6020 PSNR_train: 32.6915\n",
      "[epoch 3][538/834] loss: 0.5731 PSNR_train: 33.0069\n",
      "[epoch 3][539/834] loss: 0.5892 PSNR_train: 32.7780\n",
      "[epoch 3][540/834] loss: 0.6237 PSNR_train: 32.5545\n",
      "[epoch 3][541/834] loss: 0.5740 PSNR_train: 32.9891\n",
      "[epoch 3][542/834] loss: 0.6283 PSNR_train: 32.5812\n",
      "[epoch 3][543/834] loss: 0.5975 PSNR_train: 32.7580\n",
      "[epoch 3][544/834] loss: 0.6083 PSNR_train: 32.5424\n",
      "[epoch 3][545/834] loss: 0.5817 PSNR_train: 32.8510\n",
      "[epoch 3][546/834] loss: 0.6345 PSNR_train: 32.5019\n",
      "[epoch 3][547/834] loss: 0.6055 PSNR_train: 32.5265\n",
      "[epoch 3][548/834] loss: 0.5950 PSNR_train: 32.6898\n",
      "[epoch 3][549/834] loss: 0.5946 PSNR_train: 32.6645\n",
      "[epoch 3][550/834] loss: 0.6335 PSNR_train: 32.4443\n",
      "[epoch 3][551/834] loss: 0.6197 PSNR_train: 32.6101\n",
      "[epoch 3][552/834] loss: 0.6046 PSNR_train: 32.7609\n",
      "[epoch 3][553/834] loss: 0.5401 PSNR_train: 33.1827\n",
      "[epoch 3][554/834] loss: 0.6181 PSNR_train: 32.5752\n",
      "[epoch 3][555/834] loss: 0.6037 PSNR_train: 32.6655\n",
      "[epoch 3][556/834] loss: 0.6070 PSNR_train: 32.6024\n",
      "[epoch 3][557/834] loss: 0.5832 PSNR_train: 32.8693\n",
      "[epoch 3][558/834] loss: 0.6029 PSNR_train: 32.8994\n",
      "[epoch 3][559/834] loss: 0.6005 PSNR_train: 32.9575\n",
      "[epoch 3][560/834] loss: 0.5987 PSNR_train: 32.5676\n",
      "[epoch 3][561/834] loss: 0.6249 PSNR_train: 32.4400\n",
      "[epoch 3][562/834] loss: 0.5808 PSNR_train: 32.7792\n",
      "[epoch 3][563/834] loss: 0.6089 PSNR_train: 32.6220\n",
      "[epoch 3][564/834] loss: 0.5957 PSNR_train: 32.5701\n",
      "[epoch 3][565/834] loss: 0.6137 PSNR_train: 32.7315\n",
      "[epoch 3][566/834] loss: 0.5788 PSNR_train: 32.8579\n",
      "[epoch 3][567/834] loss: 0.6056 PSNR_train: 32.6368\n",
      "[epoch 3][568/834] loss: 0.5920 PSNR_train: 32.8273\n",
      "[epoch 3][569/834] loss: 0.5649 PSNR_train: 32.9169\n",
      "[epoch 3][570/834] loss: 0.5894 PSNR_train: 32.8397\n",
      "[epoch 3][571/834] loss: 0.5900 PSNR_train: 32.7574\n",
      "[epoch 3][572/834] loss: 0.6552 PSNR_train: 32.3420\n",
      "[epoch 3][573/834] loss: 0.5785 PSNR_train: 32.9241\n",
      "[epoch 3][574/834] loss: 0.5965 PSNR_train: 32.7375\n",
      "[epoch 3][575/834] loss: 0.5884 PSNR_train: 32.8578\n",
      "[epoch 3][576/834] loss: 0.5809 PSNR_train: 32.8368\n",
      "[epoch 3][577/834] loss: 0.5800 PSNR_train: 32.9831\n",
      "[epoch 3][578/834] loss: 0.6081 PSNR_train: 32.7085\n",
      "[epoch 3][579/834] loss: 0.6464 PSNR_train: 32.4287\n",
      "[epoch 3][580/834] loss: 0.6336 PSNR_train: 32.5892\n",
      "[epoch 3][581/834] loss: 0.5871 PSNR_train: 32.7134\n",
      "[epoch 3][582/834] loss: 0.6250 PSNR_train: 32.4817\n",
      "[epoch 3][583/834] loss: 0.6130 PSNR_train: 32.6740\n",
      "[epoch 3][584/834] loss: 0.5937 PSNR_train: 32.8803\n",
      "[epoch 3][585/834] loss: 0.6108 PSNR_train: 32.6206\n",
      "[epoch 3][586/834] loss: 0.5982 PSNR_train: 32.8337\n",
      "[epoch 3][587/834] loss: 0.6552 PSNR_train: 32.3514\n",
      "[epoch 3][588/834] loss: 0.5914 PSNR_train: 32.7282\n",
      "[epoch 3][589/834] loss: 0.5780 PSNR_train: 33.0915\n",
      "[epoch 3][590/834] loss: 0.6306 PSNR_train: 32.5481\n",
      "[epoch 3][591/834] loss: 0.6192 PSNR_train: 32.5504\n",
      "[epoch 3][592/834] loss: 0.6325 PSNR_train: 32.4162\n",
      "[epoch 3][593/834] loss: 0.5726 PSNR_train: 32.9171\n",
      "[epoch 3][594/834] loss: 0.6158 PSNR_train: 32.5292\n",
      "[epoch 3][595/834] loss: 0.5987 PSNR_train: 32.8104\n",
      "[epoch 3][596/834] loss: 0.5717 PSNR_train: 33.0254\n",
      "[epoch 3][597/834] loss: 0.5721 PSNR_train: 32.8111\n",
      "[epoch 3][598/834] loss: 0.5899 PSNR_train: 32.7552\n",
      "[epoch 3][599/834] loss: 0.6048 PSNR_train: 32.7190\n",
      "[epoch 3][600/834] loss: 0.6052 PSNR_train: 32.5564\n",
      "[epoch 3][601/834] loss: 0.5964 PSNR_train: 32.6218\n",
      "[epoch 3][602/834] loss: 0.6102 PSNR_train: 32.6640\n",
      "[epoch 3][603/834] loss: 0.6074 PSNR_train: 32.5725\n",
      "[epoch 3][604/834] loss: 0.5514 PSNR_train: 33.0352\n",
      "[epoch 3][605/834] loss: 0.6307 PSNR_train: 32.5781\n",
      "[epoch 3][606/834] loss: 0.5886 PSNR_train: 32.6874\n",
      "[epoch 3][607/834] loss: 0.6099 PSNR_train: 32.6098\n",
      "[epoch 3][608/834] loss: 0.5937 PSNR_train: 32.7508\n",
      "[epoch 3][609/834] loss: 0.5665 PSNR_train: 32.8152\n",
      "[epoch 3][610/834] loss: 0.6245 PSNR_train: 32.3795\n",
      "[epoch 3][611/834] loss: 0.6110 PSNR_train: 32.4569\n",
      "[epoch 3][612/834] loss: 0.5733 PSNR_train: 32.6731\n",
      "[epoch 3][613/834] loss: 0.6317 PSNR_train: 32.4538\n",
      "[epoch 3][614/834] loss: 0.5715 PSNR_train: 32.8086\n",
      "[epoch 3][615/834] loss: 0.6443 PSNR_train: 32.4762\n",
      "[epoch 3][616/834] loss: 0.5606 PSNR_train: 32.8752\n",
      "[epoch 3][617/834] loss: 0.6270 PSNR_train: 32.5888\n",
      "[epoch 3][618/834] loss: 0.6129 PSNR_train: 32.6293\n",
      "[epoch 3][619/834] loss: 0.5811 PSNR_train: 32.8482\n",
      "[epoch 3][620/834] loss: 0.5912 PSNR_train: 32.7378\n",
      "[epoch 3][621/834] loss: 0.6221 PSNR_train: 32.4794\n",
      "[epoch 3][622/834] loss: 0.6054 PSNR_train: 32.6403\n",
      "[epoch 3][623/834] loss: 0.6114 PSNR_train: 32.6124\n",
      "[epoch 3][624/834] loss: 0.6121 PSNR_train: 32.6424\n",
      "[epoch 3][625/834] loss: 0.6287 PSNR_train: 32.5819\n",
      "[epoch 3][626/834] loss: 0.6147 PSNR_train: 32.6054\n",
      "[epoch 3][627/834] loss: 0.6131 PSNR_train: 32.5437\n",
      "[epoch 3][628/834] loss: 0.6082 PSNR_train: 32.5955\n",
      "[epoch 3][629/834] loss: 0.5934 PSNR_train: 32.7219\n",
      "[epoch 3][630/834] loss: 0.6153 PSNR_train: 32.6770\n",
      "[epoch 3][631/834] loss: 0.6371 PSNR_train: 32.2798\n",
      "[epoch 3][632/834] loss: 0.6021 PSNR_train: 32.7578\n",
      "[epoch 3][633/834] loss: 0.5609 PSNR_train: 33.0381\n",
      "[epoch 3][634/834] loss: 0.6106 PSNR_train: 32.5625\n",
      "[epoch 3][635/834] loss: 0.5942 PSNR_train: 32.7979\n",
      "[epoch 3][636/834] loss: 0.6328 PSNR_train: 32.5407\n",
      "[epoch 3][637/834] loss: 0.5685 PSNR_train: 32.8912\n",
      "[epoch 3][638/834] loss: 0.6344 PSNR_train: 32.5201\n",
      "[epoch 3][639/834] loss: 0.5892 PSNR_train: 32.7138\n",
      "[epoch 3][640/834] loss: 0.5783 PSNR_train: 32.9371\n",
      "[epoch 3][641/834] loss: 0.5996 PSNR_train: 32.5908\n",
      "[epoch 3][642/834] loss: 0.6334 PSNR_train: 32.4806\n",
      "[epoch 3][643/834] loss: 0.5848 PSNR_train: 32.8603\n",
      "[epoch 3][644/834] loss: 0.5802 PSNR_train: 32.7380\n",
      "[epoch 3][645/834] loss: 0.5976 PSNR_train: 32.8067\n",
      "[epoch 3][646/834] loss: 0.6103 PSNR_train: 32.6546\n",
      "[epoch 3][647/834] loss: 0.6168 PSNR_train: 32.7680\n",
      "[epoch 3][648/834] loss: 0.6332 PSNR_train: 32.6162\n",
      "[epoch 3][649/834] loss: 0.5940 PSNR_train: 32.7724\n",
      "[epoch 3][650/834] loss: 0.6079 PSNR_train: 32.7175\n",
      "[epoch 3][651/834] loss: 0.5980 PSNR_train: 32.6702\n",
      "[epoch 3][652/834] loss: 0.6247 PSNR_train: 32.5074\n",
      "[epoch 3][653/834] loss: 0.6468 PSNR_train: 32.5537\n",
      "[epoch 3][654/834] loss: 0.6138 PSNR_train: 32.7850\n",
      "[epoch 3][655/834] loss: 0.5876 PSNR_train: 32.6706\n",
      "[epoch 3][656/834] loss: 0.6219 PSNR_train: 32.4796\n",
      "[epoch 3][657/834] loss: 0.6229 PSNR_train: 32.5271\n",
      "[epoch 3][658/834] loss: 0.5988 PSNR_train: 32.6631\n",
      "[epoch 3][659/834] loss: 0.5943 PSNR_train: 32.7612\n",
      "[epoch 3][660/834] loss: 0.6157 PSNR_train: 32.5302\n",
      "[epoch 3][661/834] loss: 0.5879 PSNR_train: 32.6726\n",
      "[epoch 3][662/834] loss: 0.6091 PSNR_train: 32.5484\n",
      "[epoch 3][663/834] loss: 0.5953 PSNR_train: 32.6914\n",
      "[epoch 3][664/834] loss: 0.5856 PSNR_train: 32.6159\n",
      "[epoch 3][665/834] loss: 0.6426 PSNR_train: 32.4477\n",
      "[epoch 3][666/834] loss: 0.5998 PSNR_train: 32.7021\n",
      "[epoch 3][667/834] loss: 0.6145 PSNR_train: 32.7381\n",
      "[epoch 3][668/834] loss: 0.6166 PSNR_train: 32.6144\n",
      "[epoch 3][669/834] loss: 0.5884 PSNR_train: 32.7539\n",
      "[epoch 3][670/834] loss: 0.6034 PSNR_train: 32.8234\n",
      "[epoch 3][671/834] loss: 0.5993 PSNR_train: 32.7690\n",
      "[epoch 3][672/834] loss: 0.5993 PSNR_train: 32.6378\n",
      "[epoch 3][673/834] loss: 0.5874 PSNR_train: 32.7385\n",
      "[epoch 3][674/834] loss: 0.6119 PSNR_train: 32.6936\n",
      "[epoch 3][675/834] loss: 0.5497 PSNR_train: 32.9542\n",
      "[epoch 3][676/834] loss: 0.6330 PSNR_train: 32.3745\n",
      "[epoch 3][677/834] loss: 0.5999 PSNR_train: 32.7517\n",
      "[epoch 3][678/834] loss: 0.5933 PSNR_train: 32.6317\n",
      "[epoch 3][679/834] loss: 0.5405 PSNR_train: 33.1170\n",
      "[epoch 3][680/834] loss: 0.5744 PSNR_train: 32.8044\n",
      "[epoch 3][681/834] loss: 0.6053 PSNR_train: 32.6130\n",
      "[epoch 3][682/834] loss: 0.6291 PSNR_train: 32.4514\n",
      "[epoch 3][683/834] loss: 0.5931 PSNR_train: 32.7356\n",
      "[epoch 3][684/834] loss: 0.6326 PSNR_train: 32.3470\n",
      "[epoch 3][685/834] loss: 0.5838 PSNR_train: 32.9197\n",
      "[epoch 3][686/834] loss: 0.5683 PSNR_train: 32.9319\n",
      "[epoch 3][687/834] loss: 0.6294 PSNR_train: 32.4313\n",
      "[epoch 3][688/834] loss: 0.5952 PSNR_train: 32.8087\n",
      "[epoch 3][689/834] loss: 0.5838 PSNR_train: 32.7241\n",
      "[epoch 3][690/834] loss: 0.5497 PSNR_train: 33.0028\n",
      "[epoch 3][691/834] loss: 0.5976 PSNR_train: 32.7376\n",
      "[epoch 3][692/834] loss: 0.5961 PSNR_train: 32.6889\n",
      "[epoch 3][693/834] loss: 0.6020 PSNR_train: 32.6266\n",
      "[epoch 3][694/834] loss: 0.6242 PSNR_train: 32.5606\n",
      "[epoch 3][695/834] loss: 0.6108 PSNR_train: 32.4790\n",
      "[epoch 3][696/834] loss: 0.6598 PSNR_train: 32.3270\n",
      "[epoch 3][697/834] loss: 0.5996 PSNR_train: 32.6540\n",
      "[epoch 3][698/834] loss: 0.5402 PSNR_train: 33.1607\n",
      "[epoch 3][699/834] loss: 0.5699 PSNR_train: 32.9236\n",
      "[epoch 3][700/834] loss: 0.6352 PSNR_train: 32.4498\n",
      "[epoch 3][701/834] loss: 0.5714 PSNR_train: 32.9020\n",
      "[epoch 3][702/834] loss: 0.6210 PSNR_train: 32.5691\n",
      "[epoch 3][703/834] loss: 0.5732 PSNR_train: 32.8893\n",
      "[epoch 3][704/834] loss: 0.6036 PSNR_train: 32.5930\n",
      "[epoch 3][705/834] loss: 0.6336 PSNR_train: 32.4533\n",
      "[epoch 3][706/834] loss: 0.5601 PSNR_train: 33.0693\n",
      "[epoch 3][707/834] loss: 0.6185 PSNR_train: 32.6363\n",
      "[epoch 3][708/834] loss: 0.5634 PSNR_train: 32.9800\n",
      "[epoch 3][709/834] loss: 0.5729 PSNR_train: 32.8922\n",
      "[epoch 3][710/834] loss: 0.6084 PSNR_train: 32.5997\n",
      "[epoch 3][711/834] loss: 0.5925 PSNR_train: 32.7664\n",
      "[epoch 3][712/834] loss: 0.5866 PSNR_train: 32.7948\n",
      "[epoch 3][713/834] loss: 0.6012 PSNR_train: 32.7127\n",
      "[epoch 3][714/834] loss: 0.6034 PSNR_train: 32.6733\n",
      "[epoch 3][715/834] loss: 0.6335 PSNR_train: 32.5462\n",
      "[epoch 3][716/834] loss: 0.6266 PSNR_train: 32.4240\n",
      "[epoch 3][717/834] loss: 0.6351 PSNR_train: 32.6074\n",
      "[epoch 3][718/834] loss: 0.5915 PSNR_train: 32.8925\n",
      "[epoch 3][719/834] loss: 0.5992 PSNR_train: 32.9166\n",
      "[epoch 3][720/834] loss: 0.6422 PSNR_train: 32.4065\n",
      "[epoch 3][721/834] loss: 0.6018 PSNR_train: 32.7851\n",
      "[epoch 3][722/834] loss: 0.5666 PSNR_train: 32.9768\n",
      "[epoch 3][723/834] loss: 0.5527 PSNR_train: 33.2031\n",
      "[epoch 3][724/834] loss: 0.6068 PSNR_train: 32.7192\n",
      "[epoch 3][725/834] loss: 0.6212 PSNR_train: 32.7141\n",
      "[epoch 3][726/834] loss: 0.6289 PSNR_train: 32.3794\n",
      "[epoch 3][727/834] loss: 0.5757 PSNR_train: 32.9736\n",
      "[epoch 3][728/834] loss: 0.5794 PSNR_train: 32.8995\n",
      "[epoch 3][729/834] loss: 0.5702 PSNR_train: 32.9434\n",
      "[epoch 3][730/834] loss: 0.5809 PSNR_train: 32.8652\n",
      "[epoch 3][731/834] loss: 0.6054 PSNR_train: 32.6086\n",
      "[epoch 3][732/834] loss: 0.6724 PSNR_train: 32.2454\n",
      "[epoch 3][733/834] loss: 0.5659 PSNR_train: 33.0877\n",
      "[epoch 3][734/834] loss: 0.6000 PSNR_train: 32.7873\n",
      "[epoch 3][735/834] loss: 0.6459 PSNR_train: 32.4227\n",
      "[epoch 3][736/834] loss: 0.5986 PSNR_train: 32.7464\n",
      "[epoch 3][737/834] loss: 0.6220 PSNR_train: 32.5399\n",
      "[epoch 3][738/834] loss: 0.5875 PSNR_train: 32.6561\n",
      "[epoch 3][739/834] loss: 0.5560 PSNR_train: 32.9282\n",
      "[epoch 3][740/834] loss: 0.6113 PSNR_train: 32.6047\n",
      "[epoch 3][741/834] loss: 0.5583 PSNR_train: 33.0995\n",
      "[epoch 3][742/834] loss: 0.6574 PSNR_train: 32.3411\n",
      "[epoch 3][743/834] loss: 0.5755 PSNR_train: 32.8280\n",
      "[epoch 3][744/834] loss: 0.6264 PSNR_train: 32.4714\n",
      "[epoch 3][745/834] loss: 0.6119 PSNR_train: 32.6253\n",
      "[epoch 3][746/834] loss: 0.6515 PSNR_train: 32.2131\n",
      "[epoch 3][747/834] loss: 0.6179 PSNR_train: 32.4458\n",
      "[epoch 3][748/834] loss: 0.5844 PSNR_train: 32.6954\n",
      "[epoch 3][749/834] loss: 0.6084 PSNR_train: 32.5415\n",
      "[epoch 3][750/834] loss: 0.6014 PSNR_train: 32.5145\n",
      "[epoch 3][751/834] loss: 0.6129 PSNR_train: 32.4215\n",
      "[epoch 3][752/834] loss: 0.5798 PSNR_train: 32.7508\n",
      "[epoch 3][753/834] loss: 0.5741 PSNR_train: 32.8996\n",
      "[epoch 3][754/834] loss: 0.5608 PSNR_train: 33.0525\n",
      "[epoch 3][755/834] loss: 0.6184 PSNR_train: 32.4982\n",
      "[epoch 3][756/834] loss: 0.6239 PSNR_train: 32.3330\n",
      "[epoch 3][757/834] loss: 0.5752 PSNR_train: 32.8339\n",
      "[epoch 3][758/834] loss: 0.5714 PSNR_train: 32.8398\n",
      "[epoch 3][759/834] loss: 0.6215 PSNR_train: 32.4635\n",
      "[epoch 3][760/834] loss: 0.5917 PSNR_train: 32.6507\n",
      "[epoch 3][761/834] loss: 0.5847 PSNR_train: 32.8502\n",
      "[epoch 3][762/834] loss: 0.6017 PSNR_train: 32.5855\n",
      "[epoch 3][763/834] loss: 0.6279 PSNR_train: 32.4275\n",
      "[epoch 3][764/834] loss: 0.6255 PSNR_train: 32.5927\n",
      "[epoch 3][765/834] loss: 0.6194 PSNR_train: 32.3870\n",
      "[epoch 3][766/834] loss: 0.5956 PSNR_train: 32.7502\n",
      "[epoch 3][767/834] loss: 0.6092 PSNR_train: 32.6738\n",
      "[epoch 3][768/834] loss: 0.5864 PSNR_train: 32.7820\n",
      "[epoch 3][769/834] loss: 0.5910 PSNR_train: 32.8353\n",
      "[epoch 3][770/834] loss: 0.6176 PSNR_train: 32.4992\n",
      "[epoch 3][771/834] loss: 0.5521 PSNR_train: 33.0060\n",
      "[epoch 3][772/834] loss: 0.5638 PSNR_train: 32.9159\n",
      "[epoch 3][773/834] loss: 0.5960 PSNR_train: 32.6928\n",
      "[epoch 3][774/834] loss: 0.5771 PSNR_train: 32.8212\n",
      "[epoch 3][775/834] loss: 0.6181 PSNR_train: 32.5555\n",
      "[epoch 3][776/834] loss: 0.6033 PSNR_train: 32.5878\n",
      "[epoch 3][777/834] loss: 0.6055 PSNR_train: 32.7903\n",
      "[epoch 3][778/834] loss: 0.5688 PSNR_train: 33.0788\n",
      "[epoch 3][779/834] loss: 0.5927 PSNR_train: 32.8328\n",
      "[epoch 3][780/834] loss: 0.6083 PSNR_train: 32.6973\n",
      "[epoch 3][781/834] loss: 0.6361 PSNR_train: 32.5632\n",
      "[epoch 3][782/834] loss: 0.6095 PSNR_train: 32.7281\n",
      "[epoch 3][783/834] loss: 0.5856 PSNR_train: 32.9424\n",
      "[epoch 3][784/834] loss: 0.5946 PSNR_train: 32.7802\n",
      "[epoch 3][785/834] loss: 0.5708 PSNR_train: 33.0582\n",
      "[epoch 3][786/834] loss: 0.5637 PSNR_train: 33.1019\n",
      "[epoch 3][787/834] loss: 0.6176 PSNR_train: 32.6550\n",
      "[epoch 3][788/834] loss: 0.5781 PSNR_train: 32.8171\n",
      "[epoch 3][789/834] loss: 0.6020 PSNR_train: 32.7922\n",
      "[epoch 3][790/834] loss: 0.6160 PSNR_train: 32.5632\n",
      "[epoch 3][791/834] loss: 0.6074 PSNR_train: 32.6267\n",
      "[epoch 3][792/834] loss: 0.5561 PSNR_train: 33.0719\n",
      "[epoch 3][793/834] loss: 0.6126 PSNR_train: 32.5481\n",
      "[epoch 3][794/834] loss: 0.5981 PSNR_train: 32.7619\n",
      "[epoch 3][795/834] loss: 0.6112 PSNR_train: 32.6381\n",
      "[epoch 3][796/834] loss: 0.6255 PSNR_train: 32.4537\n",
      "[epoch 3][797/834] loss: 0.5943 PSNR_train: 32.7920\n",
      "[epoch 3][798/834] loss: 0.6099 PSNR_train: 32.5656\n",
      "[epoch 3][799/834] loss: 0.6112 PSNR_train: 32.7109\n",
      "[epoch 3][800/834] loss: 0.5919 PSNR_train: 32.6160\n",
      "[epoch 3][801/834] loss: 0.6112 PSNR_train: 32.6705\n",
      "[epoch 3][802/834] loss: 0.6124 PSNR_train: 32.7013\n",
      "[epoch 3][803/834] loss: 0.5769 PSNR_train: 32.7525\n",
      "[epoch 3][804/834] loss: 0.5616 PSNR_train: 33.0861\n",
      "[epoch 3][805/834] loss: 0.6133 PSNR_train: 32.5334\n",
      "[epoch 3][806/834] loss: 0.6260 PSNR_train: 32.6396\n",
      "[epoch 3][807/834] loss: 0.5868 PSNR_train: 32.7824\n",
      "[epoch 3][808/834] loss: 0.5838 PSNR_train: 32.8528\n",
      "[epoch 3][809/834] loss: 0.6051 PSNR_train: 32.6388\n",
      "[epoch 3][810/834] loss: 0.5692 PSNR_train: 32.9276\n",
      "[epoch 3][811/834] loss: 0.5637 PSNR_train: 32.8884\n",
      "[epoch 3][812/834] loss: 0.5783 PSNR_train: 32.8327\n",
      "[epoch 3][813/834] loss: 0.5858 PSNR_train: 32.8706\n",
      "[epoch 3][814/834] loss: 0.5967 PSNR_train: 32.7486\n",
      "[epoch 3][815/834] loss: 0.5834 PSNR_train: 32.7649\n",
      "[epoch 3][816/834] loss: 0.5707 PSNR_train: 32.8919\n",
      "[epoch 3][817/834] loss: 0.5885 PSNR_train: 32.7305\n",
      "[epoch 3][818/834] loss: 0.6116 PSNR_train: 32.6912\n",
      "[epoch 3][819/834] loss: 0.6143 PSNR_train: 32.5647\n",
      "[epoch 3][820/834] loss: 0.6432 PSNR_train: 32.3695\n",
      "[epoch 3][821/834] loss: 0.6267 PSNR_train: 32.5007\n",
      "[epoch 3][822/834] loss: 0.6175 PSNR_train: 32.6446\n",
      "[epoch 3][823/834] loss: 0.5912 PSNR_train: 32.7125\n",
      "[epoch 3][824/834] loss: 0.5942 PSNR_train: 32.7373\n",
      "[epoch 3][825/834] loss: 0.6014 PSNR_train: 32.8569\n",
      "[epoch 3][826/834] loss: 0.5905 PSNR_train: 32.5900\n",
      "[epoch 3][827/834] loss: 0.6427 PSNR_train: 32.4587\n",
      "[epoch 3][828/834] loss: 0.6210 PSNR_train: 32.4460\n",
      "[epoch 3][829/834] loss: 0.6320 PSNR_train: 32.3754\n",
      "[epoch 3][830/834] loss: 0.5870 PSNR_train: 32.6646\n",
      "[epoch 3][831/834] loss: 0.5889 PSNR_train: 32.7459\n",
      "[epoch 3][832/834] loss: 0.6172 PSNR_train: 32.6841\n",
      "[epoch 3][833/834] loss: 0.6255 PSNR_train: 32.5028\n",
      "[epoch 3][834/834] loss: 0.5630 PSNR_train: 32.9943\n",
      "\n",
      "[epoch 3] PSNR_val: 36.0758\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.epochs):\n",
    "        if epoch < opt.milestone:\n",
    "            current_lr = opt.lr\n",
    "        else:\n",
    "            current_lr = opt.lr / 10.\n",
    "        # set learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = current_lr\n",
    "        print('learning rate %f' % current_lr)\n",
    "        # train\n",
    "        for i, data in enumerate(loader_train, 0):\n",
    "            # training step\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            img_train = data\n",
    "            if opt.mode == 'S':\n",
    "                noise = torch.FloatTensor(img_train.size()).normal_(mean=0, std=opt.noiseL)\n",
    "            elif opt.mode == 'B':\n",
    "                noise = torch.zeros(img_train.size())\n",
    "                stdN = np.random.uniform(noiseL_B[0], noiseL_B[1], size=noise.size()[0])\n",
    "                for n in range(noise.size()[0]):\n",
    "                    sizeN = noise[0,:,:,:].size()\n",
    "                    noise[n,:,:,:] = torch.FloatTensor(sizeN).normal_(mean=0, std=stdN[n])\n",
    "            elif opt.mode = 'M': \n",
    "                # delete some pixels \n",
    "                noise = torch.FloatTensor(img_train.size()).normal_(mean=0, std=opt.noiseL)\n",
    "\n",
    "            imgn_train = img_train + noise\n",
    "            img_train, imgn_train = Variable(img_train.cuda()), Variable(imgn_train.cuda())\n",
    "            noise = Variable(noise.cuda())\n",
    "            out_train = model(imgn_train)\n",
    "            loss = criterion(out_train, noise) / (imgn_train.size()[0]*2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # results\n",
    "            model.eval()\n",
    "            out_train = torch.clamp(imgn_train-model(imgn_train), 0., 1.)\n",
    "            psnr_train = batch_PSNR(out_train, img_train, 1.)\n",
    "            print(\"[epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f\" %\n",
    "                (epoch+1, i+1, len(loader_train), loss.item(), psnr_train))\n",
    "            # if you are using older version of PyTorch, you may need to change loss.item() to loss.data[0]\n",
    "            if step % 10 == 0:\n",
    "                # Log the scalar values\n",
    "                writer.add_scalar('loss', loss.item(), step)\n",
    "                writer.add_scalar('PSNR on training data', psnr_train, step)\n",
    "            step += 1\n",
    "        ## the end of each epoch\n",
    "        model.eval()\n",
    "        # validate\n",
    "        psnr_val = 0\n",
    "        for k in range(len(dataset_val)):\n",
    "            img_val = torch.unsqueeze(dataset_val[k], 0)\n",
    "            if opt.mode == 'S':\n",
    "                noise = torch.FloatTensor(img_val.size()).normal_(mean=0, std=opt.noiseL)\n",
    "            elif opt.mode == 'B':\n",
    "                noise = torch.zeros(img.size())\n",
    "                stdN = np.random.uniform(noiseL_B[0], noiseL_B[1], size=noise.size()[0])\n",
    "                for n in range(noise.size()[0]):\n",
    "                    sizeN = noise[0,:,:,:].size()\n",
    "                    noise[n,:,:,:] = torch.FloatTensor(sizeN).normal_(mean=0, std=stdN[n])\n",
    "            elif opt.mode = 'M': \n",
    "                # delete some pixels \n",
    "                noise = torch.FloatTensor(img_val.size()).normal_(mean=0, std=opt.noiseL)\n",
    "\n",
    "            noise = torch.FloatTensor(img_val.size()).normal_(mean=0, std=opt.val_noiseL)\n",
    "            imgn_val = img_val + noise\n",
    "            img_val, imgn_val = Variable(img_val.cuda()), Variable(imgn_val.cuda())\n",
    "            out_val = torch.clamp(imgn_val-model(imgn_val), 0., 1.)\n",
    "            psnr_val += batch_PSNR(out_val, img_val, 1.)\n",
    "        psnr_val /= len(dataset_val)\n",
    "        print(\"\\n[epoch %d] PSNR_val: %.4f\" % (epoch+1, psnr_val))\n",
    "        writer.add_scalar('PSNR on validation data', psnr_val, epoch)\n",
    "        # log the images\n",
    "#         out_train = torch.clamp(imgn_train-model(imgn_train), 0., 1.)\n",
    "#         Img = utils.make_grid(img_train.data, nrow=8, normalize=True, scale_each=True)\n",
    "#         Imgn = utils.make_grid(imgn_train.data, nrow=8, normalize=True, scale_each=True)\n",
    "#         Irecon = utils.make_grid(out_train.data, nrow=8, normalize=True, scale_each=True)\n",
    "#         writer.add_image('clean image', Img, epoch)\n",
    "#         writer.add_image('noisy image', Imgn, epoch)\n",
    "#         writer.add_image('reconstructed image', Irecon, epoch)\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), os.path.join(opt.outf, 'net_{}.pth'.format(opt.num_of_layers)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
